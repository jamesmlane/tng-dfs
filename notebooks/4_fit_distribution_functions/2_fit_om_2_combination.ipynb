{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# TITLE - 2_fit_om_2_combination.ipynb\n",
    "# AUTHOR - James Lane\n",
    "# PROJECT - tng-dfs\n",
    "#\n",
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# Docstrings and metadata:\n",
    "'''Fit Osipkov-Merritt linear combination models to data. First construct a \n",
    "grid of such models for each scale radius. Then create a grid of velocity \n",
    "dispersion.\n",
    "'''\n",
    "\n",
    "__author__ = \"James Lane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../src/nb_modules/nb_imports.txt\n",
    "### Imports\n",
    "\n",
    "## Basic\n",
    "import numpy as np\n",
    "import sys, os, dill as pickle\n",
    "import pdb, copy, glob, time, warnings, logging, multiprocessing\n",
    "\n",
    "## Plotting\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import corner\n",
    "\n",
    "## Astropy\n",
    "from astropy import units as apu\n",
    "from astropy import constants as apc\n",
    "\n",
    "## Analysis\n",
    "import scipy.optimize\n",
    "import scipy.interpolate\n",
    "import emcee\n",
    "\n",
    "## galpy\n",
    "from galpy import potential\n",
    "from galpy import df\n",
    "\n",
    "## Project-specific\n",
    "src_path = 'src/'\n",
    "while True:\n",
    "    if os.path.exists(src_path): break\n",
    "    if os.path.realpath(src_path).split('/')[-1] in ['tng-dfs','/']:\n",
    "            raise FileNotFoundError('Failed to find src/ directory.')\n",
    "    src_path = os.path.join('..',src_path)\n",
    "sys.path.insert(0,src_path)\n",
    "from tng_dfs import cutout as pcutout\n",
    "from tng_dfs import densprofile as pdens\n",
    "from tng_dfs import fitting as pfit\n",
    "from tng_dfs import io as pio\n",
    "from tng_dfs import kinematics as pkin\n",
    "from tng_dfs import util as putil\n",
    "\n",
    "### Notebook setup\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(os.path.join(src_path,'mpl/project.mplstyle')) # This must be exactly here\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../src/nb_modules/nb_setup.txt\n",
    "# Keywords\n",
    "cdict = putil.load_config_to_dict()\n",
    "keywords = ['DATA_DIR','MW_ANALOG_DIR','FIG_DIR_BASE','FITTING_DIR_BASE',\n",
    "            'RO','VO','ZO','LITTLE_H','MW_MASS_RANGE']\n",
    "data_dir,mw_analog_dir,fig_dir_base,fitting_dir_base,ro,vo,zo,h,\\\n",
    "    mw_mass_range = putil.parse_config_dict(cdict,keywords)\n",
    "\n",
    "# MW Analog \n",
    "mwsubs,mwsubs_vars = putil.prepare_mwsubs(mw_analog_dir,h=h,\n",
    "    mw_mass_range=mw_mass_range,return_vars=True,force_mwsubs=False,\n",
    "    bulge_disk_fraction_cuts=True)\n",
    "\n",
    "# Figure path\n",
    "local_fig_dir = './fig/'\n",
    "fig_dir = os.path.join(fig_dir_base, \n",
    "    'notebooks/4_fit_distribution_functions/2_fit_om_2_combination/')\n",
    "os.makedirs(local_fig_dir,exist_ok=True)\n",
    "os.makedirs(fig_dir,exist_ok=True)\n",
    "show_plots = False\n",
    "\n",
    "# Load tree data\n",
    "tree_primary_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_primaries.pkl')\n",
    "with open(tree_primary_filename,'rb') as handle: \n",
    "    tree_primaries = pickle.load(handle)\n",
    "tree_major_mergers_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_major_mergers.pkl')\n",
    "with open(tree_major_mergers_filename,'rb') as handle:\n",
    "    tree_major_mergers = pickle.load(handle)\n",
    "n_mw = len(tree_primaries)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the grid of DF data for fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some keywords and properties\n",
    "force_df_grid = False\n",
    "test_pickling = True\n",
    "verbose = True\n",
    "dens_fitting_dir = os.path.join(fitting_dir_base,'density_profile/')\n",
    "df_fitting_dir = os.path.join(fitting_dir_base,'distribution_function/')\n",
    "\n",
    "# Potential interpolator version\n",
    "interpot_version = 'all_star_dm_enclosed_mass'\n",
    "\n",
    "# Stellar halo density information\n",
    "stellar_halo_density_version = 'poisson_twopower_softening'\n",
    "stellar_halo_density_ncut = 500\n",
    "stellar_halo_densfunc = pdens.TwoPowerSpherical()\n",
    "\n",
    "# Stellar halo rotation information\n",
    "stellar_halo_rotation_version = 'tanh_rotation'\n",
    "stellar_halo_rotation_ncut = 500\n",
    "\n",
    "# Anisotropy information\n",
    "df_type = 'osipkov_merritt_2_combination'\n",
    "anisotropy_fit_version = 'ra_N20_001_to_1000_softening'\n",
    "# anisotropy_ncut = 500\n",
    "\n",
    "# DF versioning\n",
    "# df_version = 'df_density_softening'\n",
    "\n",
    "# Ignore some standard warnings\n",
    "warnings.filterwarnings(action='ignore', \n",
    "    message='No particle IDs found', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', \n",
    "    message='maxiter', category=scipy.integrate.AccuracyWarning)\n",
    "warnings.filterwarnings(action='ignore', \n",
    "    message='invalid value encountered', category=RuntimeWarning)\n",
    "warnings.filterwarnings(action='ignore',\n",
    "    message='subdivisions', category=scipy.integrate.IntegrationWarning)\n",
    "warnings.filterwarnings(action='ignore',\n",
    "    message='divergent', category=scipy.integrate.IntegrationWarning)\n",
    "\n",
    "# Begin logging\n",
    "log_filename = './log/2_fit_om_2_combination_df_grid.log'\n",
    "if os.path.exists(log_filename):\n",
    "    os.remove(log_filename)\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO, filemode='w', \n",
    "    force=True)\n",
    "logging.info('Beginning Osipkov-Merritt 2 combination DF grid creation. Time: '+\\\n",
    "             time.strftime('%a, %d %b %Y %H:%M:%S',time.localtime()))\n",
    "\n",
    "# Construct the r_a grid\n",
    "log_ra_min = -2\n",
    "log_ra_max = 3\n",
    "ra_n = 20\n",
    "ra = (10**np.linspace(log_ra_min, log_ra_max, ra_n, endpoint=True))*apu.kpc\n",
    "\n",
    "for i in range(n_mw):\n",
    "    if i > 0: continue\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    major_mergers = primary.tree_major_mergers\n",
    "    n_major = primary.n_major_mergers\n",
    "    n_snap = len(primary.snapnum)\n",
    "    primary_filename = primary.get_cutout_filename(mw_analog_dir,\n",
    "        snapnum=primary.snapnum[0])\n",
    "    co = pcutout.TNGCutout(primary_filename)\n",
    "    co.center_and_rectify()\n",
    "    pid = co.get_property('stars','ParticleIDs')\n",
    "\n",
    "    # Load the interpolator for the sphericalized potential\n",
    "    interpolator_filename = os.path.join(dens_fitting_dir,\n",
    "        'spherical_interpolated_potential/',interpot_version,\n",
    "        str(z0_sid),'interp_potential.pkl')\n",
    "    with open(interpolator_filename,'rb') as handle:\n",
    "        interpot = pickle.load(handle)\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j > 0: continue\n",
    "        if verbose:\n",
    "            msg = f'Analyzing major merger {j+1}/{n_major} for MW {i+1}/{n_mw}'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "\n",
    "        # Filename and pathing check\n",
    "        this_fitting_dir = os.path.join(df_fitting_dir, df_type, \n",
    "            anisotropy_fit_version, str(z0_sid),'merger_'+str(j+1))\n",
    "        os.makedirs(this_fitting_dir,exist_ok=True)\n",
    "        df_grid_filename = os.path.join(this_fitting_dir,'df_grid.pkl')\n",
    "        if os.path.exists(df_grid_filename) and not force_df_grid:\n",
    "            if verbose:\n",
    "                msg = f'Already have DF grid, continuing'\n",
    "                logging.info(msg)\n",
    "                print(msg)\n",
    "            continue\n",
    "            \n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_acc_sid = major_merger.subfind_id[0]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "        upid = major_merger.get_unique_particle_ids('stars',data_dir=data_dir)\n",
    "        indx = np.where(np.isin(pid,upid))[0]\n",
    "        orbs = co.get_orbs('stars')[indx]\n",
    "        rs = orbs.r().to(apu.kpc).value\n",
    "        # n_star = len(orbs)\n",
    "\n",
    "        # Get the stellar halo density profile (denspot for the DF)\n",
    "        stellar_halo_density_filename = os.path.join(dens_fitting_dir,\n",
    "            'stellar_halo/',stellar_halo_density_version,str(z0_sid),\n",
    "            'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        denspot = pfit.construct_pot_from_fit(\n",
    "            stellar_halo_density_filename, stellar_halo_densfunc, \n",
    "            stellar_halo_density_ncut, ro=ro, vo=vo)\n",
    "        \n",
    "        # Get the stellar halo beta information\n",
    "        # anisotropy_param_dir = os.path.join(df_fitting_dir, df_type,\n",
    "        #         anisotropy_fit_version, str(z0_sid),'merger_'+str(j+1))\n",
    "        # anisotropy_filename = os.path.join(anisotropy_param_dir,'sampler.pkl')\n",
    "        # ra = pio.median_params_from_emcee_sampler(anisotropy_filename,\n",
    "        #     ncut=anisotropy_ncut)[0][0]\n",
    "\n",
    "        df_grid = []\n",
    "        fQ_interp_grid = []\n",
    "\n",
    "        # Loop over the ra grid and build the DFs\n",
    "        for k in range(ra_n):\n",
    "            # Construct the distribution function and do some dummy sampling\n",
    "            # to set the interpolators. Then save.\n",
    "            try:\n",
    "                if verbose:\n",
    "                    msg = f'ra: {round(ra[k].value,3)} kpc, building DF'\n",
    "                    logging.info(msg)\n",
    "                    print(msg)\n",
    "                dfom = df.osipkovmerrittdf(pot=interpot, denspot=denspot, \n",
    "                    ra=ra[k], ro=ro, vo=vo, rmax=rs.max()*apu.kpc*1.1)\n",
    "                print('  Sampling DF')\n",
    "                _ = dfom.sample(n=100, rmin=rs.min()*apu.kpc*0.9)\n",
    "            except Exception as e:\n",
    "                msg = f'Failed to build DF, skipping. Error: {e}'\n",
    "                logging.info(msg)\n",
    "                print(msg)\n",
    "\n",
    "            # Filename built above\n",
    "            if test_pickling:\n",
    "                try:\n",
    "                    pickle.loads(pickle.dumps(dfom))\n",
    "                except RecursionError:\n",
    "                    if verbose:\n",
    "                        msg = 'Caught recursion error when (un)pickling, quiting.'\n",
    "                        logging.info(msg)\n",
    "                        print(msg)\n",
    "            # with open(df_filename,'wb') as handle:\n",
    "            #     pickle.dump(dfom,handle)\n",
    "            df_grid.append(dfom)\n",
    "            fQ_interp_grid.append(dfom._logfQ_interp)\n",
    "\n",
    "        # Save the grid\n",
    "        df_grid_filename = os.path.join(this_fitting_dir,'df_grid.pkl')\n",
    "        with open(df_grid_filename,'wb') as handle:\n",
    "            pickle.dump([df_grid,fQ_interp_grid,ra],handle)\n",
    "\n",
    "        if verbose:\n",
    "            msg = f'Done with merger'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "\n",
    "\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute a grid of velocity dispersion values, the grid is value of ra versus r/ra\n",
    "\n",
    "Use the `fQ` trick. Assign `dfom.fQ = lambda Q: np.exp(dfom._logfQ_interp(Q))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dispersion_grid_plot(sigma_r_grid, sigma_t_grid, r_ra, ra):\n",
    "    '''make_dispersion_grid_plot:\n",
    "\n",
    "    Make a plot of the dispersion grid.\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(8,12))\n",
    "    axs = fig.subplots(3,1)\n",
    "    axs[0].pcolormesh(r_ra.value, ra.value, sigma_r_grid,\n",
    "        cmap='magma',vmin=0,vmax=400)\n",
    "    axs[0].set_xscale('log')\n",
    "    axs[0].set_yscale('log')\n",
    "    axs[0].set_xlabel(r'$r$ [kpc]')\n",
    "    axs[0].set_ylabel(r'$r_a$ [kpc]')\n",
    "    axs[0].set_title(r'$\\sigma_r$ [km/s]')\n",
    "    cbar1 = fig.colorbar(axs[0].collections[0], ax=axs[0])\n",
    "    axs[1].pcolormesh(r_ra.value, ra.value, sigma_t_grid,\n",
    "        cmap='magma',vmin=0,vmax=400)\n",
    "    axs[1].set_xscale('log')\n",
    "    axs[1].set_yscale('log')\n",
    "    axs[1].set_xlabel(r'$r$ [kpc]')\n",
    "    axs[1].set_ylabel(r'$r_a$ [kpc]')\n",
    "    axs[1].set_title(r'$\\sigma_T$ [km/s]')\n",
    "    cbar2 = fig.colorbar(axs[1].collections[0], ax=axs[1])\n",
    "    beta_grid = 1-(sigma_t_grid**2)/(2*sigma_r_grid**2)\n",
    "    axs[2].pcolormesh(r_ra.value, ra.value, beta_grid,\n",
    "        cmap='magma',vmin=0,vmax=1)\n",
    "    axs[2].set_xscale('log')\n",
    "    axs[2].set_yscale('log')\n",
    "    axs[2].set_xlabel(r'$r$ [kpc]')\n",
    "    axs[2].set_ylabel(r'$r_a$ [kpc]')\n",
    "    axs[2].set_title(r'$\\beta$')\n",
    "    cbar3 = fig.colorbar(axs[2].collections[0], ax=axs[2])\n",
    "    fig.tight_layout()\n",
    "    return fig,axs\n",
    "\n",
    "def make_interpolated_dispersion_grid_plot(sigma_r_interp, sigma_t_interp,\n",
    "    r_ra, ra):\n",
    "    '''make_interpolated_dispersion_grid_plot:\n",
    "    \n",
    "    Make a plot of the interpolated dispersion grid.\n",
    "    '''\n",
    "    xnew = 10**np.linspace(np.log10(ra.min().value), \n",
    "        np.log10(ra.max().value), 100)\n",
    "    ynew = 10**np.linspace(np.log10(r_ra.min().value), \n",
    "        np.log10(r_ra.max().value), 100)\n",
    "    xx,yy = np.meshgrid(xnew,ynew,indexing='ij')\n",
    "    zz_r = sigma_r_interp((xx,yy)).T\n",
    "    zz_t = sigma_t_interp((xx,yy)).T\n",
    "    zz_beta = 1-(zz_t**2)/(2*zz_r**2)\n",
    "\n",
    "    fig = plt.figure(figsize=(8,12))\n",
    "    axs = fig.subplots(3,1)\n",
    "    axs[0].pcolormesh(xx,yy,zz_r,cmap='magma',vmin=0,vmax=400)\n",
    "    axs[0].set_xscale('log')\n",
    "    axs[0].set_yscale('log')\n",
    "    axs[0].set_xlabel(r'$r$ [kpc]')\n",
    "    axs[0].set_ylabel(r'$r_a$ [kpc]')\n",
    "    axs[0].set_title(r'$\\sigma_r$ [km/s]')\n",
    "    cbar1 = fig.colorbar(axs[0].collections[0], ax=axs[0])\n",
    "    axs[1].pcolormesh(xx,yy,zz_t,cmap='magma',vmin=0,vmax=400)\n",
    "    axs[1].set_xscale('log')\n",
    "    axs[1].set_yscale('log')\n",
    "    axs[1].set_xlabel(r'$r$ [kpc]')\n",
    "    axs[1].set_ylabel(r'$r_a$ [kpc]')\n",
    "    axs[1].set_title(r'$\\sigma_T$ [km/s]')\n",
    "    cbar2 = fig.colorbar(axs[1].collections[0], ax=axs[1])\n",
    "    axs[2].pcolormesh(xx,yy,zz_beta,cmap='magma',vmin=0,vmax=1)\n",
    "    axs[2].set_xscale('log')\n",
    "    axs[2].set_yscale('log')\n",
    "    axs[2].set_xlabel(r'$r$ [kpc]')\n",
    "    axs[2].set_ylabel(r'$r_a$ [kpc]')\n",
    "    axs[2].set_title(r'$\\beta$')\n",
    "    cbar3 = fig.colorbar(axs[2].collections[0], ax=axs[2])\n",
    "    fig.tight_layout()\n",
    "    return fig,axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some keywords and properties\n",
    "force_r_ra_grid = True\n",
    "# test_pickling = True\n",
    "verbose = True\n",
    "dens_fitting_dir = os.path.join(fitting_dir_base,'density_profile/')\n",
    "df_fitting_dir = os.path.join(fitting_dir_base,'distribution_function/')\n",
    "\n",
    "# Potential interpolator version\n",
    "interpot_version = 'all_star_dm_enclosed_mass'\n",
    "\n",
    "# Stellar halo density information\n",
    "stellar_halo_density_version = 'poisson_twopower_softening'\n",
    "stellar_halo_density_ncut = 500\n",
    "stellar_halo_densfunc = pdens.TwoPowerSpherical()\n",
    "\n",
    "# Stellar halo rotation information\n",
    "stellar_halo_rotation_version = 'tanh_rotation'\n",
    "stellar_halo_rotation_ncut = 500\n",
    "\n",
    "# Anisotropy information\n",
    "df_type = 'osipkov_merritt_2_combination'\n",
    "anisotropy_fit_version = 'ra_N10_01_to_300_softening'\n",
    "# anisotropy_ncut = 500\n",
    "\n",
    "# Begin logging\n",
    "log_filename = './log/2_fit_om_2_combination_vdisp_grid.log'\n",
    "if os.path.exists(log_filename):\n",
    "    os.remove(log_filename)\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO, filemode='w', \n",
    "    force=True)\n",
    "logging.info('Beginning Osipkov-Merritt 2 combination velocity dispersion'+\\\n",
    "             ' grid creation. Time: '+\\\n",
    "             time.strftime('%a, %d %b %Y %H:%M:%S',time.localtime()))\n",
    "\n",
    "for i in range(n_mw):\n",
    "    if i > 0: continue\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    major_mergers = primary.tree_major_mergers\n",
    "    n_major = primary.n_major_mergers\n",
    "    n_snap = len(primary.snapnum)\n",
    "    primary_filename = primary.get_cutout_filename(mw_analog_dir,\n",
    "        snapnum=primary.snapnum[0])\n",
    "    co = pcutout.TNGCutout(primary_filename)\n",
    "    co.center_and_rectify()\n",
    "    pid = co.get_property('stars','ParticleIDs')\n",
    "\n",
    "    # Load the interpolator for the sphericalized potential\n",
    "    interpolator_filename = os.path.join(dens_fitting_dir,\n",
    "        'spherical_interpolated_potential/',interpot_version,\n",
    "        str(z0_sid),'interp_potential.pkl')\n",
    "    with open(interpolator_filename,'rb') as handle:\n",
    "        interpot = pickle.load(handle)\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j > 0: continue\n",
    "        if verbose:\n",
    "            msg = f'Analyzing major merger {j+1}/{n_major} for MW {i+1}/{n_mw}'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "\n",
    "        # Filename and pathing check\n",
    "        this_fitting_dir = os.path.join(df_fitting_dir, df_type, \n",
    "            anisotropy_fit_version, str(z0_sid),'merger_'+str(j+1))\n",
    "        os.makedirs(this_fitting_dir,exist_ok=True)\n",
    "        sigma_grid_filename = os.path.join(this_fitting_dir,'sigma_grid.pkl')\n",
    "        if os.path.exists(sigma_grid_filename) and not force_r_ra_grid:\n",
    "            if verbose:\n",
    "                msg = f'Already have dispersion grids, continuing'\n",
    "                logging.info(msg)\n",
    "                print(msg)\n",
    "            continue\n",
    "        \n",
    "        # Load the DF grid\n",
    "        os.makedirs(this_fitting_dir,exist_ok=True)\n",
    "        df_grid_filename = os.path.join(this_fitting_dir,'df_grid.pkl')\n",
    "        with open(df_grid_filename,'rb') as handle:\n",
    "            df_grid,fQ_interp_grid,ra = pickle.load(handle)\n",
    "        ra_n = len(ra)\n",
    "\n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_acc_sid = major_merger.subfind_id[0]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "        upid = major_merger.get_unique_particle_ids('stars',data_dir=data_dir)\n",
    "        indx = np.where(np.isin(pid,upid))[0]\n",
    "        orbs = co.get_orbs('stars')[indx]\n",
    "        rs = orbs.r().to(apu.kpc).value\n",
    "        # n_star = len(orbs)\n",
    "\n",
    "        # Get the stellar halo density profile (denspot for the DF)\n",
    "        stellar_halo_density_filename = os.path.join(dens_fitting_dir,\n",
    "            'stellar_halo/',stellar_halo_density_version,str(z0_sid),\n",
    "            'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        denspot = pfit.construct_pot_from_fit(\n",
    "            stellar_halo_density_filename, stellar_halo_densfunc, \n",
    "            stellar_halo_density_ncut, ro=ro, vo=vo)\n",
    "\n",
    "        # Make a grid of r/ra where the individual velocity dispersions \n",
    "        # will be calculated\n",
    "        log_r_ra_min = np.log10( putil.get_softening_length('stars') )\n",
    "        log_r_ra_max = np.log10(rs.max())\n",
    "        r_ra_n = 20\n",
    "        r_ra = (10**np.linspace(log_r_ra_min, log_r_ra_max, r_ra_n,\n",
    "            endpoint=True))*apu.kpc\n",
    "\n",
    "        sigma_r_grid = np.zeros((ra_n,r_ra_n))\n",
    "        sigma_t_grid = np.zeros((ra_n,r_ra_n))\n",
    "        # Loop over the ra and r/ra grid and compute the dispersions\n",
    "        if verbose:\n",
    "            msg = f'Computing velocity dispersions'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "        for k in range(ra_n):\n",
    "\n",
    "            dfom =  pkin.reconstruct_anisotropic_df(df_grid[k],\n",
    "                interpot, denspot)\n",
    "            # Do the fQ -> interpolator trick\n",
    "            dfom.fQ = lambda Q: np.exp(dfom._logfQ_interp(Q))\n",
    "\n",
    "            dens = np.array([\n",
    "                dfom.vmomentdensity(r, 0, 0).value for r in r_ra\n",
    "                ])/apu.kpc**3\n",
    "            svr = np.sqrt(np.array([\n",
    "                dfom.vmomentdensity(r, 2, 0).value for r in r_ra\n",
    "                ])/dens)\n",
    "            svt = np.sqrt(np.array([\n",
    "                dfom.vmomentdensity(r, 0, 2).value for r in r_ra\n",
    "                ])/dens)\n",
    "\n",
    "            sigma_r_grid[k,:] = svr\n",
    "            sigma_t_grid[k,:] = svt\n",
    "\n",
    "        # Save the grid\n",
    "        sigma_grid_filename = os.path.join(this_fitting_dir,'sigma_grid.pkl')\n",
    "        with open(sigma_grid_filename,'wb') as handle:\n",
    "            pickle.dump([sigma_r_grid,sigma_t_grid,ra,r_ra],handle)\n",
    "\n",
    "        # Make some plots\n",
    "        this_fig_dir = os.path.join(fig_dir, str(z0_sid), 'merger_'+str(j+1))\n",
    "        os.makedirs(this_fig_dir,exist_ok=True)\n",
    "        \n",
    "        # Plot the dispersion grid\n",
    "        if verbose:\n",
    "            msg = f'Making dispersion grid plot'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "        fig, axs = make_dispersion_grid_plot(sigma_r_grid, sigma_t_grid,\n",
    "            r_ra, ra)\n",
    "        fig.savefig(os.path.join(this_fig_dir,'sigma_r_t_grid.png'))\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Make a plot of the interpolated dispersions\n",
    "        if verbose:\n",
    "            msg = f'Making interpolated dispersion grid plot'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "        \n",
    "        sigma_r_interp = scipy.interpolate.RegularGridInterpolator(\n",
    "            (ra.value,r_ra.value), sigma_r_grid, method='linear',\n",
    "            bounds_error=False, fill_value=None)\n",
    "        sigma_t_interp = scipy.interpolate.RegularGridInterpolator(\n",
    "            (ra.value,r_ra.value), sigma_t_grid, method='linear',\n",
    "            bounds_error=False, fill_value=None)\n",
    "        \n",
    "        fig, axs = make_interpolated_dispersion_grid_plot(sigma_r_interp,\n",
    "            sigma_t_interp, r_ra, ra)\n",
    "        fig.savefig(os.path.join(this_fig_dir,'sigma_r_t_beta_interp.png'))\n",
    "        plt.close(fig)\n",
    "\n",
    "        if verbose:\n",
    "            msg = f'Done with merger'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "\n",
    "\n",
    "# warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the best-fitting combination Osipkov-Merritt DF using dispersion grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mloglike_beta_dispersion_grid(*args, **kwargs):\n",
    "    return -loglike_beta_dispersion_grid(*args, **kwargs)\n",
    "\n",
    "def loglike_beta_dispersion_grid(params, rs, beta, sigma_r_interp, \n",
    "    sigma_t_interp, sigma=None, mass=None, usr_log_prior=None, \n",
    "    usr_log_prior_params=[], parts=False):\n",
    "    '''\n",
    "    Compute the loglikelihood for a given model using the dispersion grid\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list\n",
    "        The parameters of the model, in order: ra1, ra2, k\n",
    "    rs : numpy.ndarray\n",
    "        The radii where beta is defined for the N-body data\n",
    "    beta : numpy.ndarray\n",
    "        The anisotropy profile for the N-body data\n",
    "    sigma_r_interp : scipy.interpolate.RegularGridInterpolator\n",
    "        The sigma_r grid, indexed as (ra, r)\n",
    "    sigma_t_interp : scipy.interpolate.RegularGridInterpolator\n",
    "        The sigma_t grid, indexed as (ra, r)\n",
    "    sigma : numpy.ndarray, optional\n",
    "        User supplied weights for the each radial bin. If not provided, the\n",
    "        loglikelihood will be unweighted.\n",
    "    mass : numpy.ndarray, optional\n",
    "        The mass contained in each radial bin. To be used as sigma\n",
    "    usr_log_prior : function, optional\n",
    "        A function that returns the log of the prior for the model. If not \n",
    "        provided, the loglikelihood will be unweighted. Takes the parameters\n",
    "        as the argument, and any additional arguments can be passed via the\n",
    "        usr_log_prior_params keyword.\n",
    "    usr_log_prior_params : list, optional\n",
    "        A list of arguments to pass to the usr_log_prior function.\n",
    "    parts : bool, optional\n",
    "        If True, return the individual parts of the loglikelihood calculation.\n",
    "        Default is False.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loglike : float\n",
    "        The loglikelihood of the model given the data.\n",
    "    '''\n",
    "    # Evaluate the domain prior\n",
    "    ra_min, ra_max = sigma_r_interp.grid[0].min(), sigma_r_interp.grid[0].max()\n",
    "    r_min, r_max = sigma_r_interp.grid[1].min(), sigma_r_interp.grid[1].max()\n",
    "    if np.min(rs) < r_min or np.max(rs) > r_max:\n",
    "        warnings.warn('Data outside of interpolation grid.')\n",
    "    if not domain_prior_beta_dispersion_grid(params, ra_min, ra_max):\n",
    "        return -np.inf\n",
    "\n",
    "    # Evaluate the prior on the parameters\n",
    "    # logprior = logprior_beta_dispersion_grid(params)\n",
    "    logprior = 0.\n",
    "\n",
    "    # Evaluate the user supplied log prior\n",
    "    if callable(usr_log_prior):\n",
    "        usrlogprior = usr_log_prior(params, *usr_log_prior_params)\n",
    "        if np.isinf(usrlogprior):\n",
    "            return -np.inf\n",
    "    else:\n",
    "        usrlogprior = 0.\n",
    "\n",
    "    # Compute the anisotropy for the model\n",
    "    ra1, ra2, k = params\n",
    "    svr1 = sigma_r_interp((ra1, rs))\n",
    "    svt1 = sigma_t_interp((ra1, rs))\n",
    "    svr2 = sigma_r_interp((ra2, rs))\n",
    "    svt2 = sigma_t_interp((ra2, rs))\n",
    "    svr = k*svr1**2 + (1-k)*svr2**2\n",
    "    svt = k*svt1**2 + (1-k)*svt2**2\n",
    "    model_beta = 1 - svt/svr/2\n",
    "\n",
    "    # Sigma for the likelihood\n",
    "    if sigma is not None:\n",
    "        _sigma = sigma\n",
    "    elif mass is not None:\n",
    "        mass_frac = mass/np.sum(mass)\n",
    "        _sigma = 1/mass_frac\n",
    "    else:\n",
    "        _sigma = np.ones_like(beta)\n",
    "    \n",
    "    # Compute the log objective\n",
    "    logobj = -0.5*(beta-model_beta)**2/_sigma**2\n",
    "\n",
    "    # Compute the log likelihood\n",
    "    loglike = np.sum(logobj) + logprior + usrlogprior\n",
    "\n",
    "    if parts:\n",
    "        return loglike, logobj, logprior, usrlogprior\n",
    "    else:\n",
    "        return loglike\n",
    "\n",
    "def domain_prior_beta_dispersion_grid(params, ra_min, ra_max):\n",
    "    ra1, ra2, k = params\n",
    "    if ra1 < ra_min or ra1 > ra_max:\n",
    "        return False\n",
    "    if ra2 < ra_min or ra2 > ra_max:\n",
    "        return False\n",
    "    if k < 0 or k > 1:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def generate_mcmc_init(init, nwalkers, domain_prior, scale=0.1):\n",
    "    mcmc_init = np.array([\n",
    "            init+scale*np.random.randn(len(init)) for i in range(nwalkers)\n",
    "            ])\n",
    "    for i in range(nwalkers):\n",
    "        counter = 0\n",
    "        while not domain_prior(mcmc_init[i]):\n",
    "            mcmc_init[i] = init+scale*np.random.randn(len(init))\n",
    "            counter += 1\n",
    "            if counter > 100:\n",
    "                raise RuntimeError('Failed to generate initial conditions.')\n",
    "    return mcmc_init\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some keywords and properties\n",
    "force_fit = True\n",
    "# test_pickling = True\n",
    "verbose = True\n",
    "dens_fitting_dir = os.path.join(fitting_dir_base,'density_profile/')\n",
    "df_fitting_dir = os.path.join(fitting_dir_base,'distribution_function/')\n",
    "\n",
    "# MCMC params\n",
    "nwalkers = 50\n",
    "nit = 1000\n",
    "ncut = 500\n",
    "nprocs = 10\n",
    "n_bin = 500\n",
    "n_bs = 100\n",
    "\n",
    "# Anisotropy information\n",
    "df_type = 'osipkov_merritt_2_combination'\n",
    "anisotropy_fit_version = 'ra_N10_01_to_300_softening'\n",
    "# anisotropy_ncut = 500\n",
    "\n",
    "# Begin logging\n",
    "log_filename = './log/2_fit_om_2_combination_vdisp_fit.log'\n",
    "if os.path.exists(log_filename):\n",
    "    os.remove(log_filename)\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO, filemode='w', \n",
    "    force=True)\n",
    "logging.info('Beginning Osipkov-Merritt 2 combination velocity dispersion'+\\\n",
    "             ' grid anisotropy fitting. Time: '+\\\n",
    "             time.strftime('%a, %d %b %Y %H:%M:%S',time.localtime()))\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i > 0: continue\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    major_mergers = primary.tree_major_mergers\n",
    "    n_major = primary.n_major_mergers\n",
    "    n_snap = len(primary.snapnum)\n",
    "    primary_filename = primary.get_cutout_filename(mw_analog_dir,\n",
    "        snapnum=primary.snapnum[0])\n",
    "    co = pcutout.TNGCutout(primary_filename)\n",
    "    co.center_and_rectify()\n",
    "    pid = co.get_property('stars','ParticleIDs')\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j > 0: continue\n",
    "        if verbose:\n",
    "            msg = f'Analyzing major merger {j+1}/{n_major} for MW {i+1}/{n_mw}'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "\n",
    "        # Filename and pathing check\n",
    "        this_fitting_dir = os.path.join(df_fitting_dir, df_type, \n",
    "            anisotropy_fit_version, str(z0_sid),'merger_'+str(j+1))\n",
    "        os.makedirs(this_fitting_dir,exist_ok=True)\n",
    "        sampler_filename = os.path.join(this_fitting_dir,'sampler.pkl')\n",
    "        if os.path.exists(sampler_filename) and not force_fit:\n",
    "            if verbose:\n",
    "                msg = f'Already have fit model, continuing'\n",
    "                logging.info(msg)\n",
    "                print(msg)\n",
    "            continue\n",
    "        \n",
    "        # Load the dispersion grids\n",
    "        sigma_grid_filename = os.path.join(this_fitting_dir,'sigma_grid.pkl')\n",
    "        with open(sigma_grid_filename,'rb') as handle:\n",
    "            sigma_r_grid,sigma_t_grid,ra,r_ra = pickle.load(handle)\n",
    "\n",
    "        # Convert the dispersion grids into interpolators\n",
    "        sigma_r_interp = scipy.interpolate.RegularGridInterpolator(\n",
    "            (ra.value,r_ra.value), sigma_r_grid, method='linear',\n",
    "            bounds_error=False, fill_value=None)\n",
    "        sigma_t_interp = scipy.interpolate.RegularGridInterpolator(\n",
    "            (ra.value,r_ra.value), sigma_t_grid, method='linear',\n",
    "            bounds_error=False, fill_value=None)\n",
    "\n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_acc_sid = major_merger.subfind_id[0]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "        upid = major_merger.get_unique_particle_ids('stars',data_dir=data_dir)\n",
    "        indx = np.where(np.isin(pid,upid))[0]\n",
    "        orbs = co.get_orbs('stars')[indx]\n",
    "        rs = orbs.r().to(apu.kpc).value\n",
    "\n",
    "        # Bin the data to calculate beta\n",
    "        _n_bin = round(n_bin) if (len(orbs) > 10*n_bin) else round(len(orbs)/10)\n",
    "        r_softening = putil.get_softening_length('stars', z=0, physical=True)\n",
    "        rmin = np.max([r_softening, np.min(rs)])\n",
    "        # rmin = 0.\n",
    "        rmax = np.max(rs)\n",
    "        adaptive_binning_kwargs = {\n",
    "            'n':_n_bin,\n",
    "            'rmin':rmin,\n",
    "            'rmax':rmax,\n",
    "            'bin_mode':'exact numbers',\n",
    "            'bin_equal_n':True,\n",
    "            'end_mode':'ignore',\n",
    "            'bin_cents_mode':'median',\n",
    "        }\n",
    "        bin_edges, bin_cents, bin_n = pkin.get_radius_binning(orbs, \n",
    "            **adaptive_binning_kwargs)\n",
    "        \n",
    "        # Compute the ingredients for the anisotropy, use dispersions\n",
    "        compute_betas_kwargs = {'use_dispersions':True,\n",
    "                                'return_kinematics':True}\n",
    "        beta, vr2, vp2, vz2 = pkin.compute_betas_bootstrap(orbs,bin_edges,\n",
    "            n_bootstrap=n_bs, compute_betas_kwargs=compute_betas_kwargs)\n",
    "        lbeta, mbeta, ubeta = np.percentile(beta, [16,50,84], axis=0)\n",
    "        sbeta = ubeta-lbeta\n",
    "    \n",
    "        # Do the optimization\n",
    "        if verbose:\n",
    "            msg = f'Optimizing to find MCMC start point'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "        init = [1., 10., 0.5]\n",
    "        opt_fn = lambda params: mloglike_beta_dispersion_grid(params, bin_cents, \n",
    "            beta, sigma_r_interp, sigma_t_interp, sigma=sbeta, \n",
    "            usr_log_prior=None, usr_log_prior_params=[], parts=False)\n",
    "        opt = scipy.optimize.minimize(opt_fn, init, method='Powell')# , \n",
    "            # options={'maxiter',1000,})\n",
    "        \n",
    "        # Do MCMC\n",
    "        if verbose:\n",
    "            msg = f'Doing MCMC'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "        def llfunc(params):\n",
    "            return loglike_beta_dispersion_grid(params, bin_cents, beta, \n",
    "                sigma_r_interp, sigma_t_interp, sigma=sbeta, \n",
    "                usr_log_prior=None, usr_log_prior_params=[], parts=False)\n",
    "        mcmc_init = generate_mcmc_init(opt.x, nwalkers, \n",
    "            lambda x: domain_prior_beta_dispersion_grid(x, \n",
    "                ra.value.min(), ra.value.max()))\n",
    "        with multiprocessing.Pool(processes=nprocs) as pool:\n",
    "            sampler = emcee.EnsembleSampler(nwalkers, len(mcmc_init[0]), \n",
    "                llfunc, args=[], pool=pool)\n",
    "            sampler.run_mcmc(mcmc_init, nit, progress=True)\n",
    "        chain = sampler.get_chain(discard=ncut, flat=True, \n",
    "            thin=1)\n",
    "        \n",
    "        # Save the results\n",
    "        opt_filename = os.path.join(this_fitting_dir,'opt.pkl')\n",
    "        with open(opt_filename,'wb') as handle:\n",
    "            pickle.dump(opt,handle)\n",
    "        sampler_filename = os.path.join(this_fitting_dir,'sampler.pkl')\n",
    "        with open(sampler_filename,'wb') as handle:\n",
    "            pickle.dump(sampler,handle)\n",
    "        chain_filename = os.path.join(this_fitting_dir,'chain.pkl')\n",
    "        with open(chain_filename,'wb') as handle:\n",
    "            pickle.dump(chain,handle)\n",
    "\n",
    "        # Make some plots\n",
    "        this_fig_dir = os.path.join(fig_dir, str(z0_sid), 'merger_'+str(j+1))\n",
    "        os.makedirs(this_fig_dir,exist_ok=True)\n",
    "        \n",
    "        # Corner plot\n",
    "        if verbose:\n",
    "            msg = f'Making corner plot'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "        \n",
    "        figc = corner.corner(chain, labels=[r'$r_{a,1}$',r'$r_{a,2}$',r'$k$'],\n",
    "            quantiles=[0.16,0.5,0.84], show_titles=True, truths=opt.x,\n",
    "            truth_color='Red', title_kwargs={'fontsize':12})\n",
    "        figc.tight_layout()\n",
    "        figname = os.path.join(this_fig_dir,'corner_dispersion_grid_fit.png')\n",
    "        figc.savefig(figname)\n",
    "        plt.close(figc)\n",
    "        \n",
    "        # Make a mock beta plot\n",
    "        if verbose:\n",
    "            msg = f'Making beta plot'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "        fig = plt.figure(figsize=(4,4))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(bin_cents, mbeta, color='Black', alpha=1.0)\n",
    "        ax.fill_between(bin_cents, lbeta, ubeta, color='Black', alpha=0.25)\n",
    "        n_plot = 50\n",
    "        for k in range(n_plot):\n",
    "            ra1, ra2, kom = chain[np.random.randint(len(chain))]\n",
    "            svr1 = sigma_r_interp((ra1, bin_cents))\n",
    "            svt1 = sigma_t_interp((ra1, bin_cents))\n",
    "            svr2 = sigma_r_interp((ra2, bin_cents))\n",
    "            svt2 = sigma_t_interp((ra2, bin_cents))\n",
    "            svr = kom*svr1**2 + (1-kom)*svr2**2\n",
    "            svt = kom*svt1**2 + (1-kom)*svt2**2\n",
    "            model_beta = 1 - svt/svr/2\n",
    "            ax.plot(bin_cents, model_beta, color='Red', alpha=0.1)\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_xlabel(r'$r$ [kpc]')\n",
    "        ax.set_ylabel(r'$\\beta$')\n",
    "        fig.tight_layout()\n",
    "        figname = os.path.join(this_fig_dir,'beta_dispersion_grid_fit.png')\n",
    "        fig.savefig(figname)\n",
    "        plt.close(fig)\n",
    "\n",
    "        if verbose:\n",
    "            msg = f'Done with merger'\n",
    "            logging.info(msg)\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute DFs for the best-fitting pair of Osipkov-Merritt scale radii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some keywords and properties\n",
    "force_df_creation = True\n",
    "test_pickling = True\n",
    "verbose = True\n",
    "dens_fitting_dir = os.path.join(fitting_dir_base,'density_profile/')\n",
    "df_fitting_dir = os.path.join(fitting_dir_base,'distribution_function/')\n",
    "\n",
    "# Potential interpolator version\n",
    "interpot_version = 'all_star_dm_enclosed_mass'\n",
    "\n",
    "# Stellar halo density information\n",
    "stellar_halo_density_version = 'poisson_twopower_softening'\n",
    "stellar_halo_density_ncut = 500\n",
    "stellar_halo_densfunc = pdens.TwoPowerSpherical()\n",
    "\n",
    "# Stellar halo rotation information\n",
    "stellar_halo_rotation_version = 'tanh_rotation'\n",
    "stellar_halo_rotation_ncut = 500\n",
    "\n",
    "# Anisotropy information\n",
    "df_type = 'osipkov_merritt_2_combination'\n",
    "anisotropy_fit_version = 'ra_N10_01_to_300_softening'\n",
    "# anisotropy_ncut = 500\n",
    "\n",
    "# Ignore some standard warnings\n",
    "warnings.filterwarnings(action='ignore', \n",
    "    message='No particle IDs found', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', \n",
    "    message='maxiter', category=scipy.integrate.AccuracyWarning)\n",
    "warnings.filterwarnings(action='ignore', \n",
    "    message='invalid value encountered', category=RuntimeWarning)\n",
    "warnings.filterwarnings(action='ignore',\n",
    "    message='subdivisions', category=scipy.integrate.IntegrationWarning)\n",
    "warnings.filterwarnings(action='ignore',\n",
    "    message='divergent', category=scipy.integrate.IntegrationWarning)\n",
    "\n",
    "# Begin logging\n",
    "log_filename = './log/2_fit_om_2_combination_best_fit_dfs.log'\n",
    "if os.path.exists(log_filename):\n",
    "    os.remove(log_filename)\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO, filemode='w', \n",
    "    force=True)\n",
    "logging.info('Beginning Osipkov-Merritt 2 combination DF grid creation. Time: '+\\\n",
    "             time.strftime('%a, %d %b %Y %H:%M:%S',time.localtime()))\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i > 0: continue\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    major_mergers = primary.tree_major_mergers\n",
    "    n_major = primary.n_major_mergers\n",
    "    n_snap = len(primary.snapnum)\n",
    "    primary_filename = primary.get_cutout_filename(mw_analog_dir,\n",
    "        snapnum=primary.snapnum[0])\n",
    "    co = pcutout.TNGCutout(primary_filename)\n",
    "    co.center_and_rectify()\n",
    "    pid = co.get_property('stars','ParticleIDs')\n",
    "\n",
    "    # Load the interpolator for the sphericalized potential\n",
    "    interpolator_filename = os.path.join(dens_fitting_dir,\n",
    "        'spherical_interpolated_potential/',interpot_version,\n",
    "        str(z0_sid),'interp_potential.pkl')\n",
    "    with open(interpolator_filename,'rb') as handle:\n",
    "        interpot = pickle.load(handle)\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j > 0: continue\n",
    "        if verbose:\n",
    "            msg = f'Analyzing major merger {j+1}/{n_major} for MW {i+1}/{n_mw}'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "\n",
    "        # Filename and pathing check\n",
    "        this_fitting_dir = os.path.join(df_fitting_dir, df_type, \n",
    "            anisotropy_fit_version, str(z0_sid),'merger_'+str(j+1))\n",
    "        os.makedirs(this_fitting_dir,exist_ok=True)\n",
    "        df_filename = os.path.join(this_fitting_dir,'df.pkl')\n",
    "        if os.path.exists(df_filename) and not force_df_creation:\n",
    "            if verbose:\n",
    "                msg = f'Already have DF best fits, continuing'\n",
    "                logging.info(msg)\n",
    "                print(msg)\n",
    "            continue\n",
    "            \n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_acc_sid = major_merger.subfind_id[0]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "        upid = major_merger.get_unique_particle_ids('stars',data_dir=data_dir)\n",
    "        indx = np.where(np.isin(pid,upid))[0]\n",
    "        orbs = co.get_orbs('stars')[indx]\n",
    "        rs = orbs.r().to(apu.kpc).value\n",
    "        r_softening = putil.get_softening_length('stars', z=0, physical=True)\n",
    "        rmin = np.max([r_softening, np.min(rs)])\n",
    "        # n_star = len(orbs)\n",
    "\n",
    "        # Get the stellar halo density profile (denspot for the DF)\n",
    "        stellar_halo_density_filename = os.path.join(dens_fitting_dir,\n",
    "            'stellar_halo/',stellar_halo_density_version,str(z0_sid),\n",
    "            'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        denspot = pfit.construct_pot_from_fit(\n",
    "            stellar_halo_density_filename, stellar_halo_densfunc, \n",
    "            stellar_halo_density_ncut, ro=ro, vo=vo)\n",
    "\n",
    "        # Load the best-fits\n",
    "        chain_filename = os.path.join(this_fitting_dir,'chain.pkl')\n",
    "        with open(chain_filename,'rb') as handle:\n",
    "            chain = pickle.load(handle)\n",
    "        ra1, ra2, kom = np.median(chain,axis=0)\n",
    "\n",
    "        ras = [ra1, ra2]\n",
    "        dfs = []\n",
    "        \n",
    "        # Loop over the pair of best-fitting ra and build the DFs\n",
    "        for k in range(len(ras)):\n",
    "            # Construct the distribution function and do some dummy sampling\n",
    "            # to set the interpolators. Then save.\n",
    "            try:\n",
    "                if verbose:\n",
    "                    msg = f'ra: {round(ras[k],3)} kpc, building DF'\n",
    "                    logging.info(msg)\n",
    "                    print(msg)\n",
    "                dfom = df.osipkovmerrittdf(pot=interpot, denspot=denspot, \n",
    "                    ra=ras[k]*apu.kpc, ro=ro, vo=vo, rmax=rs.max()*apu.kpc*1.1)\n",
    "                print('  Sampling DF')\n",
    "                _ = dfom.sample(n=100, rmin=rmin*apu.kpc)\n",
    "            except Exception as e:\n",
    "                msg = f'Failed to build DF, skipping. Error: {e}'\n",
    "                logging.info(msg)\n",
    "                print(msg)\n",
    "\n",
    "            # Filename built above\n",
    "            if test_pickling:\n",
    "                try:\n",
    "                    pickle.loads(pickle.dumps(dfom))\n",
    "                except RecursionError:\n",
    "                    if verbose:\n",
    "                        msg = 'Caught recursion error when (un)pickling, quiting.'\n",
    "                        logging.info(msg)\n",
    "                        print(msg)\n",
    "            # with open(df_filename,'wb') as handle:\n",
    "            #     pickle.dump(dfom,handle)\n",
    "            dfs.append(dfom)\n",
    "\n",
    "        # Save the grid\n",
    "        with open(df_filename,'wb') as handle:\n",
    "            pickle.dump([dfs, ras, kom, 'array of dfs, array of [ra1,ra2], k'],\n",
    "                handle)\n",
    "\n",
    "        if verbose:\n",
    "            msg = f'Done with merger'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "\n",
    "warnings.resetwarnings()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "nteract": {
   "version": "0.28.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "39281e2d1d208f5d5f10b92e49c40383b657448f443f22dc78686b7e0f8179a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
