{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# TITLE - 1.1_sample_anisotropic_dfs.ipynb\n",
    "# AUTHOR - James Lane\n",
    "# PROJECT - tng-dfs\n",
    "#\n",
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# Docstrings and metadata:\n",
    "'''Draw samples and plot fundamental properties of the anisotropic DFs\n",
    "'''\n",
    "\n",
    "__author__ = \"James Lane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../src/nb_modules/nb_imports.txt\n",
    "### Imports\n",
    "\n",
    "## Basic\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import dill as pickle\n",
    "import time, logging\n",
    "\n",
    "## Plotting\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## Astropy\n",
    "from astropy import units as apu\n",
    "\n",
    "## galpy\n",
    "from galpy import potential\n",
    "\n",
    "## Project-specific\n",
    "src_path = 'src/'\n",
    "while True:\n",
    "    if os.path.exists(src_path): break\n",
    "    if os.path.realpath(src_path).split('/')[-1] in ['tng-dfs','/']:\n",
    "            raise FileNotFoundError('Failed to find src/ directory.')\n",
    "    src_path = os.path.join('..',src_path)\n",
    "sys.path.insert(0,src_path)\n",
    "from tng_dfs import cutout as pcutout\n",
    "from tng_dfs import densprofile as pdens\n",
    "from tng_dfs import fitting as pfit\n",
    "from tng_dfs import io as pio\n",
    "from tng_dfs import kinematics as pkin\n",
    "from tng_dfs import util as putil\n",
    "\n",
    "### Notebook setup\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(os.path.join(src_path,'mpl/project.mplstyle')) # This must be exactly here\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords, loading, pathing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../src/nb_modules/nb_setup.txt\n",
    "# Keywords\n",
    "cdict = putil.load_config_to_dict()\n",
    "keywords = ['DATA_DIR','MW_ANALOG_DIR','FIG_DIR_BASE','FITTING_DIR_BASE',\n",
    "            'RO','VO','ZO','LITTLE_H','MW_MASS_RANGE']\n",
    "data_dir,mw_analog_dir,fig_dir_base,fitting_dir_base,ro,vo,zo,h,\\\n",
    "    mw_mass_range = putil.parse_config_dict(cdict,keywords)\n",
    "\n",
    "# MW Analog \n",
    "mwsubs,mwsubs_vars = putil.prepare_mwsubs(mw_analog_dir,h=h,\n",
    "    mw_mass_range=mw_mass_range,return_vars=True,force_mwsubs=False,\n",
    "    bulge_disk_fraction_cuts=True)\n",
    "\n",
    "# Figure path\n",
    "local_fig_dir = './fig/'\n",
    "fig_dir = os.path.join(fig_dir_base, \n",
    "    'notebooks/5_compare_distribution_functions/1.1_sample_anisotropic_dfs')\n",
    "os.makedirs(local_fig_dir,exist_ok=True)\n",
    "os.makedirs(fig_dir,exist_ok=True)\n",
    "show_plots = False\n",
    "\n",
    "# Load tree data\n",
    "tree_primary_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_primaries.pkl')\n",
    "with open(tree_primary_filename,'rb') as handle: \n",
    "    tree_primaries = pickle.load(handle)\n",
    "tree_major_mergers_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_major_mergers.pkl')\n",
    "with open(tree_major_mergers_filename,'rb') as handle:\n",
    "    tree_major_mergers = pickle.load(handle)\n",
    "n_mw = len(tree_primaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ELz(nbody_orbs, sample_orbs, nbody_E, pot, interpot=None, \n",
    "    plot_hist=True, fig=None, axs=None, cmap=mpl.colormaps.get_cmap('rainbow')):\n",
    "    '''Plot energy and angular momentum'''\n",
    "\n",
    "    # Some plotting kwargs\n",
    "    Lz_range = [-3,3]\n",
    "    label_fs = 12\n",
    "    E_range = np.percentile(nbody_E, [0.5,99.5])\n",
    "\n",
    "    # Set up figure\n",
    "    if fig is None or axs is None:\n",
    "        fig = plt.figure(figsize=(12,3))\n",
    "        axs = fig.subplots(nrows=1, ncols=3)\n",
    "\n",
    "    # N-body properties\n",
    "    nbody_Lz = nbody_orbs.Lz().to_value(apu.kpc*apu.km/apu.s)\n",
    "\n",
    "    # Sample properties\n",
    "    sample_Lz = sample_orbs.Lz().to_value(apu.kpc*apu.km/apu.s)\n",
    "    sample_potE = sample_orbs.E(pot=pot).to_value(apu.km**2/apu.s**2)\n",
    "    sample_interE = sample_orbs.E(pot=interpot).to_value(apu.km**2/apu.s**2)\n",
    "\n",
    "    # Angular momentum vs energy\n",
    "    Lzs = [nbody_Lz, sample_Lz, sample_Lz]\n",
    "    energies = [nbody_E, sample_potE, sample_interE]\n",
    "    labels = ['N-body',\n",
    "            'Samples (interpolated pot)', \n",
    "            'Samples (Best-fit pot)']\n",
    "    for k in range(3):\n",
    "        E_range = [np.nanmin(energies[k])/1e5, np.nanmax(energies[k])/1e5]\n",
    "        if plot_hist:\n",
    "            H, xedges, yedges = np.histogram2d(Lzs[k]/1e3, energies[k]/1e5, \n",
    "                bins=[45,30], range=[Lz_range,E_range])\n",
    "            H = np.rot90(H)\n",
    "            H = np.flipud(H)\n",
    "            Hmasked = np.log10(np.ma.masked_where(H==0,H))\n",
    "            if k == 0:\n",
    "                vmin = np.nanmin(Hmasked)\n",
    "                vmax = np.nanmax(Hmasked)\n",
    "            cmap.set_bad(color='white')\n",
    "            axs[k].pcolormesh(xedges,yedges,Hmasked,cmap=cmap,vmin=vmin,\n",
    "                vmax=vmax)        \n",
    "        else:\n",
    "            axs[k].scatter(Lzs[k], energies[k], s=1, color='Black', alpha=0.1)\n",
    "        \n",
    "        # Decorate\n",
    "        axs[k].axvline(0, linestyle='dashed', linewidth=1., color='Grey')\n",
    "        _annotate_bbox_kwargs = dict(facecolor='White', edgecolor='Black', \n",
    "            fill=True, alpha=0.5)\n",
    "        axs[k].annotate(labels[k], xy=(0.05,0.05), xycoords='axes fraction',\n",
    "            fontsize=8, bbox=_annotate_bbox_kwargs)\n",
    "        axs[k].set_xlabel(r'Lz [$10^{3}$ kpc km/s]', fontsize=label_fs)\n",
    "        axs[k].set_ylabel(r'E [$10^{5}$ km$^{2}$/s$^{2}$]', fontsize=label_fs)\n",
    "        \n",
    "        axs[k].set_xlim(Lz_range[0],Lz_range[1])\n",
    "        axs[k].set_ylim(E_range[0],E_range[1])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig, axs\n",
    "\n",
    "def plot_beta_vdisp(nbody_orbs, sample_orbs, n_bs=100, fig=None, axs=None):\n",
    "    '''Plot the beta and then the velocity dispersion'''\n",
    "\n",
    "    # Some kwargs for plotting\n",
    "    nbody_color = 'Black'\n",
    "    sample_color = 'DodgerBlue'\n",
    "\n",
    "    # Set up figure\n",
    "    if fig is None or axs is None:\n",
    "        fig = plt.figure(figsize=(5,12))\n",
    "        axs = fig.subplots(nrows=4, ncols=1)\n",
    "\n",
    "    # Binning for velocity dispersions and betas\n",
    "    n_bin = np.min([500, len(nbody_orbs)//10]) # n per bin\n",
    "    adaptive_binning_kwargs = {\n",
    "        'n':n_bin,\n",
    "        'rmin':0.,\n",
    "        'rmax':np.max( nbody_orbs.r().to_value(apu.kpc) ),\n",
    "        'bin_mode':'exact numbers',\n",
    "        'bin_equal_n':True,\n",
    "        'end_mode':'ignore',\n",
    "        'bin_cents_mode':'median',\n",
    "    }\n",
    "    bin_edges, bin_cents, bin_n = pkin.get_radius_binning(nbody_orbs, \n",
    "        **adaptive_binning_kwargs)\n",
    "\n",
    "    # Compute velocity dispersions for N-body\n",
    "    compute_betas_kwargs = {'use_dispersions':True,\n",
    "                            'return_kinematics':True}\n",
    "    nbody_beta, nbody_vr2, nbody_vp2, nbody_vz2 = \\\n",
    "        pkin.compute_betas_bootstrap(nbody_orbs, bin_edges, n_bootstrap=n_bs, \n",
    "        compute_betas_kwargs=compute_betas_kwargs)\n",
    "\n",
    "    # Compute velocity dispersions for the DF samples\n",
    "    compute_betas_kwargs = {'use_dispersions':True,\n",
    "                            'return_kinematics':True}\n",
    "    sample_beta, sample_vr2, sample_vp2, sample_vz2 = \\\n",
    "        pkin.compute_betas_bootstrap(sample_orbs, bin_edges, n_bootstrap=n_bs, \n",
    "        compute_betas_kwargs=compute_betas_kwargs)\n",
    "\n",
    "    # Beta for the N-body\n",
    "    axs[0].plot(bin_cents, np.median(nbody_beta, axis=0), color=nbody_color, \n",
    "        label='N-body')\n",
    "    axs[0].fill_between(bin_cents, np.percentile(nbody_beta, 16, axis=0),\n",
    "        np.percentile(nbody_beta, 84, axis=0), color=nbody_color, alpha=0.25)\n",
    "\n",
    "    # Beta for the DF samples\n",
    "    axs[0].plot(bin_cents, np.median(sample_beta, axis=0), color=sample_color, \n",
    "        label='DF Samples')\n",
    "    axs[0].fill_between(bin_cents, np.percentile(sample_beta, 16, axis=0),\n",
    "        np.percentile(sample_beta, 84, axis=0), color=sample_color, alpha=0.25)\n",
    "\n",
    "    # Velocity dispersions for the N-body\n",
    "    sv2s = [nbody_vr2,nbody_vp2,nbody_vz2]\n",
    "    v_suffixes = ['r','\\phi','z']\n",
    "    for k in range(3):\n",
    "        axs[k+1].plot(bin_cents, np.sqrt(np.median(sv2s[k], axis=0)), \n",
    "            color=nbody_color)\n",
    "        axs[k+1].fill_between(bin_cents, \n",
    "            np.sqrt(np.percentile(sv2s[k], 16, axis=0)),\n",
    "            np.sqrt(np.percentile(sv2s[k], 84, axis=0)), \n",
    "            color=nbody_color, alpha=0.25)\n",
    "        axs[k+1].set_ylabel(r'$\\sigma_'+v_suffixes[k]+r'$')\n",
    "\n",
    "    # Velocity dispersions for the DF samples\n",
    "    sv2s = [sample_vr2,sample_vp2,sample_vz2]\n",
    "    v_suffixes = ['r','\\phi','z']\n",
    "    for k in range(3):\n",
    "        axs[k+1].plot(bin_cents, np.sqrt(np.median(sv2s[k], axis=0)), \n",
    "            color=sample_color)\n",
    "        axs[k+1].fill_between(bin_cents, \n",
    "            np.sqrt(np.percentile(sv2s[k], 16, axis=0)),\n",
    "            np.sqrt(np.percentile(sv2s[k], 84, axis=0)), \n",
    "            color=sample_color, alpha=0.25)\n",
    "        axs[k+1].set_ylabel(r'$\\sigma_'+v_suffixes[k]+r'$')\n",
    "\n",
    "    # Labels\n",
    "    axs[0].set_ylabel(r'$\\beta$')\n",
    "    axs[0].legend()\n",
    "    for k in range(4):\n",
    "        axs[k].set_xscale('log')\n",
    "        axs[k].set_xlabel(r'$r$ [kpc]')\n",
    "        if k > 0:\n",
    "            axs[k].set_yscale('log')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig, axs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample and plot all DFs\n",
    "- Constant anisotropy\n",
    "- Osipkov-Merritt\n",
    "- Osipkov-Merritt linear combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "dens_fitting_dir = os.path.join(fitting_dir_base,'density_profile')\n",
    "df_fitting_dir = os.path.join(fitting_dir_base,'distribution_function')\n",
    "analysis_version = 'v1.1'\n",
    "analysis_dir = os.path.join(mw_analog_dir,'analysis',analysis_version)\n",
    "\n",
    "# Begin logging\n",
    "log_filename = './log/1.1_sample_anisotropic_dfs.log'\n",
    "if os.path.exists(log_filename):\n",
    "    os.remove(log_filename)\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO, filemode='w', \n",
    "    force=True)\n",
    "logging.info('Beginning anisotropic DF sample creation and plotting. Time: '+\\\n",
    "             time.strftime('%a, %d %b %Y %H:%M:%S',time.localtime()))\n",
    "\n",
    "# Potential interpolator version\n",
    "interpot_version = 'all_star_dm_enclosed_mass'\n",
    "\n",
    "# DM halo information\n",
    "dm_halo_version = 'poisson_nfw'\n",
    "dm_halo_ncut = 500\n",
    "\n",
    "# Stellar bulge and disk information\n",
    "stellar_bulge_disk_version = 'miyamoto_disk_pswc_bulge_tps_halo'\n",
    "stellar_bulge_disk_ncut = 2000\n",
    "\n",
    "# Stellar halo density information\n",
    "stellar_halo_density_version = 'poisson_twopower_softening'\n",
    "stellar_halo_density_ncut = 500\n",
    "\n",
    "# Stellar halo rotation information\n",
    "stellar_halo_rotation_dftype = 'tanh_rotation'\n",
    "stellar_halo_rotation_version = 'asymmetry_fit'\n",
    "stellar_halo_rotation_ncut = 500\n",
    "\n",
    "# Define density profiles\n",
    "dm_halo_densfunc = pdens.NFWSpherical()\n",
    "disk_densfunc = pdens.MiyamotoNagaiDisk()\n",
    "bulge_densfunc = pdens.SinglePowerCutoffSpherical()\n",
    "stellar_halo_densfunc = pdens.TwoPowerSpherical()\n",
    "stellar_bulge_disk_densfunc = pdens.CompositeDensityProfile(\n",
    "    [disk_densfunc,\n",
    "     bulge_densfunc,\n",
    "     pdens.TwoPowerSpherical()]\n",
    "     )\n",
    "\n",
    "sample_data_cb = []\n",
    "sample_data_om = []\n",
    "sample_data_om2 = []\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i > 0: continue\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    major_mergers = primary.tree_major_mergers\n",
    "    n_major = primary.n_major_mergers\n",
    "    n_snap = len(primary.snapnum)\n",
    "    primary_filename = primary.get_cutout_filename(mw_analog_dir,\n",
    "        snapnum=primary.snapnum[0])\n",
    "    co = pcutout.TNGCutout(primary_filename)\n",
    "    co.center_and_rectify()\n",
    "    pid = co.get_property('stars','ParticleIDs')\n",
    "\n",
    "    # Get the dark halo\n",
    "    dm_halo_filename = os.path.join(dens_fitting_dir,'dm_halo/',dm_halo_version,\n",
    "        str(z0_sid), 'sampler.pkl')\n",
    "    dm_halo_pot = pfit.construct_pot_from_fit(dm_halo_filename,\n",
    "        dm_halo_densfunc, dm_halo_ncut, ro=ro, vo=vo)\n",
    "    \n",
    "    # Get the stellar bulge and disk\n",
    "    stellar_bulge_disk_filename = os.path.join(dens_fitting_dir,\n",
    "        'stellar_bulge_disk/',stellar_bulge_disk_version,str(z0_sid),\n",
    "        'sampler.pkl')\n",
    "    stellar_pots = pfit.construct_pot_from_fit(stellar_bulge_disk_filename,\n",
    "        stellar_bulge_disk_densfunc, stellar_bulge_disk_ncut, ro=ro, vo=vo)\n",
    "    fpot = [stellar_pots[1], stellar_pots[0], dm_halo_pot] # bulge, disk, halo\n",
    "\n",
    "    # Load the interpolator for the sphericalized potential\n",
    "    interpolator_filename = os.path.join(dens_fitting_dir,\n",
    "        'spherical_interpolated_potential/',interpot_version,\n",
    "        str(z0_sid),'interp_potential.pkl')\n",
    "    with open(interpolator_filename,'rb') as handle:\n",
    "        interpot = pickle.load(handle)\n",
    "\n",
    "    for j in range(n_major):\n",
    "        if verbose: \n",
    "            msg = f'Constructing DF samples for MW analog {i+1}/{n_mw}'+\\\n",
    "                  f' merger {j+1}/{n_major}'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "\n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_acc_sid = major_merger.subfind_id[0]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "        upid = major_merger.get_unique_particle_ids('stars',data_dir=data_dir)\n",
    "        indx = np.where(np.isin(pid,upid))[0]\n",
    "        orbs = co.get_orbs('stars')[indx]\n",
    "        rs = orbs.r().to(apu.kpc).value\n",
    "        masses = co.get_masses('stars')[indx].to_value(apu.Msun)\n",
    "        n_star = len(orbs)\n",
    "        star_mass = co.get_masses('stars')[indx].to_value(apu.Msun)\n",
    "        pe = co.get_potential_energy('stars')[indx].to_value(apu.km**2/apu.s**2)\n",
    "        vels = co.get_velocities('stars')[indx].to_value(apu.km/apu.s)\n",
    "        vmag = np.linalg.norm(vels,axis=1)\n",
    "        energy = pe + 0.5*vmag**2\n",
    "        Lz = orbs.Lz().to_value(apu.kpc*apu.km/apu.s)\n",
    "\n",
    "        # Mask the input orbits to only include those with radius greater\n",
    "        # than the softening length\n",
    "        r_softening = putil.get_softening_length('stars', z=0, physical=True)\n",
    "        mask = rs > r_softening\n",
    "        orbs = orbs[mask]\n",
    "        rs = rs[mask]\n",
    "        masses = masses[mask]\n",
    "        n_star = len(orbs)\n",
    "        star_mass = star_mass[mask]\n",
    "        pe = pe[mask]\n",
    "        vels = vels[mask]\n",
    "        vmag = vmag[mask]\n",
    "        energy = energy[mask]\n",
    "        Lz = Lz[mask]\n",
    "\n",
    "        # Get the stellar halo density profile (denspot for the DF)\n",
    "        stellar_halo_density_filename = os.path.join(dens_fitting_dir,\n",
    "            'stellar_halo/',stellar_halo_density_version,str(z0_sid),\n",
    "            'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        denspot = pfit.construct_pot_from_fit(\n",
    "            stellar_halo_density_filename, stellar_halo_densfunc, \n",
    "            stellar_halo_density_ncut, ro=ro, vo=vo)\n",
    "        \n",
    "        # Get the stellar halo rotation kernel\n",
    "        stellar_halo_rotation_filename = os.path.join(df_fitting_dir,\n",
    "            stellar_halo_rotation_dftype,stellar_halo_rotation_version,\n",
    "            str(z0_sid),'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        stellar_halo_k, stellar_halo_chi = \\\n",
    "            pio.median_params_from_emcee_sampler(stellar_halo_rotation_filename,\n",
    "                ncut=stellar_halo_rotation_ncut)\n",
    "        \n",
    "        ### Constant beta DF ###\n",
    "        if verbose:\n",
    "            msg = 'Sampling constant beta DF'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "        df_type = 'constant_beta'\n",
    "        df_version = 'df_density_softening'\n",
    "        try:\n",
    "            # Load the distribution function and wrangle\n",
    "            df_filename = os.path.join(df_fitting_dir,df_type,df_version,\n",
    "                str(z0_sid),'merger_'+str(j+1),'df.pkl')\n",
    "            with open(df_filename,'rb') as handle:\n",
    "                dfcb = pickle.load(handle)\n",
    "            dfcb = pkin.reconstruct_anisotropic_df(dfcb, interpot, denspot)\n",
    "        \n",
    "            # Create sample and apply rotation\n",
    "            sample = dfcb.sample(n=n_star, rmin=rs.min()*apu.kpc*0.9)\n",
    "            sample = pkin.rotate_df_samples(sample,stellar_halo_k,\n",
    "                stellar_halo_chi)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                msg = 'Caught an error when loading DF / sampling: '+str(e)+\\\n",
    "                    ' continuing'\n",
    "                logging.info(msg)\n",
    "                print(msg)\n",
    "            continue\n",
    "\n",
    "        _data_cb = (z0_sid,\n",
    "                    major_acc_sid,\n",
    "                    major_mlpid,\n",
    "                    j+1, # merger number\n",
    "                    sample)\n",
    "        sample_data_cb.append(_data_cb)\n",
    "\n",
    "        # Scale the N-body energies by the potential energy of interpot at the \n",
    "        # stellar half-mass radius\n",
    "        rhalf = pkin.half_mass_radius(rs, masses)\n",
    "        rhalf_perc = 0.05\n",
    "        rhalf_mask = np.abs(rs-rhalf) < rhalf_perc*rhalf\n",
    "        rhalf_pe_star = np.median(pe[rhalf_mask])\n",
    "        rhalf_pe_interpot = potential.evaluatePotentials(\n",
    "            interpot, rhalf*apu.kpc, 0.).to_value(apu.km**2/apu.s**2)\n",
    "        rhalf_pe_offset = rhalf_pe_star - rhalf_pe_interpot\n",
    "\n",
    "        # Plotting\n",
    "        this_fig_dir = os.path.join(fig_dir, df_type, df_version, str(z0_sid), \n",
    "            'merger_'+str(j+1))\n",
    "        os.makedirs(this_fig_dir,exist_ok=True)\n",
    "\n",
    "        fig,axs = plot_ELz(orbs, sample, energy-rhalf_pe_offset, fpot, \n",
    "            interpot)\n",
    "        fig.tight_layout()\n",
    "        figname = os.path.join(this_fig_dir,'energy_Lz.png')\n",
    "        fig.savefig(figname, dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        fig,axs = plot_beta_vdisp(orbs, sample)\n",
    "        for ax in axs: ax.axvline(r_softening, linestyle='dashed', \n",
    "            linewidth=1., color='Grey')\n",
    "        fig.tight_layout()\n",
    "        figname = os.path.join(this_fig_dir,'velocity_dispersions.png')\n",
    "        fig.savefig(figname, dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        ### Osipkov-Merritt DF ###\n",
    "        if verbose:\n",
    "            msg = 'Sampling Osipkov-Merritt DF'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "        df_type = 'osipkov_merritt'\n",
    "        df_version = 'df_density_softening'\n",
    "        try:\n",
    "            # Load the distribution function and wrangle\n",
    "            df_filename = os.path.join(df_fitting_dir,df_type,df_version,\n",
    "                str(z0_sid),'merger_'+str(j+1),'df.pkl')\n",
    "            with open(df_filename,'rb') as handle:\n",
    "                dfom = pickle.load(handle)\n",
    "            dfom = pkin.reconstruct_anisotropic_df(dfom, interpot, denspot)\n",
    "        \n",
    "            # Create sample and apply rotation\n",
    "            sample = dfom.sample(n=n_star, rmin=rs.min()*apu.kpc*0.9)\n",
    "            sample = pkin.rotate_df_samples(sample,stellar_halo_k,\n",
    "                stellar_halo_chi)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                msg = 'Caught an error when loading DF / sampling: '+str(e)+\\\n",
    "                    ' continuing'\n",
    "                logging.info(msg)\n",
    "                print(msg)\n",
    "            continue\n",
    "\n",
    "        _data_om = (z0_sid,\n",
    "                    major_acc_sid,\n",
    "                    major_mlpid,\n",
    "                    j+1, # merger number\n",
    "                    sample)\n",
    "        sample_data_om.append(_data_om)\n",
    "\n",
    "        # Scale the N-body energies by the potential energy of interpot at the \n",
    "        # stellar half-mass radius\n",
    "        rhalf = pkin.half_mass_radius(rs, masses)\n",
    "        rhalf_perc = 0.05\n",
    "        rhalf_mask = np.abs(rs-rhalf) < rhalf_perc*rhalf\n",
    "        rhalf_pe_star = np.median(pe[rhalf_mask])\n",
    "        rhalf_pe_interpot = potential.evaluatePotentials(\n",
    "            interpot, rhalf*apu.kpc, 0.).to_value(apu.km**2/apu.s**2)\n",
    "        rhalf_pe_offset = rhalf_pe_star - rhalf_pe_interpot\n",
    "\n",
    "        # Plotting\n",
    "        this_fig_dir = os.path.join(fig_dir, df_type, df_version, str(z0_sid), \n",
    "            'merger_'+str(j+1))\n",
    "        os.makedirs(this_fig_dir,exist_ok=True)\n",
    "\n",
    "        fig,axs = plot_ELz(orbs, sample, energy-rhalf_pe_offset, fpot, \n",
    "            interpot)\n",
    "        fig.tight_layout()\n",
    "        figname = os.path.join(this_fig_dir,'energy_Lz.png')\n",
    "        fig.savefig(figname, dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        fig,axs = plot_beta_vdisp(orbs, sample)\n",
    "        for ax in axs: ax.axvline(r_softening, linestyle='dashed', \n",
    "            linewidth=1., color='Grey')\n",
    "        fig.tight_layout()\n",
    "        figname = os.path.join(this_fig_dir,'velocity_dispersions.png')\n",
    "        fig.savefig(figname, dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        ### Osipkov-Merritt linear combination DF ###\n",
    "        if verbose:\n",
    "            msg = 'Sampling Osipkov-Merritt linear combination DF'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "        df_type = 'osipkov_merritt_2_combination'\n",
    "        df_version = 'ra_N10_01_to_300_softening'\n",
    "        try:\n",
    "            # Load the distribution function and wrangle\n",
    "            df_filename = os.path.join(df_fitting_dir,df_type,df_version,\n",
    "                str(z0_sid),'merger_'+str(j+1),'df.pkl')\n",
    "            with open(df_filename,'rb') as handle:\n",
    "                data = pickle.load(handle)\n",
    "            dfoms = data[0]\n",
    "            ras = data[1]\n",
    "            kom = data[2]\n",
    "            ns = [int(kom*n_star), n_star-int(kom*n_star)]\n",
    "            samples = []\n",
    "            for k in range(len(dfoms)):\n",
    "                dfoms[k] = pkin.reconstruct_anisotropic_df(dfoms[k], interpot, \n",
    "                    denspot)\n",
    "                _sample = dfoms[k].sample(n=ns[k], rmin=rs.min()*apu.kpc*0.9)\n",
    "                _sample = pkin.rotate_df_samples(_sample,stellar_halo_k,\n",
    "                    stellar_halo_chi)\n",
    "                samples.append(_sample)\n",
    "            sample = putil.join_orbs(samples)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                msg = 'Caught an error when loading DF / sampling: '+str(e)+\\\n",
    "                    ' continuing'\n",
    "                logging.info(msg)\n",
    "                print(msg)\n",
    "            continue\n",
    "        \n",
    "        _data_om2 = (z0_sid,\n",
    "                    major_acc_sid,\n",
    "                    major_mlpid,\n",
    "                    j+1, # merger number\n",
    "                    sample)\n",
    "        sample_data_om2.append(_data_om2)\n",
    "\n",
    "        # Plotting\n",
    "        this_fig_dir = os.path.join(fig_dir, df_type, df_version, str(z0_sid), \n",
    "            'merger_'+str(j+1))\n",
    "        os.makedirs(this_fig_dir,exist_ok=True)\n",
    "\n",
    "        fig,axs = plot_ELz(orbs, sample, energy-rhalf_pe_offset, fpot,\n",
    "            interpot)\n",
    "        fig.tight_layout()\n",
    "        figname = os.path.join(this_fig_dir,'energy_Lz.png')\n",
    "        fig.savefig(figname, dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "        fig,axs = plot_beta_vdisp(orbs, sample)\n",
    "        for ax in axs: \n",
    "            ax.axvline(r_softening, linestyle='dashed', \n",
    "                linewidth=1., color='Grey')\n",
    "            for ra in ras: ax.axvline(ra, linestyle='dotted', \n",
    "                linewidth=1., color='Black')\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        figname = os.path.join(this_fig_dir,'velocity_dispersions.png')\n",
    "        fig.savefig(figname, dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "# Save the data as a pickle\n",
    "header = ['z0_sid','major_acc_sid','major_mlpid','merger_number','sample']\n",
    "with open(os.path.join(analysis_dir,'sample_data_cb.pkl'),'wb') as handle:\n",
    "    pickle.dump([header,sample_data_cb],handle)\n",
    "with open(os.path.join(analysis_dir,'sample_data_om.pkl'),'wb') as handle:\n",
    "    pickle.dump([header,sample_data_om],handle)\n",
    "with open(os.path.join(analysis_dir,'sample_data_om2.pkl'),'wb') as handle:\n",
    "    pickle.dump([header,sample_data_om2],handle)\n",
    "\n",
    "# Also save as a structured array\n",
    "dtype = [('z0_sid',int),\n",
    "         ('major_acc_sid',int),\n",
    "         ('major_mlpid',int),\n",
    "         ('merger_number',int),\n",
    "         ('sample',object)]\n",
    "sample_data_cb_rec = np.array(sample_data_cb, dtype=dtype)\n",
    "sample_data_om_rec = np.array(sample_data_om, dtype=dtype)\n",
    "sample_data_om2_rec = np.array(sample_data_om2, dtype=dtype)\n",
    "np.save(os.path.join(analysis_dir,'sample_data_cb.npy'),sample_data_cb_rec)\n",
    "np.save(os.path.join(analysis_dir,'sample_data_om.npy'),sample_data_om_rec)\n",
    "np.save(os.path.join(analysis_dir,'sample_data_om2.npy'),sample_data_om2_rec)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
