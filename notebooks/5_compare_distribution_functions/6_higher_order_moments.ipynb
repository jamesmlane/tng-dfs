{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# TITLE - 6_higher_order_moments.ipynb\n",
    "# AUTHOR - James Lane\n",
    "# PROJECT - tng-dfs\n",
    "#\n",
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# Docstrings and metadata:\n",
    "'''Compute the higher order moments of the data and the DF realizations\n",
    "'''\n",
    "\n",
    "__author__ = \"James Lane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../src/nb_modules/nb_imports.txt\n",
    "### Imports\n",
    "\n",
    "## Basic\n",
    "import numpy as np\n",
    "import sys, os, dill as pickle, logging, time\n",
    "\n",
    "## Matplotlib\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## Astropy\n",
    "from astropy import units as apu\n",
    "from astropy import constants as apc\n",
    "\n",
    "## Analysis\n",
    "import scipy.stats\n",
    "import scipy.interpolate\n",
    "\n",
    "## galpy\n",
    "from galpy import orbit\n",
    "from galpy import potential\n",
    "\n",
    "## Project-specific\n",
    "src_path = 'src/'\n",
    "while True:\n",
    "    if os.path.exists(src_path): break\n",
    "    if os.path.realpath(src_path).split('/')[-1] in ['tng-dfs','/']:\n",
    "            raise FileNotFoundError('Failed to find src/ directory.')\n",
    "    src_path = os.path.join('..',src_path)\n",
    "sys.path.insert(0,src_path)\n",
    "from tng_dfs import cutout as pcutout\n",
    "from tng_dfs import densprofile as pdens\n",
    "from tng_dfs import fitting as pfit\n",
    "from tng_dfs import kinematics as pkin\n",
    "from tng_dfs import plot as pplot\n",
    "from tng_dfs import util as putil\n",
    "\n",
    "### Notebook setup\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(os.path.join(src_path,'mpl/project.mplstyle')) # This must be exactly here\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords, loading, pathing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../src/nb_modules/nb_setup.txt\n",
    "# Keywords\n",
    "cdict = putil.load_config_to_dict()\n",
    "keywords = ['DATA_DIR','MW_ANALOG_DIR','FIG_DIR_BASE','FITTING_DIR_BASE',\n",
    "            'RO','VO','ZO','LITTLE_H','MW_MASS_RANGE']\n",
    "data_dir,mw_analog_dir,fig_dir_base,fitting_dir_base,ro,vo,zo,h,\\\n",
    "    mw_mass_range = putil.parse_config_dict(cdict,keywords)\n",
    "\n",
    "# MW Analog \n",
    "mwsubs,mwsubs_vars = putil.prepare_mwsubs(mw_analog_dir,h=h,\n",
    "    mw_mass_range=mw_mass_range,return_vars=True,force_mwsubs=False,\n",
    "    bulge_disk_fraction_cuts=True)\n",
    "\n",
    "# Figure path\n",
    "local_fig_dir = './fig/'\n",
    "fig_dir = os.path.join(fig_dir_base, \n",
    "    'notebooks/5_compare_distribution_functions/6_higher_order_moments/')\n",
    "os.makedirs(local_fig_dir,exist_ok=True)\n",
    "os.makedirs(fig_dir,exist_ok=True)\n",
    "show_plots = False\n",
    "\n",
    "# Load tree data\n",
    "tree_primary_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_primaries.pkl')\n",
    "with open(tree_primary_filename,'rb') as handle: \n",
    "    tree_primaries = pickle.load(handle)\n",
    "tree_major_mergers_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_major_mergers.pkl')\n",
    "with open(tree_major_mergers_filename,'rb') as handle:\n",
    "    tree_major_mergers = pickle.load(handle)\n",
    "n_mw = len(tree_primaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vmoment(orbs, moment, bin_edges, n_bootstrap=100, \n",
    "    stdev_normalization=True, pearson_correction=True):\n",
    "    '''compute_vmoment:\n",
    "    \n",
    "    Wrapper for compute_standard_vmoment() and compute_mean_std_vmoment()\n",
    "\n",
    "    Args:\n",
    "        orbs (galpy.orbit.Orbit): Orbits\n",
    "        moment (int): Moment to compute\n",
    "        bin_edges (np.ndarray): Bin edges\n",
    "        n_bootstrap (int): Number of times to bootstrap the orbits to compute\n",
    "            the moment for error estimation\n",
    "        kwargs (dict): kwargs for compute_standard_vmoment() or \n",
    "            compute_mean_std_vmoment()\n",
    "    '''\n",
    "    if moment in [1,2,3,4]:\n",
    "        return compute_standard_vmoment(orbs, moment, bin_edges, \n",
    "            n_bootstrap=n_bootstrap, stdev_normalization=stdev_normalization,\n",
    "            pearson_correction=pearson_correction)\n",
    "    elif moment in ['mean','std','meansq']:\n",
    "        return compute_mean_std_vmoment(orbs, moment, bin_edges, \n",
    "            n_bootstrap=n_bootstrap)\n",
    "\n",
    "def compute_standard_vmoment(orbs, n, bin_edges, n_bootstrap=100, \n",
    "    stdev_normalization=True, pearson_correction=True):\n",
    "    '''compute_vmoment:\n",
    "    \n",
    "    Compute the nth standard moment of the spherical velocity distribution in \n",
    "    bins.\n",
    "    \n",
    "    Args:\n",
    "        orbs (galpy.orbit.Orbit): Orbits\n",
    "        n (int): Moment to compute\n",
    "        bin_edges (np.ndarray): Bin edges\n",
    "        n_bootstrap (int): Number of times to bootstrap the orbits to compute\n",
    "            the moment for error estimation\n",
    "        stdev_normalization (bool): If True, normalize the moment \n",
    "        pearson_correction (bool): If True, apply Pearson's correction to the \n",
    "            4th standardized moment such that it gives excess kurtosis.\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "        v_moment (np.ndarray): Velocity moments, shape (3, len(bin_edges)-1, n_bs)\n",
    "    '''\n",
    "    r = orbs.r().to_value(apu.kpc)\n",
    "    vr = orbs.vr().to_value(apu.km/apu.s)\n",
    "    vt = orbs.vtheta().to_value(apu.km/apu.s)\n",
    "    vp = orbs.vT().to_value(apu.km/apu.s)\n",
    "\n",
    "    vmom = np.zeros((3,n_bootstrap,len(bin_edges)-1))\n",
    "    if n == 1:\n",
    "        vmom[:,:,:] = 0 # Definitionaly\n",
    "        return vmom\n",
    "    if n == 2 and stdev_normalization:\n",
    "        vmom[:,:,:] = 1 # Definitionaly\n",
    "        return vmom\n",
    "    n_orbs = len(orbs)\n",
    "\n",
    "    for i in range(n_bootstrap):\n",
    "        idx = np.random.randint(0,n_orbs,n_orbs)\n",
    "\n",
    "        for j in range(len(bin_edges)-1):\n",
    "            bidx = (r[idx] > bin_edges[j]) & (r[idx] < bin_edges[j+1])\n",
    "            if np.sum(bidx) == 0:\n",
    "                vmom[:,i,j] = np.nan\n",
    "                continue\n",
    "            for k,v in enumerate([vr,vp,vt]):\n",
    "                _vmom = np.mean((v[idx][bidx]-np.mean(v[idx][bidx]))**n)\n",
    "                if stdev_normalization:\n",
    "                    _std = np.std(v[idx][bidx])\n",
    "                    _vmom /= _std**n\n",
    "                vmom[k,i,j] = _vmom\n",
    "    \n",
    "    if n == 4 and stdev_normalization and pearson_correction:\n",
    "        vmom -= 3.\n",
    "\n",
    "    return vmom\n",
    "\n",
    "def compute_mean_std_vmoment(orbs, moment, bin_edges, n_bootstrap=100):\n",
    "    '''compute_mean_std_vmoment:\n",
    "    \n",
    "    Compute the usual spherical velocity mean/std with the same interface as \n",
    "    compute_standard_vmoment()\n",
    "    \n",
    "    Args:\n",
    "        orbs (galpy.orbit.Orbit): Orbits\n",
    "        moment (str): 'mean', 'std', 'meansq'\n",
    "        bin_edges (np.ndarray): Bin edges\n",
    "        n_bootstrap (int): Number of times to bootstrap the orbits to compute\n",
    "            the moment for error estimation\n",
    "    \n",
    "    Returns:\n",
    "        v_moment (np.ndarray): Velocity means, shape (3, len(bin_edges)-1, n_bs)\n",
    "    '''\n",
    "    assert moment in ['mean','std','meansq']\n",
    "    r = orbs.r().to_value(apu.kpc)\n",
    "    vr = orbs.vr().to_value(apu.km/apu.s)\n",
    "    vt = orbs.vtheta().to_value(apu.km/apu.s)\n",
    "    vp = orbs.vT().to_value(apu.km/apu.s)\n",
    "    vmom = np.zeros((3,n_bootstrap,len(bin_edges)-1))\n",
    "    n_orbs = len(orbs)\n",
    "\n",
    "    for i in range(n_bootstrap):\n",
    "        idx = np.random.randint(0,n_orbs,n_orbs)\n",
    "\n",
    "        for j in range(len(bin_edges)-1):\n",
    "            bidx = (r[idx] > bin_edges[j]) & (r[idx] < bin_edges[j+1])\n",
    "            if np.sum(bidx) == 0:\n",
    "                vmom[:,i,j] = np.nan\n",
    "                continue\n",
    "            for k,v in enumerate([vr,vp,vt]):\n",
    "                if moment == 'mean':\n",
    "                    vmom[k,i,j] = np.mean(v[idx][bidx])\n",
    "                elif moment == 'std':\n",
    "                    vmom[k,i,j] = np.std(v[idx][bidx])\n",
    "                elif moment == 'meansq':\n",
    "                    vmom[k,i,j] = np.mean(v[idx][bidx]**2)\n",
    "\n",
    "    return vmom\n",
    "\n",
    "def compute_mass_error_weighted_deviation_vmoment(nbody_orbs, sample_orbs, \n",
    "    nbody_mass, n, n_bs=100, adaptive_binning_kwargs={}, \n",
    "    raise_inverse_power=False, stdev_normalization=True, pearson_correction=True):\n",
    "    '''compute_mass_error_weighted_deviation_v4:\n",
    "    \n",
    "    Compute the mass- and uncertainty-weighted deviation of the N-body \n",
    "    velocity 4th order moments of vr, vp, and vt from the DF samples.\n",
    "\n",
    "    For the binning scheme the default kwargs are:\n",
    "    - n: min(500, number of N-body particles//10)\n",
    "    - rmin: 0.\n",
    "    - rmax: max(N-body particle radii)\n",
    "    - bin_mode: 'exact numbers'\n",
    "    - bin_equal_n: True\n",
    "    - end_mode: 'ignore'\n",
    "    - bin_cents_mode: 'median'\n",
    "\n",
    "    Args:\n",
    "        nbody_orbs (galpy.orbit.Orbit): N-body orbits\n",
    "        sample_orbs (galpy.orbit.Orbit): DF samples\n",
    "        nbody_mass (np.ndarray): N-body particle masses\n",
    "        n (int): Moment to compute\n",
    "        n_bs (int): Number of times to bootstrap the DF/N-body samples\n",
    "            to compute the deviation statistic for error estimation\n",
    "        adaptive_binning_kwargs (dict): kwargs for get_radius_binning(), will\n",
    "            be populated with defaults listed above if not provided.\n",
    "        moment_inverse_power (bool): If True, raise each moment to the \n",
    "            inverse power of n.\n",
    "        standardized (bool): If True, compute the standardized moment, \n",
    "            i.e. divide by the standard deviation to the nth power\n",
    "        pearson (bool): If True, apply Pearson's correction to the 4th \n",
    "            standardized moment such that it gives excess kurtosis.\n",
    "    \n",
    "    Returns:\n",
    "        mwed_[beta,vr2,vp2,vt2] (np.ndarray): Mass-weighted error deviation\n",
    "    '''\n",
    "    # Binning for velocity dispersions and betas\n",
    "    n_bin = np.min([500, len(nbody_orbs)//10]) # n per bin\n",
    "    if 'n' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['n'] = n_bin\n",
    "    if 'rmin' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['rmin'] = 0.\n",
    "    if 'rmax' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['rmax'] = np.max( nbody_orbs.r().to_value(apu.kpc) )\n",
    "    if 'bin_mode' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['bin_mode'] = 'exact numbers'\n",
    "    if 'bin_equal_n' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['bin_equal_n'] = True\n",
    "    if 'end_mode' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['end_mode'] = 'ignore'\n",
    "    if 'bin_cents_mode' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['bin_cents_mode'] = 'median'\n",
    "\n",
    "    bin_edges, bin_cents, _ = pkin.get_radius_binning(nbody_orbs, \n",
    "        **adaptive_binning_kwargs)\n",
    "\n",
    "    # Compute velocity moments for the N-body data and DF samples\n",
    "    nbody_vmom = compute_vmoment(nbody_orbs, n, bin_edges, n_bootstrap=n_bs,\n",
    "        stdev_normalization=stdev_normalization, \n",
    "        pearson_correction=pearson_correction)\n",
    "    sample_vmom = compute_vmoment(sample_orbs, n, bin_edges, n_bootstrap=n_bs,\n",
    "        stdev_normalization=stdev_normalization, \n",
    "        pearson_correction=pearson_correction)\n",
    "\n",
    "    if raise_inverse_power:\n",
    "        nbody_vmom = nbody_vmom**(1/n)\n",
    "        sample_vmom = sample_vmom**(1/n)\n",
    "\n",
    "    # Compute the mass profile for the N-body data\n",
    "    mass_profile = np.zeros(len(bin_cents))\n",
    "    rs = nbody_orbs.r().to_value(apu.kpc)\n",
    "    for i in range(len(bin_cents)):\n",
    "        mass_profile[i] = np.sum(nbody_mass[(rs > bin_edges[i]) &\\\n",
    "                                            (rs < bin_edges[i+1])])\n",
    "\n",
    "    # Compute the inter-sigma range for the N-body data, which will be the error\n",
    "    nbody_err = np.zeros((3,len(bin_cents)))\n",
    "    for i in range(3):\n",
    "        nbody_err[i] = np.percentile(nbody_vmom[i], 84, axis=0) - \\\n",
    "                       np.percentile(nbody_vmom[i], 16, axis=0)\n",
    "\n",
    "    # Compute the mass-error-weighted deviation between the N-body and DF \n",
    "    # sample trends\n",
    "    mewd = np.zeros((3,n_bs))\n",
    "    for i in range(3):\n",
    "        mewd[i] = np.sum( np.abs(nbody_vmom[i] - sample_vmom[i])*\\\n",
    "                            mass_profile/nbody_err[i], axis=1 )/\\\n",
    "                    np.sum(mass_profile)\n",
    "\n",
    "    return mewd\n",
    "\n",
    "def plot_velocity_moments(orbs, sample, bin_edges, bin_cents, moms, n_bs=100,\n",
    "                         plot_log=False, stdev_normalization=True, \n",
    "                         pearson_correction=True):\n",
    "    if isinstance(stdev_normalization, bool):\n",
    "        stdev_normalization = [stdev_normalization]*len(moms)\n",
    "    if isinstance(pearson_correction, bool):\n",
    "        pearson_correction = [pearson_correction]*len(moms)\n",
    "    if isinstance(plot_log, bool):\n",
    "        plot_log = [plot_log]*len(moms)\n",
    "\n",
    "    vtext = [r'v_{r}', r'v_{\\phi}', r'v_{\\theta}']\n",
    "\n",
    "    fig = plt.figure(figsize=(len(moms)*4,10))\n",
    "    axs = fig.subplots(nrows=3, ncols=len(moms))\n",
    "\n",
    "    for k,m in enumerate(moms):\n",
    "        vm_nbody = compute_vmoment(orbs, m, bin_edges, n_bootstrap=n_bs,\n",
    "            stdev_normalization=stdev_normalization[k],\n",
    "            pearson_correction=pearson_correction[k])\n",
    "        vm_sample = compute_vmoment(sample, m, bin_edges, n_bootstrap=n_bs,\n",
    "            stdev_normalization=stdev_normalization[k],\n",
    "            pearson_correction=pearson_correction[k])\n",
    "\n",
    "        for l in range(3):\n",
    "            vl, vm, vu = np.percentile(vm_nbody[l], [16,50,84], axis=0)\n",
    "            axs[l,k].plot(bin_cents, vm, color='Black', alpha=1.0)\n",
    "            axs[l,k].fill_between(bin_cents, vl, vu, color='Black', \n",
    "                alpha=0.3)\n",
    "\n",
    "            vl, vm, vu = np.percentile(vm_sample[l], [16,50,84], axis=0)\n",
    "            axs[l,k].plot(bin_cents, vm, color='Red', alpha=1.0)\n",
    "            axs[l,k].fill_between(bin_cents, vl, vu, color='Red', \n",
    "                alpha=0.3)\n",
    "\n",
    "            axs[l,k].set_xscale('log')\n",
    "            if plot_log[k]: axs[l,k].set_yscale('log')\n",
    "\n",
    "\n",
    "            if m == 'mean':\n",
    "                axs[l,k].set_ylabel(r'$\\overline{'+vtext[l]+r'}$')\n",
    "            elif m == 'std':\n",
    "                axs[l,k].set_ylabel(r'$\\sigma_{'+vtext[l]+r'}$')\n",
    "            elif m == 'meansq':\n",
    "                axs[l,k].set_ylabel(r'$\\overline{'+vtext[l]+r'^2}$')\n",
    "            else:\n",
    "                axs[l,k].set_ylabel(r'$\\mu^{'+str(m)+r'}_{'+vtext[l]+r'}$')\n",
    "\n",
    "        axs[2,k].set_xlabel(r'$r\\,(\\mathrm{kpc})$')\n",
    "    \n",
    "    return fig, axs\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the mean, std, 3rd, 4th order moments and deltas for the N-body data and DF samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "make_moment_plot = False\n",
    "\n",
    "# Pathing\n",
    "dens_fitting_dir = os.path.join(fitting_dir_base,'density_profile')\n",
    "df_fitting_dir = os.path.join(fitting_dir_base,'distribution_function')\n",
    "analysis_version = 'v1.1'\n",
    "analysis_dir = os.path.join(mw_analog_dir,'analysis',analysis_version)\n",
    "\n",
    "# Moment calculation keywords\n",
    "n_bs = 10\n",
    "raise_inverse_power = False\n",
    "# moms = ['mean','std',3,4]\n",
    "moms = ['std',]\n",
    "stdev_normalization = [False, False, True, True]\n",
    "pearson_correction = [False, False, False, True]\n",
    "\n",
    "# Get the sample orbits\n",
    "sample_data_cb = np.load(os.path.join(analysis_dir,'sample_data_cb.npy'),\n",
    "    allow_pickle=True)\n",
    "sample_data_om = np.load(os.path.join(analysis_dir,'sample_data_om.npy'),\n",
    "    allow_pickle=True)\n",
    "sample_data_om2 = np.load(os.path.join(analysis_dir,'sample_data_om2.npy'),\n",
    "    allow_pickle=True)\n",
    "\n",
    "# Begin logging\n",
    "os.makedirs('./log/',exist_ok=True)\n",
    "log_filename = './log/6_higher_order_moments.log'\n",
    "if os.path.exists(log_filename):\n",
    "    os.remove(log_filename)\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO, filemode='w', \n",
    "    force=True)\n",
    "logging.info('Beginning higher order moment computation. Time: '+\\\n",
    "             time.strftime('%a, %d %b %Y %H:%M:%S',time.localtime()))\n",
    "\n",
    "mewd_data = []\n",
    "mewd_data_dtype_list = []\n",
    "make_mewd_data_dtype_list = True\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i != 0: continue\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    major_mergers = primary.tree_major_mergers\n",
    "    n_major = primary.n_major_mergers\n",
    "    n_snap = len(primary.snapnum)\n",
    "    primary_filename = primary.get_cutout_filename(mw_analog_dir,\n",
    "        snapnum=primary.snapnum[0])\n",
    "    co = pcutout.TNGCutout(primary_filename)\n",
    "    co.center_and_rectify()\n",
    "    pid = co.get_property('stars','ParticleIDs')\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j > 0: continue\n",
    "\n",
    "        if verbose: \n",
    "            msg = f'Calculating moment profiles for MW analog '+\\\n",
    "                  f'{i+1}/{n_mw}, merger {j+1}/{n_major}'\n",
    "            logging.info(msg)\n",
    "            print(msg, end='\\r')\n",
    "\n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_acc_sid = major_merger.subfind_id[0]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "        upid = major_merger.get_unique_particle_ids('stars',data_dir=data_dir)\n",
    "        indx = np.where(np.isin(pid,upid))[0]\n",
    "        orbs = co.get_orbs('stars')[indx]\n",
    "        n_star = len(orbs)\n",
    "        star_mass = co.get_masses('stars')[indx].to_value(apu.Msun)\n",
    "        r = orbs.r().to_value(apu.kpc)\n",
    "        r_softening = putil.get_softening_length('stars', z=0, physical=True)\n",
    "        rmin = np.max([np.min(r), r_softening])\n",
    "\n",
    "        # Define the adaptive binning keyword dict and then bin\n",
    "        n_bin = np.min([500, n_star//10]) # n per bin\n",
    "        adaptive_binning_kwargs = {'n':n_bin,\n",
    "                                   'rmin':rmin,\n",
    "                                   'rmax':np.max(r),\n",
    "                                   'bin_mode':'exact numbers',\n",
    "                                   'bin_equal_n':True,\n",
    "                                   'end_mode':'ignore',\n",
    "                                   'bin_cents_mode':'median'}\n",
    "        bin_edges, bin_cents, _ = pkin.get_radius_binning(orbs, \n",
    "            **adaptive_binning_kwargs)\n",
    "        bin_size = bin_edges[1:] - bin_edges[:-1]\n",
    "\n",
    "        # Loop over each set of DF samples and compute moments and statistics\n",
    "        sample_data_arr = [sample_data_cb,\n",
    "                           sample_data_om,\n",
    "                           sample_data_om2]\n",
    "        sample_data_suffix = ['cb','om','om2']\n",
    "\n",
    "        _data = (\n",
    "            z0_sid,\n",
    "            major_acc_sid,\n",
    "            major_mlpid,\n",
    "            j+1,\n",
    "        )\n",
    "        if make_mewd_data_dtype_list:\n",
    "            mewd_data_dtype_list.extend([\n",
    "                ('z0_sid',int),\n",
    "                ('major_acc_sid',int),\n",
    "                ('major_mlpid',int),\n",
    "                ('merger_number',int),\n",
    "            ])\n",
    "\n",
    "        for k in range(len(sample_data_arr)):\n",
    "\n",
    "            sample_data = sample_data_arr[k]\n",
    "\n",
    "            # Mask the sample data correctly\n",
    "            mask = (sample_data['z0_sid'] == z0_sid) &\\\n",
    "                   (sample_data['major_acc_sid'] == major_acc_sid) &\\\n",
    "                   (sample_data['major_mlpid'] == major_mlpid) &\\\n",
    "                   (sample_data['merger_number'] == j+1)\n",
    "            indx = np.where(mask)[0]\n",
    "            assert len(indx) == 1, 'Something went wrong'\n",
    "            sample = sample_data[indx[0]]['sample']\n",
    "\n",
    "            # Make plots of the moments\n",
    "            if make_moment_plot:\n",
    "                # moms = [1,2,3,4]\n",
    "                plot_log = [False, False, False, False]\n",
    "                fig,axs = plot_velocity_moments(orbs, sample, bin_edges, \n",
    "                    bin_cents, moms, n_bs=n_bs, plot_log=plot_log, \n",
    "                    stdev_normalization=stdev_normalization, \n",
    "                    pearson_correction=pearson_correction)\n",
    "\n",
    "                fig.tight_layout()\n",
    "                this_fig_dir = os.path.join(fig_dir, str(z0_sid), \n",
    "                    'merger_'+str(j+1))\n",
    "                os.makedirs(this_fig_dir, exist_ok=True)\n",
    "                this_figname = os.path.join(this_fig_dir,\n",
    "                    'moments_'+'_'.join([str(m) for m in moms])+\\\n",
    "                    '_'+sample_data_suffix[k]+'.png')\n",
    "                fig.savefig(this_figname, dpi=300, bbox_inches='tight')\n",
    "                if not show_plots: plt.close(fig)\n",
    "                else: plt.show()\n",
    "            \n",
    "            # Loop over each moment and compute the MEWD\n",
    "            for l in range(len(moms)):\n",
    "                mewd = compute_mass_error_weighted_deviation_vmoment(orbs,\n",
    "                    sample, star_mass, moms[l], n_bs=n_bs, \n",
    "                    adaptive_binning_kwargs=adaptive_binning_kwargs,\n",
    "                    raise_inverse_power=raise_inverse_power, \n",
    "                    stdev_normalization=stdev_normalization[l], \n",
    "                    pearson_correction=pearson_correction[l])\n",
    "                _data += (mewd[0], mewd[1], mewd[2])\n",
    "                if make_mewd_data_dtype_list:\n",
    "                    mewd_data_dtype_list.extend([\n",
    "                        ('mewd_'+sample_data_suffix[k]+'_vr'+str(moms[l]),object),\n",
    "                        ('mewd_'+sample_data_suffix[k]+'_vp'+str(moms[l]),object),\n",
    "                        ('mewd_'+sample_data_suffix[k]+'_vt'+str(moms[l]),object),\n",
    "                    ])\n",
    "        \n",
    "        for l in range(len(moms)):\n",
    "            mewd_self = compute_mass_error_weighted_deviation_vmoment(\n",
    "                orbs, orbs, star_mass, moms[l], n_bs=n_bs, \n",
    "                adaptive_binning_kwargs=adaptive_binning_kwargs,\n",
    "                raise_inverse_power=raise_inverse_power, \n",
    "                stdev_normalization=stdev_normalization[l], \n",
    "                pearson_correction=pearson_correction[l])\n",
    "            _data += (mewd_self[0], mewd_self[1], mewd_self[2])\n",
    "            if make_mewd_data_dtype_list:\n",
    "                mewd_data_dtype_list.extend([\n",
    "                    ('mewd_self_vr'+str(moms[l]),object),\n",
    "                    ('mewd_self_vp'+str(moms[l]),object),\n",
    "                    ('mewd_self_vt'+str(moms[l]),object),\n",
    "                ])\n",
    "\n",
    "        mewd_data.append(_data)\n",
    "        make_mewd_data_dtype_list = False\n",
    "        \n",
    "\n",
    "# Save the data as a pickle\n",
    "header = [f'kewords: stdev_normalization={stdev_normalization}, pearson_correctino={pearson_correction}, '+\\\n",
    "          f'raise_inverse_power={raise_inverse_power}, n_bs={n_bs}',\n",
    "    [mewd_data_dtype_list[i][0] for i in range(len(mewd_data_dtype_list))]\n",
    "]\n",
    "mewd_data_filename = os.path.join(analysis_dir,\n",
    "    'mewd_data_vmom_'+'_'.join([str(m) for m in moms]))\n",
    "with open(mewd_data_filename+'.pkl','wb') as handle:\n",
    "    pickle.dump([header,mewd_data], handle)\n",
    "\n",
    "# Also save as a structured array\n",
    "mewd_data_dtype = np.dtype(mewd_data_dtype_list)\n",
    "mewd_data = np.array(mewd_data, dtype=mewd_data_dtype)\n",
    "np.save(os.path.join(analysis_dir,\n",
    "    'mewd_data_vmom_'+'_'.join([str(m) for m in moms])+'.npy'), mewd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the stashed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_version = 'v1.1'\n",
    "analysis_dir = os.path.join(mw_analog_dir,'analysis',analysis_version)\n",
    "\n",
    "# Or load the structured arrays\n",
    "# moms = [1,2,3,4]\n",
    "moms = ['mean','std',3,4]\n",
    "mewd_data_filename = os.path.join(analysis_dir,\n",
    "    'mewd_data_vmom_'+'_'.join([str(m) for m in moms])+'.npy')\n",
    "mewd_data = np.load(mewd_data_filename, allow_pickle=True)\n",
    "\n",
    "# Load the merger information\n",
    "merger_data = np.load(os.path.join(analysis_dir,'merger_data.npy'), \n",
    "    allow_pickle=True)\n",
    "\n",
    "checks = True\n",
    "if checks:\n",
    "    assert np.all( mewd_data['z0_sid'] == merger_data['z0_sid'] ), \\\n",
    "        'Something went wrong'\n",
    "    assert np.all( mewd_data['major_acc_sid'] == merger_data['major_acc_sid'] ), \\\n",
    "        'Something went wrong'\n",
    "    assert np.all( mewd_data['major_mlpid'] == merger_data['major_mlpid'] ), \\\n",
    "        'Something went wrong'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show a comparison of the MEWD values, Moments 1-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnwidth, textwidth = pplot.get_latex_columnwidth_textwidth_inches()\n",
    "\n",
    "# fig = plt.figure(figsize=(textwidth,5))\n",
    "# axs = fig.subplots(nrows=3, ncols=4)\n",
    "\n",
    "df_names = ['cb','om','om2']\n",
    "velocity_names = ['vr','vp','vt']\n",
    "velocity_labels = [r'v_r',r'v_\\phi',r'v_\\theta']\n",
    "\n",
    "n_merger = len(mewd_data)\n",
    "xlims = [[0.4,2],\n",
    "         [0.4,2],\n",
    "         [0.4,2],\n",
    "         [0.4,2]]\n",
    "ylims = [[0.4,10],\n",
    "         [0.4,10],\n",
    "         [0.4,10],\n",
    "         [0.4,10]]\n",
    "marker_size = 4\n",
    "marker_alpha = 0.5\n",
    "ticklabel_fs = 8\n",
    "xaxis_label_fs = 10\n",
    "yaxis_label_fs = 10\n",
    "panel_label_fs = 8\n",
    "title_fs = 10\n",
    "\n",
    "for g in range(len(df_names)):\n",
    "    df_name = df_names[g]\n",
    "\n",
    "    fig = plt.figure(figsize=(textwidth,5))\n",
    "    axs = fig.subplots(nrows=3, ncols=4)\n",
    "\n",
    "    for i in range(len(moms)):\n",
    "\n",
    "\n",
    "        for j in range(len(velocity_names)):\n",
    "            \n",
    "            x = mewd_data['mewd_self_'+velocity_names[j]+str(moms[i])]\n",
    "            y = mewd_data['mewd_'+df_name+'_'+velocity_names[j]+str(moms[i])]\n",
    "\n",
    "            for k in range(n_merger):\n",
    "                \n",
    "                xl, xm, xu = np.percentile(x[k], [16,50,84])\n",
    "                yl, ym, yu = np.percentile(y[k], [16,50,84])\n",
    "\n",
    "                # axs[j,i].errorbar(xm, ym, xerr=[[xm-xl],[xu-xm]],\n",
    "                #     yerr=[[ym-yl],[yu-ym]], fmt='o',\n",
    "                #     markersize=marker_size, color='Black', alpha=marker_alpha)\n",
    "                axs[j,i].scatter(xm, ym, s=marker_size, color='Black', \n",
    "                    alpha=marker_alpha)\n",
    "                axs[j,i].axline([0.1,0.1],[1,1], color='Black', linestyle='--')\n",
    "        \n",
    "            # axs[j,i].set_xscale('log')\n",
    "            axs[j,i].set_yscale('log')\n",
    "            # axs[j,i].set_xlim(xlims[i])\n",
    "            # axs[j,i].set_ylim(ylims[i])\n",
    "\n",
    "            if i != 0:\n",
    "                axs[j,i].tick_params(labelleft=False)\n",
    "            else:\n",
    "                axs[j,i].set_ylabel('$\\delta_{NB-DF}$', \n",
    "                    fontsize=yaxis_label_fs)\n",
    "                axs[j,i].tick_params(labelsize=ticklabel_fs)\n",
    "\n",
    "            if j != 2:\n",
    "                # pass\n",
    "                axs[j,i].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "                axs[j,i].xaxis.set_minor_formatter(plt.NullFormatter())\n",
    "                axs[j,i].tick_params(labelbottom=False)\n",
    "            else:\n",
    "                axs[j,i].set_xlabel('$\\delta_{NB-NB}$', fontsize=xaxis_label_fs)\n",
    "                axs[j,i].tick_params(labelsize=ticklabel_fs)\n",
    "\n",
    "            if moms[i] == 'mean':\n",
    "                vtext = r'$\\overline{'+velocity_labels[j]+r'}$'\n",
    "            elif moms[i] == 'std':\n",
    "                vtext = r'$\\sigma_{'+velocity_labels[j]+r'}$'\n",
    "            elif moms[i] == 'meansq':\n",
    "                vtext = r'$\\overline{'+velocity_labels[j]+r'^2}$'\n",
    "            else:\n",
    "                vtext = r'$\\mu^{'+str(moms[i])+r'}_{'+velocity_labels[j]+r'}$'\n",
    "\n",
    "            axs[j,i].text(0.95,0.15, vtext,\n",
    "                ha='right', va='top', transform=axs[j,i].transAxes, \n",
    "                fontsize=panel_label_fs)\n",
    "\n",
    "    fig.suptitle(df_name.upper()+'DF', fontsize=title_fs)\n",
    "    fig.tight_layout()\n",
    "    this_figname = os.path.join(local_fig_dir,\n",
    "        'mewd_vmom_comparison_'+df_name+'.png')\n",
    "    fig.savefig(this_figname, dpi=300, bbox_inches='tight')\n",
    "    if not show_plots: plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute histograms of the Nbody - DF delta divided by the Nbody - Nbody delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnwidth, textwidth = pplot.get_latex_columnwidth_textwidth_inches()\n",
    "\n",
    "fig = plt.figure(figsize=(textwidth,5))\n",
    "axs = fig.subplots(nrows=3, ncols=4)\n",
    "\n",
    "df_names = ['cb','om','om2']\n",
    "df_colors = ['DodgerBlue','Red','Black']\n",
    "df_linewidths = [4.0, 2.0, 1.0]\n",
    "df_zorders = [1,2,3]\n",
    "df_linestyles = ['solid','solid','dashed']\n",
    "velocity_names = ['vr','vp','vt']\n",
    "velocity_labels = [r'v_r',r'v_\\phi',r'v_\\theta']\n",
    "n_merger = len(mewd_data)\n",
    "# xlims = [[0.4,2],\n",
    "#          [0.4,2],\n",
    "#          [0.4,2],\n",
    "#          [0.4,2]]\n",
    "# ylims = [[0.4,100],\n",
    "#          [0.4,100],\n",
    "#          [0.4,100],\n",
    "#          [0.4,100]]\n",
    "marker_size = 4\n",
    "marker_alpha = 0.5\n",
    "ticklabel_fs = 8\n",
    "xaxis_label_fs = 8\n",
    "yaxis_label_fs = 10\n",
    "panel_label_fs = 8\n",
    "title_fs = 10\n",
    "\n",
    "for g in range(len(df_names)):\n",
    "    # if g > 0: continue\n",
    "    df_name = df_names[g]\n",
    "\n",
    "    for i in range(len(moms)):\n",
    "\n",
    "\n",
    "        for j in range(len(velocity_names)):\n",
    "            \n",
    "            x = np.concatenate(\n",
    "                mewd_data['mewd_self_'+velocity_names[j]+str(moms[i])]\n",
    "                )\n",
    "            y = np.concatenate(\n",
    "                mewd_data['mewd_'+df_name+'_'+velocity_names[j]+str(moms[i])]\n",
    "                )\n",
    "            \n",
    "            axs[j,i].hist(np.log10(y/x), bins=15, histtype='step', \n",
    "                range=(-0.5,1.5), color=df_colors[g], \n",
    "                zorder=df_zorders[g], linestyle=df_linestyles[g],\n",
    "                linewidth=df_linewidths[g], \n",
    "                density=True)\n",
    "        \n",
    "            # axs[j,i].set_xscale('log')\n",
    "            # axs[j,i].set_yscale('log')\n",
    "            # axs[j,i].set_xlim(xlims[i])\n",
    "            # axs[j,i].set_ylim(ylims[i])\n",
    "\n",
    "            if i != 0:\n",
    "                axs[j,i].tick_params(labelleft=False)\n",
    "            else:\n",
    "                axs[j,i].set_ylabel('Density', \n",
    "                    fontsize=yaxis_label_fs)\n",
    "                axs[j,i].tick_params(labelsize=ticklabel_fs)\n",
    "\n",
    "            if j != 2:\n",
    "                # pass\n",
    "                # axs[j,i].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "                # axs[j,i].xaxis.set_minor_formatter(plt.NullFormatter())\n",
    "                axs[j,i].tick_params(labelbottom=False)\n",
    "            else:\n",
    "                axs[j,i].set_xlabel(\n",
    "                    '$\\log_{10}[ \\delta_{NB-DF}/\\delta_{NB-NB} ]$', \n",
    "                    fontsize=xaxis_label_fs)\n",
    "                axs[j,i].tick_params(labelsize=ticklabel_fs)\n",
    "\n",
    "            if moms[i] == 'mean':\n",
    "                vtext = r'$\\overline{'+velocity_labels[j]+r'}$'\n",
    "            elif moms[i] == 'std':\n",
    "                vtext = r'$\\sigma_{'+velocity_labels[j]+r'}$'\n",
    "            elif moms[i] == 'meansq':\n",
    "                vtext = r'$\\overline{'+velocity_labels[j]+r'^2}$'\n",
    "            else:\n",
    "                vtext = r'$\\mu^{'+str(moms[i])+r'}_{'+velocity_labels[j]+r'}$'\n",
    "\n",
    "            axs[j,i].text(0.95,0.95, vtext,\n",
    "                ha='right', va='top', transform=axs[j,i].transAxes, \n",
    "                fontsize=panel_label_fs)\n",
    "\n",
    "            # Turn on gridlines\n",
    "            axs[j,i].xaxis.set_minor_locator(plt.MultipleLocator(0.1))\n",
    "            axs[j,i].grid(True, linestyle='solid', alpha=1)\n",
    "            axs[j,i].grid(which='minor', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Make the legend\n",
    "for i in range(len(df_names)):\n",
    "    axs[0,0].plot([],[],color=df_colors[i],label=df_names[i].upper(),\n",
    "        linestyle=df_linestyles[i], linewidth=df_linewidths[i])\n",
    "axs[0,0].legend(loc='center right', fontsize=ticklabel_fs)\n",
    "\n",
    "# fig.suptitle(df_name.upper()+' DF', fontsize=title_fs)\n",
    "fig.tight_layout()\n",
    "this_figname = os.path.join(local_fig_dir,\n",
    "    'mewd_vmom_hist.png')\n",
    "fig.savefig(this_figname, dpi=300, bbox_inches='tight')\n",
    "if not show_plots: plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
