{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# TITLE - 6_higher_order_moments.ipynb\n",
    "# AUTHOR - James Lane\n",
    "# PROJECT - tng-dfs\n",
    "#\n",
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# Docstrings and metadata:\n",
    "'''Compute the higher order moments of the data and the DF realizations\n",
    "'''\n",
    "\n",
    "__author__ = \"James Lane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mA new version of galpy (1.9.1) is available, please upgrade using pip/conda/... to get the latest features and bug fixes!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# %load ../../src/nb_modules/nb_imports.txt\n",
    "### Imports\n",
    "\n",
    "## Basic\n",
    "import numpy as np\n",
    "import sys, os, dill as pickle\n",
    "\n",
    "## Matplotlib\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## Astropy\n",
    "from astropy import units as apu\n",
    "from astropy import constants as apc\n",
    "\n",
    "## Analysis\n",
    "import scipy.stats\n",
    "import scipy.interpolate\n",
    "\n",
    "## galpy\n",
    "from galpy import orbit\n",
    "from galpy import potential\n",
    "\n",
    "## Project-specific\n",
    "src_path = 'src/'\n",
    "while True:\n",
    "    if os.path.exists(src_path): break\n",
    "    if os.path.realpath(src_path).split('/')[-1] in ['tng-dfs','/']:\n",
    "            raise FileNotFoundError('Failed to find src/ directory.')\n",
    "    src_path = os.path.join('..',src_path)\n",
    "sys.path.insert(0,src_path)\n",
    "from tng_dfs import cutout as pcutout\n",
    "from tng_dfs import densprofile as pdens\n",
    "from tng_dfs import fitting as pfit\n",
    "from tng_dfs import kinematics as pkin\n",
    "from tng_dfs import plot as pplot\n",
    "from tng_dfs import util as putil\n",
    "\n",
    "### Notebook setup\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(os.path.join(src_path,'mpl/project.mplstyle')) # This must be exactly here\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords, loading, pathing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 Milky way like galaxies found\n",
      "Loading subhalo data from /epsen_data/scr/lane/projects/tng-dfs/data/mw_analogs/subs/mwsubs.pkl\n",
      "File has 46 subhalos\n",
      "Warning: new key not in the first dict, supplementary_data:MWM31s_cutouts\n",
      "Cutting on bulge and disk fractions\n",
      "Cut to  30  subhalos\n"
     ]
    }
   ],
   "source": [
    "# %load ../../src/nb_modules/nb_setup.txt\n",
    "# Keywords\n",
    "cdict = putil.load_config_to_dict()\n",
    "keywords = ['DATA_DIR','MW_ANALOG_DIR','FIG_DIR_BASE','FITTING_DIR_BASE',\n",
    "            'RO','VO','ZO','LITTLE_H','MW_MASS_RANGE']\n",
    "data_dir,mw_analog_dir,fig_dir_base,fitting_dir_base,ro,vo,zo,h,\\\n",
    "    mw_mass_range = putil.parse_config_dict(cdict,keywords)\n",
    "\n",
    "# MW Analog \n",
    "mwsubs,mwsubs_vars = putil.prepare_mwsubs(mw_analog_dir,h=h,\n",
    "    mw_mass_range=mw_mass_range,return_vars=True,force_mwsubs=False,\n",
    "    bulge_disk_fraction_cuts=True)\n",
    "\n",
    "# Figure path\n",
    "local_fig_dir = './fig/'\n",
    "fig_dir = os.path.join(fig_dir_base, \n",
    "    'notebooks/5_compare_distribution_functions/6_higher_order_moments/')\n",
    "os.makedirs(local_fig_dir,exist_ok=True)\n",
    "os.makedirs(fig_dir,exist_ok=True)\n",
    "show_plots = False\n",
    "\n",
    "# Load tree data\n",
    "tree_primary_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_primaries.pkl')\n",
    "with open(tree_primary_filename,'rb') as handle: \n",
    "    tree_primaries = pickle.load(handle)\n",
    "tree_major_mergers_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_major_mergers.pkl')\n",
    "with open(tree_major_mergers_filename,'rb') as handle:\n",
    "    tree_major_mergers = pickle.load(handle)\n",
    "n_mw = len(tree_primaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vmoment(orbs, n, bin_edges, n_bootstrap=100):\n",
    "    '''compute_vmoment:\n",
    "    \n",
    "    Compute the nth moment of the velocity distribution in bins.\n",
    "    \n",
    "    Args:\n",
    "        orbs (galpy.orbit.Orbit): Orbits\n",
    "        n (int): Moment to compute\n",
    "        bin_edges (np.ndarray): Bin edges\n",
    "        n_bootstrap (int): Number of times to bootstrap the orbits to compute\n",
    "            the moment for error estimation\n",
    "    \n",
    "    Returns:\n",
    "        v_moment (np.ndarray): Velocity moments, shape (3, len(bin_edges)-1, n_bs)\n",
    "    '''\n",
    "    r = orbs.r().to_value(apu.kpc)\n",
    "    vr = orbs.vr().to_value(apu.km/apu.s)\n",
    "    vt = orbs.vtheta().to_value(apu.km/apu.s)\n",
    "    vp = orbs.vT().to_value(apu.km/apu.s)\n",
    "\n",
    "    vmom = np.zeros((3,n_bootstrap,len(bin_edges)-1))\n",
    "    n_orbs = len(orbs)\n",
    "\n",
    "    for i in range(n_bootstrap):\n",
    "        idx = np.random.randint(0,n_orbs,n_orbs)\n",
    "\n",
    "        for j in range(len(bin_edges)-1):\n",
    "            bidx = (r[idx] > bin_edges[j]) & (r[idx] < bin_edges[j+1])\n",
    "            if np.sum(bidx) == 0:\n",
    "                vmom[:,i,j] = np.nan\n",
    "                continue\n",
    "            for k,v in enumerate([vr,vp,vt]):\n",
    "                vmom[k,i,j] = np.mean((v[idx][bidx]-np.mean(v[idx][bidx]))**n)\n",
    "    \n",
    "    return vmom\n",
    "\n",
    "def compute_mass_error_weighted_deviation_vmoment(nbody_orbs, sample_orbs, \n",
    "    nbody_mass, n, n_bs=100, adaptive_binning_kwargs={}, \n",
    "    raise_inverse_power=False):\n",
    "    '''compute_mass_error_weighted_deviation_v4:\n",
    "    \n",
    "    Compute the mass- and uncertainty-weighted deviation of the N-body \n",
    "    velocity 4th order moments of vr, vp, and vt from the DF samples.\n",
    "\n",
    "    For the binning scheme the default kwargs are:\n",
    "    - n: min(500, number of N-body particles//10)\n",
    "    - rmin: 0.\n",
    "    - rmax: max(N-body particle radii)\n",
    "    - bin_mode: 'exact numbers'\n",
    "    - bin_equal_n: True\n",
    "    - end_mode: 'ignore'\n",
    "    - bin_cents_mode: 'median'\n",
    "\n",
    "    Args:\n",
    "        nbody_orbs (galpy.orbit.Orbit): N-body orbits\n",
    "        sample_orbs (galpy.orbit.Orbit): DF samples\n",
    "        nbody_mass (np.ndarray): N-body particle masses\n",
    "        n (int): Moment to compute\n",
    "        n_bs (int): Number of times to bootstrap the DF/N-body samples\n",
    "            to compute the deviation statistic for error estimation\n",
    "        adaptive_binning_kwargs (dict): kwargs for get_radius_binning(), will\n",
    "            be populated with defaults listed above if not provided.\n",
    "        moment_inverse_power (bool): If True, raise each moment to the \n",
    "            inverse power of n.\n",
    "    \n",
    "    Returns:\n",
    "        mwed_[beta,vr2,vp2,vt2] (np.ndarray): Mass-weighted error deviation\n",
    "    '''\n",
    "    # Binning for velocity dispersions and betas\n",
    "    n_bin = np.min([500, len(nbody_orbs)//10]) # n per bin\n",
    "    if 'n' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['n'] = n_bin\n",
    "    if 'rmin' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['rmin'] = 0.\n",
    "    if 'rmax' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['rmax'] = np.max( nbody_orbs.r().to_value(apu.kpc) )\n",
    "    if 'bin_mode' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['bin_mode'] = 'exact numbers'\n",
    "    if 'bin_equal_n' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['bin_equal_n'] = True\n",
    "    if 'end_mode' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['end_mode'] = 'ignore'\n",
    "    if 'bin_cents_mode' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['bin_cents_mode'] = 'median'\n",
    "\n",
    "    bin_edges, bin_cents, _ = pkin.get_radius_binning(nbody_orbs, \n",
    "        **adaptive_binning_kwargs)\n",
    "\n",
    "    # Compute velocity moments for the N-body data and DF samples\n",
    "    nbody_vmom = compute_vmoment(nbody_orbs, n, bin_edges, n_bootstrap=n_bs)\n",
    "    sample_vmom = compute_vmoment(sample_orbs, n, bin_edges, n_bootstrap=n_bs)\n",
    "\n",
    "    if raise_inverse_power:\n",
    "        nbody_vmom = nbody_vmom**(1/n)\n",
    "        sample_vmom = sample_vmom**(1/n)\n",
    "\n",
    "    # Compute the mass profile for the N-body data\n",
    "    mass_profile = np.zeros(len(bin_cents))\n",
    "    rs = nbody_orbs.r().to_value(apu.kpc)\n",
    "    for i in range(len(bin_cents)):\n",
    "        mass_profile[i] = np.sum(nbody_mass[(rs > bin_edges[i]) &\\\n",
    "                                            (rs < bin_edges[i+1])])\n",
    "\n",
    "    # Compute the inter-sigma range for the N-body data, which will be the error\n",
    "    nbody_err = np.zeros((3,len(bin_cents)))\n",
    "    for i in range(3):\n",
    "        nbody_err[i] = np.percentile(nbody_vmom[i], 84, axis=0) - \\\n",
    "                       np.percentile(nbody_vmom[i], 16, axis=0)\n",
    "\n",
    "    # Compute the mass-error-weighted deviation between the N-body and DF \n",
    "    # sample trends\n",
    "    mewd = np.zeros((3,n_bs))\n",
    "    for i in range(3):\n",
    "        mewd[i] = np.sum( np.abs(nbody_vmom[i] - sample_vmom[i])*\\\n",
    "                            mass_profile/nbody_err[i], axis=1 )/\\\n",
    "                    np.sum(mass_profile)\n",
    "\n",
    "    return mewd\n",
    "\n",
    "def plot_velocity_moments(orbs, sample, bin_edges, bin_cents, moms, n_bs=100,\n",
    "                         plot_log=False):\n",
    "    if isinstance(plot_log, bool):\n",
    "        plot_log = [plot_log]*len(moms)\n",
    "\n",
    "    fig = plt.figure(figsize=(len(moms)*4,10))\n",
    "    axs = fig.subplots(nrows=3, ncols=len(moms))\n",
    "\n",
    "    for k,m in enumerate(moms):\n",
    "        vm_nbody = compute_vmoment(orbs, m, bin_edges, n_bootstrap=n_bs)\n",
    "        vm_sample = compute_vmoment(sample, m, bin_edges, n_bootstrap=n_bs)\n",
    "\n",
    "        for l in range(3):\n",
    "            vl, vm, vu = np.percentile(vm_nbody[l], [16,50,84], axis=0)\n",
    "            axs[l,k].plot(bin_cents, vm, color='Black', alpha=1.0)\n",
    "            axs[l,k].fill_between(bin_cents, vl, vu, color='Black', \n",
    "                alpha=0.3)\n",
    "\n",
    "            vl, vm, vu = np.percentile(vm_sample[l], [16,50,84], axis=0)\n",
    "            axs[l,k].plot(bin_cents, vm, color='Red', alpha=1.0)\n",
    "            axs[l,k].fill_between(bin_cents, vl, vu, color='Red', \n",
    "                alpha=0.3)\n",
    "\n",
    "            axs[l,k].set_xscale('log')\n",
    "            if plot_log[k]: axs[l,k].set_yscale('log')\n",
    "\n",
    "            axs[0,k].set_ylabel(r'$\\langle v_r^'+str(m)+r' \\rangle$')\n",
    "            axs[1,k].set_ylabel(r'$\\langle v_\\phi^'+str(m)+r' \\rangle$')\n",
    "            axs[2,k].set_ylabel(r'$\\langle v_\\theta^'+str(m)+r' \\rangle$')\n",
    "\n",
    "        axs[2,k].set_xlabel(r'$r\\,(\\mathrm{kpc})$')\n",
    "    \n",
    "    return fig, axs\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the 4th order moment for the N-body data and DF samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating moment profiles for MW analog 30/30, merger 2/211\r"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "make_moment_plot = False\n",
    "\n",
    "# Pathing\n",
    "dens_fitting_dir = os.path.join(fitting_dir_base,'density_profile')\n",
    "df_fitting_dir = os.path.join(fitting_dir_base,'distribution_function')\n",
    "analysis_version = 'v1.1'\n",
    "analysis_dir = os.path.join(mw_analog_dir,'analysis',analysis_version)\n",
    "\n",
    "# Moment calculation keywords\n",
    "n_bs = 10\n",
    "raise_inverse_power = False\n",
    "\n",
    "# Get the sample orbits\n",
    "sample_data_cb = np.load(os.path.join(analysis_dir,'sample_data_cb.npy'),\n",
    "    allow_pickle=True)\n",
    "sample_data_om = np.load(os.path.join(analysis_dir,'sample_data_om.npy'),\n",
    "    allow_pickle=True)\n",
    "sample_data_om2 = np.load(os.path.join(analysis_dir,'sample_data_om2.npy'),\n",
    "    allow_pickle=True)\n",
    "\n",
    "mewd_data = []\n",
    "mewd_data_dtype_list = []\n",
    "make_mewd_data_dtype_list = True\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i != 12: continue\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    major_mergers = primary.tree_major_mergers\n",
    "    n_major = primary.n_major_mergers\n",
    "    n_snap = len(primary.snapnum)\n",
    "    primary_filename = primary.get_cutout_filename(mw_analog_dir,\n",
    "        snapnum=primary.snapnum[0])\n",
    "    co = pcutout.TNGCutout(primary_filename)\n",
    "    co.center_and_rectify()\n",
    "    pid = co.get_property('stars','ParticleIDs')\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j > 0: continue\n",
    "\n",
    "        if verbose: print(f'Calculating moment profiles for MW analog '\n",
    "                          f'{i+1}/{n_mw}, merger {j+1}/{n_major}', end='\\r')\n",
    "\n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_acc_sid = major_merger.subfind_id[0]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "        upid = major_merger.get_unique_particle_ids('stars',data_dir=data_dir)\n",
    "        indx = np.where(np.isin(pid,upid))[0]\n",
    "        orbs = co.get_orbs('stars')[indx]\n",
    "        n_star = len(orbs)\n",
    "        star_mass = co.get_masses('stars')[indx].to_value(apu.Msun)\n",
    "        r = orbs.r().to_value(apu.kpc)\n",
    "        r_softening = putil.get_softening_length('stars', z=0, physical=True)\n",
    "        rmin = np.max([np.min(r), r_softening])\n",
    "\n",
    "        # Define the adaptive binning keyword dict and then bin\n",
    "        n_bin = np.min([500, n_star//10]) # n per bin\n",
    "        adaptive_binning_kwargs = {'n':n_bin,\n",
    "                                   'rmin':rmin,\n",
    "                                   'rmax':np.max(r),\n",
    "                                   'bin_mode':'exact numbers',\n",
    "                                   'bin_equal_n':True,\n",
    "                                   'end_mode':'ignore',\n",
    "                                   'bin_cents_mode':'median'}\n",
    "        bin_edges, bin_cents, _ = pkin.get_radius_binning(orbs, \n",
    "            **adaptive_binning_kwargs)\n",
    "        bin_size = bin_edges[1:] - bin_edges[:-1]\n",
    "\n",
    "        # Loop over each set of DF samples and compute moments and statistics\n",
    "        sample_data_arr = [sample_data_cb,\n",
    "                           sample_data_om,\n",
    "                           sample_data_om2]\n",
    "        sample_data_suffix = ['cb','om','om2']\n",
    "        moms = [1,2,3,4]\n",
    "\n",
    "        _data = (\n",
    "            z0_sid,\n",
    "            major_acc_sid,\n",
    "            major_mlpid,\n",
    "            j+1,\n",
    "        )\n",
    "        if make_mewd_data_dtype_list:\n",
    "            mewd_data_dtype_list.extend([\n",
    "                ('z0_sid',int),\n",
    "                ('major_acc_sid',int),\n",
    "                ('major_mlpid',int),\n",
    "                ('merger_number',int),\n",
    "            ])\n",
    "\n",
    "        for k in range(len(sample_data_arr)):\n",
    "\n",
    "            sample_data = sample_data_arr[k]\n",
    "\n",
    "            # Mask the sample data correctly\n",
    "            mask = (sample_data['z0_sid'] == z0_sid) &\\\n",
    "                   (sample_data['major_acc_sid'] == major_acc_sid) &\\\n",
    "                   (sample_data['major_mlpid'] == major_mlpid) &\\\n",
    "                   (sample_data['merger_number'] == j+1)\n",
    "            indx = np.where(mask)[0]\n",
    "            assert len(indx) == 1, 'Something went wrong'\n",
    "            sample = sample_data[indx[0]]['sample']\n",
    "\n",
    "            # Make plots of the moments\n",
    "            if make_moment_plot:\n",
    "                # moms = [1,2,3,4]\n",
    "                plot_log = [False, True, False, True]\n",
    "                fig,axs = plot_velocity_moments(orbs, sample, bin_edges, \n",
    "                    bin_cents, moms, n_bs=n_bs, plot_log=plot_log)\n",
    "\n",
    "                fig.tight_layout()\n",
    "                this_fig_dir = os.path.join(fig_dir, str(z0_sid), \n",
    "                    'merger_'+str(j+1))\n",
    "                os.makedirs(this_fig_dir, exist_ok=True)\n",
    "                this_figname = os.path.join(this_fig_dir,\n",
    "                    'moments_'+'_'.join([str(m) for m in moms])+\\\n",
    "                    '_'+sample_data_suffix[k]+'.png')\n",
    "                fig.savefig(this_figname, dpi=300, bbox_inches='tight')\n",
    "                if not show_plots: plt.close(fig)\n",
    "                else: plt.show()\n",
    "            \n",
    "            # Loop over each moment and compute the MEWD\n",
    "            for l in range(len(moms)):\n",
    "                mewd = compute_mass_error_weighted_deviation_vmoment(orbs,\n",
    "                    sample, star_mass, moms[l], n_bs=n_bs, \n",
    "                    adaptive_binning_kwargs=adaptive_binning_kwargs,\n",
    "                    raise_inverse_power=raise_inverse_power)\n",
    "                _data += (mewd[0], mewd[1], mewd[2])\n",
    "                if make_mewd_data_dtype_list:\n",
    "                    mewd_data_dtype_list.extend([\n",
    "                        ('mewd_'+sample_data_suffix[k]+'_vr'+str(moms[l]),object),\n",
    "                        ('mewd_'+sample_data_suffix[k]+'_vp'+str(moms[l]),object),\n",
    "                        ('mewd_'+sample_data_suffix[k]+'_vt'+str(moms[l]),object),\n",
    "                    ])\n",
    "        \n",
    "        for l in range(len(moms)):\n",
    "            mewd_self = compute_mass_error_weighted_deviation_vmoment(\n",
    "                orbs, orbs, star_mass, moms[l], n_bs=n_bs, \n",
    "                adaptive_binning_kwargs=adaptive_binning_kwargs,\n",
    "                raise_inverse_power=raise_inverse_power)\n",
    "            _data += (mewd_self[0], mewd_self[1], mewd_self[2])\n",
    "            if make_mewd_data_dtype_list:\n",
    "                mewd_data_dtype_list.extend([\n",
    "                    ('mewd_self_vr'+str(moms[l]),object),\n",
    "                    ('mewd_self_vp'+str(moms[l]),object),\n",
    "                    ('mewd_self_vt'+str(moms[l]),object),\n",
    "                ])\n",
    "\n",
    "        mewd_data.append(_data)\n",
    "        make_mewd_data_dtype_list = False\n",
    "        \n",
    "\n",
    "# # Save the data as a pickle\n",
    "header = [mewd_data_dtype_list[i][0] for i in range(len(mewd_data_dtype_list))]\n",
    "mewd_data_filename = os.path.join(analysis_dir,\n",
    "    'mewd_data_vmom_'+'_'.join([str(m) for m in moms]))\n",
    "with open(mewd_data_filename+'.pkl','wb') as handle:\n",
    "    pickle.dump([header,mewd_data], handle)\n",
    "\n",
    "# Also save as a structured array\n",
    "mewd_data_dtype = np.dtype(mewd_data_dtype_list)\n",
    "mewd_data = np.array(mewd_data, dtype=mewd_data_dtype)\n",
    "np.save(os.path.join(analysis_dir,\n",
    "    'mewd_data_vmom_'+'_'.join([str(m) for m in moms])+'.npy'), mewd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the stashed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_version = 'v1.1'\n",
    "analysis_dir = os.path.join(mw_analog_dir,'analysis',analysis_version)\n",
    "\n",
    "# Or load the structured arrays\n",
    "moms = [1,2,3,4]\n",
    "mewd_data_filename = os.path.join(analysis_dir,\n",
    "    'mewd_data_vmom_'+'_'.join([str(m) for m in moms])+'.npy')\n",
    "mewd_data = np.load(mewd_data_filename, allow_pickle=True)\n",
    "\n",
    "# Load the merger information\n",
    "merger_data = np.load(os.path.join(analysis_dir,'merger_data.npy'), \n",
    "    allow_pickle=True)\n",
    "\n",
    "checks = True\n",
    "if checks:\n",
    "    assert np.all( mewd_data['z0_sid'] == merger_data['z0_sid'] ), \\\n",
    "        'Something went wrong'\n",
    "    assert np.all( mewd_data['major_acc_sid'] == merger_data['major_acc_sid'] ), \\\n",
    "        'Something went wrong'\n",
    "    assert np.all( mewd_data['major_mlpid'] == merger_data['major_mlpid'] ), \\\n",
    "        'Something went wrong'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show a comparison of the MEWD values, Moments 1-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnwidth, textwidth = pplot.get_latex_columnwidth_textwidth_inches()\n",
    "\n",
    "fig = plt.figure(figsize=(textwidth,5))\n",
    "axs = fig.subplots(nrows=3, ncols=4)\n",
    "\n",
    "df_names = ['cb','om','om2']\n",
    "velocity_names = ['vr','vp','vt']\n",
    "velocity_labels = [r'v_r',r'v_\\phi',r'v_\\theta']\n",
    "n_merger = len(mewd_data)\n",
    "xlims = [[0.4,2],\n",
    "         [0.4,2],\n",
    "         [0.4,2],\n",
    "         [0.4,2]]\n",
    "ylims = [[0.4,100],\n",
    "         [0.4,100],\n",
    "         [0.4,100],\n",
    "         [0.4,100]]\n",
    "marker_size = 4\n",
    "marker_alpha = 0.5\n",
    "ticklabel_fs = 8\n",
    "xaxis_label_fs = 10\n",
    "yaxis_label_fs = 10\n",
    "panel_label_fs = 8\n",
    "title_fs = 10\n",
    "\n",
    "for g in range(len(df_names)):\n",
    "    df_name = df_names[g]\n",
    "\n",
    "    for i in range(len(moms)):\n",
    "\n",
    "\n",
    "        for j in range(len(velocity_names)):\n",
    "            \n",
    "            x = mewd_data['mewd_self_'+velocity_names[j]+str(moms[i])]\n",
    "            y = mewd_data['mewd_'+df_name+'_'+velocity_names[j]+str(moms[i])]\n",
    "\n",
    "            for k in range(n_merger):\n",
    "                \n",
    "                xl, xm, xu = np.percentile(x[k], [16,50,84])\n",
    "                yl, ym, yu = np.percentile(y[k], [16,50,84])\n",
    "\n",
    "                # axs[j,i].errorbar(xm, ym, xerr=[[xm-xl],[xu-xm]],\n",
    "                #     yerr=[[ym-yl],[yu-ym]], fmt='o',\n",
    "                #     markersize=marker_size, color='Black', alpha=marker_alpha)\n",
    "                axs[j,i].scatter(xm, ym, s=marker_size, color='Black', \n",
    "                    alpha=marker_alpha)\n",
    "                axs[j,i].axline([0.1,0.1],[1,1], color='Black', linestyle='--')\n",
    "        \n",
    "            # axs[j,i].set_xscale('log')\n",
    "            axs[j,i].set_yscale('log')\n",
    "            axs[j,i].set_xlim(xlims[i])\n",
    "            axs[j,i].set_ylim(ylims[i])\n",
    "\n",
    "            if i != 0:\n",
    "                axs[j,i].tick_params(labelleft=False)\n",
    "            else:\n",
    "                axs[j,i].set_ylabel('$\\delta_{NB-MODEL}$', \n",
    "                    fontsize=yaxis_label_fs)\n",
    "                axs[j,i].tick_params(labelsize=ticklabel_fs)\n",
    "\n",
    "            if j != 2:\n",
    "                # pass\n",
    "                axs[j,i].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "                axs[j,i].xaxis.set_minor_formatter(plt.NullFormatter())\n",
    "                axs[j,i].tick_params(labelbottom=False)\n",
    "            else:\n",
    "                axs[j,i].set_xlabel('$\\delta_{NB-NB}$', fontsize=xaxis_label_fs)\n",
    "                axs[j,i].tick_params(labelsize=ticklabel_fs)\n",
    "\n",
    "            axs[j,i].text(0.99,0.95, \n",
    "                r'$\\langle '+velocity_labels[j]+r'^'+str(moms[i])+r'\\rangle$',\n",
    "                ha='right', va='top', transform=axs[j,i].transAxes, \n",
    "                fontsize=panel_label_fs)\n",
    "\n",
    "    fig.suptitle('DF '+df_name.upper(), fontsize=title_fs)\n",
    "    fig.tight_layout()\n",
    "    this_figname = os.path.join(local_fig_dir,\n",
    "        'mewd_vmom_comparison_'+df_name+'.png')\n",
    "    fig.savefig(this_figname, dpi=300, bbox_inches='tight')\n",
    "    if not show_plots: plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute histograms of the Nbody - DF delta divided by the Nbody - Nbody delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnwidth, textwidth = pplot.get_latex_columnwidth_textwidth_inches()\n",
    "\n",
    "fig = plt.figure(figsize=(textwidth,5))\n",
    "axs = fig.subplots(nrows=3, ncols=4)\n",
    "\n",
    "df_names = ['cb','om','om2']\n",
    "velocity_names = ['vr','vp','vt']\n",
    "velocity_labels = [r'v_r',r'v_\\phi',r'v_\\theta']\n",
    "n_merger = len(mewd_data)\n",
    "xlims = [[0.4,2],\n",
    "         [0.4,2],\n",
    "         [0.4,2],\n",
    "         [0.4,2]]\n",
    "ylims = [[0.4,100],\n",
    "         [0.4,100],\n",
    "         [0.4,100],\n",
    "         [0.4,100]]\n",
    "marker_size = 4\n",
    "marker_alpha = 0.5\n",
    "ticklabel_fs = 8\n",
    "xaxis_label_fs = 10\n",
    "yaxis_label_fs = 10\n",
    "panel_label_fs = 8\n",
    "title_fs = 10\n",
    "\n",
    "for g in range(len(df_names)):\n",
    "    df_name = df_names[g]\n",
    "\n",
    "    for i in range(len(moms)):\n",
    "\n",
    "\n",
    "        for j in range(len(velocity_names)):\n",
    "            \n",
    "            x = mewd_data['mewd_self_'+velocity_names[j]+str(moms[i])]\n",
    "            y = mewd_data['mewd_'+df_name+'_'+velocity_names[j]+str(moms[i])]\n",
    "\n",
    "            # Concatenate each of the arrays within x\n",
    "            xs = np.concatenate(x)\n",
    "\n",
    "            for k in range(n_merger):\n",
    "                \n",
    "                xl, xm, xu = np.percentile(x[k], [16,50,84])\n",
    "                yl, ym, yu = np.percentile(y[k], [16,50,84])\n",
    "\n",
    "                # axs[j,i].errorbar(xm, ym, xerr=[[xm-xl],[xu-xm]],\n",
    "                #     yerr=[[ym-yl],[yu-ym]], fmt='o',\n",
    "                #     markersize=marker_size, color='Black', alpha=marker_alpha)\n",
    "                axs[j,i].scatter(xm, ym, s=marker_size, color='Black', \n",
    "                    alpha=marker_alpha)\n",
    "                axs[j,i].axline([0.1,0.1],[1,1], color='Black', linestyle='--')\n",
    "        \n",
    "            # axs[j,i].set_xscale('log')\n",
    "            axs[j,i].set_yscale('log')\n",
    "            axs[j,i].set_xlim(xlims[i])\n",
    "            axs[j,i].set_ylim(ylims[i])\n",
    "\n",
    "            if i != 0:\n",
    "                axs[j,i].tick_params(labelleft=False)\n",
    "            else:\n",
    "                axs[j,i].set_ylabel('$\\delta_{NB-MODEL}$', \n",
    "                    fontsize=yaxis_label_fs)\n",
    "                axs[j,i].tick_params(labelsize=ticklabel_fs)\n",
    "\n",
    "            if j != 2:\n",
    "                # pass\n",
    "                axs[j,i].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "                axs[j,i].xaxis.set_minor_formatter(plt.NullFormatter())\n",
    "                axs[j,i].tick_params(labelbottom=False)\n",
    "            else:\n",
    "                axs[j,i].set_xlabel('$\\delta_{NB-NB}$', fontsize=xaxis_label_fs)\n",
    "                axs[j,i].tick_params(labelsize=ticklabel_fs)\n",
    "\n",
    "            axs[j,i].text(0.99,0.95, \n",
    "                r'$\\langle '+velocity_labels[j]+r'^'+str(moms[i])+r'\\rangle$',\n",
    "                ha='right', va='top', transform=axs[j,i].transAxes, \n",
    "                fontsize=panel_label_fs)\n",
    "\n",
    "    fig.suptitle('DF '+df_name.upper(), fontsize=title_fs)\n",
    "    fig.tight_layout()\n",
    "    this_figname = os.path.join(local_fig_dir,\n",
    "        'mewd_vmom_comparison_'+df_name+'.png')\n",
    "    fig.savefig(this_figname, dpi=300, bbox_inches='tight')\n",
    "    if not show_plots: plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
