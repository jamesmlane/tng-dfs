{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# TITLE - 1_anisotropic_dfs.ipynb\n",
    "# AUTHOR - James Lane\n",
    "# PROJECT - tng-dfs\n",
    "#\n",
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# Docstrings and metadata:\n",
    "'''Use density profile fits to construct anisotropic DFs\n",
    "'''\n",
    "\n",
    "__author__ = \"James Lane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../src/nb_modules/nb_imports.txt\n",
    "### Imports\n",
    "\n",
    "## Basic\n",
    "import numpy as np\n",
    "import sys, os, dill as pickle\n",
    "import pdb, copy, glob, time, warnings\n",
    "\n",
    "## Matplotlib\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## Astropy\n",
    "from astropy import units as apu\n",
    "from astropy import constants as apc\n",
    "\n",
    "## Analysis\n",
    "import scipy.stats\n",
    "import scipy.interpolate\n",
    "\n",
    "## galpy\n",
    "from galpy import orbit\n",
    "from galpy import potential\n",
    "from galpy import actionAngle as aA\n",
    "from galpy import df\n",
    "from galpy import util as gputil\n",
    "\n",
    "## Project-specific\n",
    "src_path = 'src/'\n",
    "while True:\n",
    "    if os.path.exists(src_path): break\n",
    "    if os.path.realpath(src_path).split('/')[-1] in ['tng-dfs','/']:\n",
    "            raise FileNotFoundError('Failed to find src/ directory.')\n",
    "    src_path = os.path.join('..',src_path)\n",
    "sys.path.insert(0,src_path)\n",
    "from tng_dfs import cutout as pcutout\n",
    "from tng_dfs import densprofile as pdens\n",
    "from tng_dfs import fitting as pfit\n",
    "from tng_dfs import kinematics as pkin\n",
    "from tng_dfs import util as putil\n",
    "\n",
    "### Notebook setup\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(os.path.join(src_path,'mpl/project.mplstyle')) # This must be exactly here\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords, loading, pathing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../src/nb_modules/nb_setup.txt\n",
    "# Keywords\n",
    "cdict = putil.load_config_to_dict()\n",
    "keywords = ['DATA_DIR','MW_ANALOG_DIR','RO','VO','ZO','LITTLE_H',\n",
    "            'MW_MASS_RANGE']\n",
    "data_dir,mw_analog_dir,ro,vo,zo,h,mw_mass_range = \\\n",
    "    putil.parse_config_dict(cdict,keywords)\n",
    "\n",
    "# MW Analog \n",
    "mwsubs,mwsubs_vars = putil.prepare_mwsubs(mw_analog_dir,h=h,\n",
    "    mw_mass_range=mw_mass_range,return_vars=True,force_mwsubs=False,\n",
    "    bulge_disk_fraction_cuts=True)\n",
    "\n",
    "# Figure path\n",
    "fig_dir = './fig/sample/'\n",
    "epsen_fig_dir = '/epsen_data/scr/lane/projects/tng-dfs/figs/notebooks/sample/'\n",
    "os.makedirs(fig_dir,exist_ok=True)\n",
    "os.makedirs(epsen_fig_dir,exist_ok=True)\n",
    "show_plots = False\n",
    "\n",
    "# Load tree data\n",
    "tree_primary_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_primaries.pkl')\n",
    "with open(tree_primary_filename,'rb') as handle: \n",
    "    tree_primaries = pickle.load(handle)\n",
    "tree_major_mergers_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_major_mergers.pkl')\n",
    "with open(tree_major_mergers_filename,'rb') as handle:\n",
    "    tree_major_mergers = pickle.load(handle)\n",
    "n_mw = len(tree_primaries)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all the density profile fits and construct the constant anisotropy distribution functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ignore some standard warnings\n",
    "warnings.filterwarnings(action='ignore', message='No particle IDs found', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', message='maxiter', category=scipy.integrate.AccuracyWarning)\n",
    "warnings.filterwarnings(action='ignore', message='invalid value encountered', category=RuntimeWarning)\n",
    "\n",
    "force_df = True\n",
    "test_pickling = True\n",
    "verbose = True\n",
    "epsen_dens_fitting_dir = '/epsen_data/scr/lane/projects/tng-dfs/fitting/'+\\\n",
    "    'density_profile/'\n",
    "epsen_df_fitting_dir = '/epsen_data/scr/lane/projects/tng-dfs/fitting/'+\\\n",
    "    'distribution_function/'\n",
    "\n",
    "# DM halo information\n",
    "dm_halo_version = 'poisson_nfw'\n",
    "dm_halo_ncut = 500\n",
    "\n",
    "# Stellar bulge and disk information\n",
    "stellar_bulge_disk_version = 'miyamoto_disk_pswc_bulge_tps_halo'\n",
    "stellar_bulge_disk_ncut = 2000\n",
    "\n",
    "# Stellar halo density information\n",
    "stellar_halo_density_version = 'poisson_twopower'\n",
    "stellar_halo_density_ncut = 500\n",
    "\n",
    "# Stellar halo rotation information\n",
    "stellar_halo_rotation_version = 'tanh_rotation'\n",
    "stellar_halo_rotation_ncut = 500\n",
    "\n",
    "# Beta information\n",
    "beta_version = 'beta_constant'\n",
    "beta_ncut = 500\n",
    "\n",
    "# Define density profiles\n",
    "dm_halo_densfunc = pdens.NFWSpherical()\n",
    "disk_densfunc = pdens.MiyamotoNagaiDisk()\n",
    "bulge_densfunc = pdens.SinglePowerCutoffSpherical()\n",
    "stellar_halo_densfunc = pdens.TwoPowerSpherical()\n",
    "stellar_bulge_disk_densfunc = pdens.CompositeDensityProfile(\n",
    "    [disk_densfunc,\n",
    "     bulge_densfunc,\n",
    "     pdens.TwoPowerSpherical()]\n",
    "     )\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i > 0: continue\n",
    "    if verbose: print(f'Analyzing MW {i+1}/{n_mw}')\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    major_mergers = primary.tree_major_mergers\n",
    "    n_major = primary.n_major_mergers\n",
    "    n_snap = len(primary.snapnum)\n",
    "    primary_filename = primary.get_cutout_filename(mw_analog_dir,\n",
    "        snapnum=primary.snapnum[0])\n",
    "    co = pcutout.TNGCutout(primary_filename)\n",
    "    co.center_and_rectify()\n",
    "    pid = co.get_property('stars','ParticleIDs')\n",
    "\n",
    "    # Load the interpolator for the sphericalized potential\n",
    "    interpolator_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "        'spherical_interpolated_potential/',str(z0_sid),'interp_potential.pkl')\n",
    "    with open(interpolator_filename,'rb') as handle:\n",
    "        interpot = pickle.load(handle)\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j > 0: continue\n",
    "        if verbose: print(f'  Constructing DF for major merger {j+1}/{n_major}')\n",
    "\n",
    "        # Filename and pathing check\n",
    "        this_fitting_dir = os.path.join(epsen_df_fitting_dir,'constant_beta',\n",
    "            str(z0_sid),'merger_'+str(j+1))\n",
    "        os.makedirs(this_fitting_dir,exist_ok=True)\n",
    "        df_filename = os.path.join(this_fitting_dir,'df.pkl')\n",
    "        if os.path.exists(df_filename) and not force_df:\n",
    "            print('  Already have DF, continuing')\n",
    "            continue\n",
    "            \n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_acc_sid = major_merger.subfind_id[0]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "        upid = major_merger.get_unique_particle_ids('stars',data_dir=data_dir)\n",
    "        indx = np.where(np.isin(pid,upid))[0]\n",
    "        orbs = co.get_orbs('stars')[indx]\n",
    "        rs = orbs.r().to(apu.kpc).value\n",
    "        # n_star = len(orbs)\n",
    "\n",
    "        # Get the stellar halo density profile (denspot for the DF)\n",
    "        stellar_halo_density_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "            'stellar_halo/',stellar_halo_density_version,str(z0_sid),\n",
    "            'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        denspot = pfit.construct_pot_from_fit(\n",
    "            stellar_halo_density_filename, stellar_halo_densfunc, \n",
    "            stellar_halo_density_ncut, ro=ro, vo=vo)\n",
    "\n",
    "        # # Get the stellar halo density profile\n",
    "        # stellar_halo_density_dir = os.path.join(epsen_dens_fitting_dir,\n",
    "        #     'stellar_halo/',stellar_halo_density_version,str(z0_sid),\n",
    "        #     'merger_'+str(j+1)+'/')\n",
    "        # stellar_halo_density_filename = os.path.join(stellar_halo_density_dir,\n",
    "        #     'sampler.pkl')\n",
    "        # assert os.path.exists(stellar_halo_density_filename)\n",
    "        # with open(stellar_halo_density_filename,'rb') as handle:\n",
    "        #     stellar_halo_density_sampler = pickle.load(handle)\n",
    "        # stellar_halo_density_samples = stellar_halo_density_sampler.get_chain(\n",
    "        #     discard=stellar_halo_density_ncut, flat=True)\n",
    "        # # Params are alpha, beta, a, amp\n",
    "        # stellar_halo_alpha, stellar_halo_beta, stellar_halo_a, stellar_halo_amp = \\\n",
    "        #     np.median(stellar_halo_density_samples,axis=0)\n",
    "        \n",
    "        # Get the stellar halo beta information\n",
    "        beta_dir = os.path.join(epsen_dens_fitting_dir,'stellar_halo/',beta_version,\n",
    "            str(z0_sid),'merger_'+str(j+1)+'/')\n",
    "        beta_filename = os.path.join(beta_dir,'sampler.pkl')\n",
    "        assert os.path.exists(beta_filename)\n",
    "        with open(beta_filename,'rb') as handle:\n",
    "            beta_sampler = pickle.load(handle)\n",
    "        beta_samples = beta_sampler.get_chain(discard=beta_ncut, flat=True)\n",
    "        beta = np.median(beta_samples,axis=0)[0]\n",
    "        if beta < -5: \n",
    "            print('    Beta < -5, setting beta=-5')\n",
    "            beta = -5\n",
    "        if beta >= 1.: \n",
    "            print('    Beta >= 1, setting beta=0.9')\n",
    "            beta = 0.9\n",
    "        \n",
    "        # Construct the distribution function and do some dummy sampling\n",
    "        # to set the interpolators. Then save.\n",
    "        try:\n",
    "            print(f'  Beta: {round(beta,3)}')\n",
    "            print('  Building DF')\n",
    "            dfcb = df.constantbetadf(pot=interpot, denspot=denspot, beta=beta, ro=ro, \n",
    "                vo=vo, rmax=rs.max()*apu.kpc*1.1)\n",
    "            print('  Sampling DF')\n",
    "            _ = dfcb.sample(n=100, rmin=rs.min()*apu.kpc*0.9)\n",
    "        except Exception as e:\n",
    "            print('Caught an error:',e,'skipping...') \n",
    "\n",
    "        # Filename built above\n",
    "        if test_pickling:\n",
    "            try:\n",
    "                pickle.loads(pickle.dumps(dfcb))\n",
    "            except RecursionError:\n",
    "                print('Caught recursion error when pickle/unpickling, quiting...')\n",
    "                sys.exit()\n",
    "        with open(df_filename,'wb') as handle:\n",
    "            pickle.dump(dfcb,handle)\n",
    "\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also pickle the f(E) interpolator separately because it's the expensive thing to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_mw):\n",
    "    # if i > 0: continue\n",
    "    if verbose: print(f'Analyzing MW {i+1}/{n_mw}')\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    n_major = primary.n_major_mergers\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j > 0: continue\n",
    "        if verbose: print(f'  Saving f(E) interpolator for major merger {j+1}/{n_major}')\n",
    "\n",
    "        # Filename and pathing check\n",
    "        this_fitting_dir = os.path.join(epsen_df_fitting_dir,'constant_beta',\n",
    "            str(z0_sid),'merger_'+str(j+1))\n",
    "        df_filename = os.path.join(this_fitting_dir,'df.pkl')\n",
    "\n",
    "        if not os.path.exists(df_filename):\n",
    "            print('Pickled DF not found, skipping')\n",
    "            continue\n",
    "\n",
    "        with open(df_filename, 'rb') as handle:\n",
    "            dfcb = pickle.load(handle)\n",
    "\n",
    "        # Pull the f(E) interpolator\n",
    "        fE_interp = dfcb._fE_interp\n",
    "        fE_interp_filename = os.path.join(this_fitting_dir, 'fE_interp.pkl')\n",
    "        with open(fE_interp_filename, 'wb') as handle:\n",
    "            pickle.dump(fE_interp, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct and save the Osipkov-Merritt distribution functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore some standard warnings\n",
    "warnings.filterwarnings(action='ignore', message='No particle IDs found', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', message='maxiter', category=scipy.integrate.AccuracyWarning)\n",
    "warnings.filterwarnings(action='ignore', message='invalid value encountered', category=RuntimeWarning)\n",
    "\n",
    "force_df = True\n",
    "test_pickling = True\n",
    "verbose = True\n",
    "epsen_dens_fitting_dir = '/epsen_data/scr/lane/projects/tng-dfs/fitting/'+\\\n",
    "    'density_profile/'\n",
    "epsen_df_fitting_dir = '/epsen_data/scr/lane/projects/tng-dfs/fitting/'+\\\n",
    "    'distribution_function/'\n",
    "\n",
    "# DM halo information\n",
    "dm_halo_version = 'poisson_nfw'\n",
    "dm_halo_ncut = 500\n",
    "\n",
    "# Stellar bulge and disk information\n",
    "stellar_bulge_disk_version = 'miyamoto_disk_pswc_bulge_tps_halo'\n",
    "stellar_bulge_disk_ncut = 2000\n",
    "\n",
    "# Stellar halo density information\n",
    "stellar_halo_density_version = 'poisson_twopower'\n",
    "stellar_halo_density_ncut = 500\n",
    "\n",
    "# Stellar halo rotation information\n",
    "stellar_halo_rotation_version = 'tanh_rotation'\n",
    "stellar_halo_rotation_ncut = 500\n",
    "\n",
    "# Beta information\n",
    "beta_version = 'beta_osipkov_merritt'\n",
    "beta_ncut = 500\n",
    "\n",
    "# Define density profiles\n",
    "dm_halo_densfunc = pdens.NFWSpherical()\n",
    "disk_densfunc = pdens.MiyamotoNagaiDisk()\n",
    "bulge_densfunc = pdens.SinglePowerCutoffSpherical()\n",
    "stellar_halo_densfunc = pdens.TwoPowerSpherical()\n",
    "stellar_bulge_disk_densfunc = pdens.CompositeDensityProfile(\n",
    "    [disk_densfunc,\n",
    "     bulge_densfunc,\n",
    "     pdens.TwoPowerSpherical()]\n",
    "     )\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i > 0: continue\n",
    "    if verbose: print(f'Analyzing MW {i+1}/{n_mw}')\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    major_mergers = primary.tree_major_mergers\n",
    "    n_major = primary.n_major_mergers\n",
    "    n_snap = len(primary.snapnum)\n",
    "    primary_filename = primary.get_cutout_filename(mw_analog_dir,\n",
    "        snapnum=primary.snapnum[0])\n",
    "    co = pcutout.TNGCutout(primary_filename)\n",
    "    co.center_and_rectify()\n",
    "    pid = co.get_property('stars','ParticleIDs')\n",
    "\n",
    "    # Load the interpolator for the sphericalized potential\n",
    "    interpolator_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "        'spherical_interpolated_potential/',str(z0_sid),'interp_potential.pkl')\n",
    "    with open(interpolator_filename,'rb') as handle:\n",
    "        interpot = pickle.load(handle)\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j > 0: continue\n",
    "        if verbose: print(f'  Constructing DF for major merger {j+1}/{n_major}')\n",
    "\n",
    "        # Filename and pathing check\n",
    "        this_fitting_dir = os.path.join(epsen_df_fitting_dir,'osipkov_merritt',\n",
    "            str(z0_sid),'merger_'+str(j+1))\n",
    "        os.makedirs(this_fitting_dir,exist_ok=True)\n",
    "        df_filename = os.path.join(this_fitting_dir,'df.pkl')\n",
    "        if os.path.exists(df_filename) and not force_df:\n",
    "            print('  Already have DF, continuing')\n",
    "            continue\n",
    "            \n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_acc_sid = major_merger.subfind_id[0]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "        upid = major_merger.get_unique_particle_ids('stars',data_dir=data_dir)\n",
    "        indx = np.where(np.isin(pid,upid))[0]\n",
    "        orbs = co.get_orbs('stars')[indx]\n",
    "        rs = orbs.r().to(apu.kpc).value\n",
    "        # n_star = len(orbs)\n",
    "\n",
    "        # Get the stellar halo density profile (denspot for the DF)\n",
    "        stellar_halo_density_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "            'stellar_halo/',stellar_halo_density_version,str(z0_sid),\n",
    "            'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        denspot = pfit.construct_pot_from_fit(\n",
    "            stellar_halo_density_filename, stellar_halo_densfunc, \n",
    "            stellar_halo_density_ncut, ro=ro, vo=vo)\n",
    "\n",
    "        # # Get the stellar halo density profile\n",
    "        # stellar_halo_density_dir = os.path.join(epsen_dens_fitting_dir,\n",
    "        #     'stellar_halo/',stellar_halo_density_version,str(z0_sid),\n",
    "        #     'merger_'+str(j+1)+'/')\n",
    "        # stellar_halo_density_filename = os.path.join(stellar_halo_density_dir,\n",
    "        #     'sampler.pkl')\n",
    "        # assert os.path.exists(stellar_halo_density_filename)\n",
    "        # with open(stellar_halo_density_filename,'rb') as handle:\n",
    "        #     stellar_halo_density_sampler = pickle.load(handle)\n",
    "        # stellar_halo_density_samples = stellar_halo_density_sampler.get_chain(\n",
    "        #     discard=stellar_halo_density_ncut, flat=True)\n",
    "        # # Params are alpha, beta, a, amp\n",
    "        # stellar_halo_alpha, stellar_halo_beta, stellar_halo_a, stellar_halo_amp = \\\n",
    "        #     np.median(stellar_halo_density_samples,axis=0)\n",
    "        \n",
    "        # Get the stellar halo beta information\n",
    "        om_dir = os.path.join(epsen_dens_fitting_dir,'stellar_halo/',beta_version,\n",
    "            str(z0_sid),'merger_'+str(j+1)+'/')\n",
    "        om_filename = os.path.join(om_dir,'sampler.pkl')\n",
    "        assert os.path.exists(om_filename)\n",
    "        with open(om_filename,'rb') as handle:\n",
    "            om_sampler = pickle.load(handle)\n",
    "        ra_samples = om_sampler.get_chain(discard=beta_ncut, flat=True)\n",
    "        ra = np.median(ra_samples,axis=0)[0]\n",
    "        if ra < 0: \n",
    "            print('    ra < 0, skipping...')\n",
    "            continue\n",
    "        \n",
    "        # Construct the distribution function and do some dummy sampling\n",
    "        # to set the interpolators. Then save.\n",
    "        try:\n",
    "            print(f'  r_a: {round(ra,3)}')\n",
    "            print('  Building DF')\n",
    "            dfom = df.osipkovmerrittdf(pot=interpot, denspot=denspot, \n",
    "                ra=ra*apu.kpc, ro=ro, vo=vo, rmax=rs.max()*apu.kpc*1.1)\n",
    "            print('  Sampling DF')\n",
    "            _ = dfom.sample(n=100, rmin=rs.min()*apu.kpc*0.9)\n",
    "        except Exception as e:\n",
    "            print('Caught an error:',e,'skipping...') \n",
    "\n",
    "        # Filename built above\n",
    "        if test_pickling:\n",
    "            try:\n",
    "                pickle.loads(pickle.dumps(dfom))\n",
    "            except RecursionError:\n",
    "                print('Caught recursion error when pickle/unpickling, quiting...')\n",
    "                sys.exit()\n",
    "        with open(df_filename,'wb') as handle:\n",
    "            pickle.dump(dfom,handle)\n",
    "\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also pickle the f(Q) interpolator separately because it's the expensive thing to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_mw):\n",
    "    # if i > 0: continue\n",
    "    if verbose: print(f'Analyzing MW {i+1}/{n_mw}')\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    n_major = primary.n_major_mergers\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j > 0: continue\n",
    "        if verbose: print(f'  Saving f(Q) interpolator for major merger {j+1}/{n_major}')\n",
    "\n",
    "        # Filename and pathing check\n",
    "        this_fitting_dir = os.path.join(epsen_df_fitting_dir,'osipkov_merritt',\n",
    "            str(z0_sid),'merger_'+str(j+1))\n",
    "        df_filename = os.path.join(this_fitting_dir,'df.pkl')\n",
    "\n",
    "        if not os.path.exists(df_filename):\n",
    "            print('Pickled DF not found, skipping')\n",
    "            continue\n",
    "\n",
    "        with open(df_filename, 'rb') as handle:\n",
    "            dfom = pickle.load(handle)\n",
    "\n",
    "        # Pull the f(Q) interpolator\n",
    "        fQ_interp = dfom._logfQ_interp\n",
    "        fQ_interp_filename = os.path.join(this_fitting_dir, 'fQ_interp.pkl')\n",
    "        with open(fQ_interp_filename, 'wb') as handle:\n",
    "            pickle.dump(fQ_interp, handle)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
