{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# TITLE - 1_construct_anisotropic_dfs.ipynb\n",
    "# AUTHOR - James Lane\n",
    "# PROJECT - tng-dfs\n",
    "#\n",
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# Docstrings and metadata:\n",
    "'''Use density profile fits to construct anisotropic DFs\n",
    "'''\n",
    "\n",
    "__author__ = \"James Lane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control jax platform\n",
    "import jax\n",
    "jax.config.update('jax_platform_name', 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../src/nb_modules/nb_imports.txt\n",
    "### Imports\n",
    "\n",
    "## Basic\n",
    "import numpy as np\n",
    "import sys, os, dill as pickle\n",
    "import pdb, copy, glob, time, warnings, logging\n",
    "\n",
    "## Matplotlib\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## Astropy\n",
    "from astropy import units as apu\n",
    "from astropy import constants as apc\n",
    "\n",
    "## Analysis\n",
    "import scipy.stats\n",
    "import scipy.interpolate\n",
    "\n",
    "## galpy\n",
    "from galpy import orbit\n",
    "from galpy import potential\n",
    "from galpy import actionAngle as aA\n",
    "from galpy import df\n",
    "from galpy import util as gputil\n",
    "\n",
    "## Project-specific\n",
    "src_path = 'src/'\n",
    "while True:\n",
    "    if os.path.exists(src_path): break\n",
    "    if os.path.realpath(src_path).split('/')[-1] in ['tng-dfs','/']:\n",
    "            raise FileNotFoundError('Failed to find src/ directory.')\n",
    "    src_path = os.path.join('..',src_path)\n",
    "sys.path.insert(0,src_path)\n",
    "from tng_dfs import cutout as pcutout\n",
    "from tng_dfs import densprofile as pdens\n",
    "from tng_dfs import fitting as pfit\n",
    "from tng_dfs import io as pio\n",
    "from tng_dfs import kinematics as pkin\n",
    "from tng_dfs import util as putil\n",
    "\n",
    "### Notebook setup\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(os.path.join(src_path,'mpl/project.mplstyle')) # This must be exactly here\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords, loading, pathing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../src/nb_modules/nb_setup.txt\n",
    "# Keywords\n",
    "cdict = putil.load_config_to_dict()\n",
    "keywords = ['DATA_DIR','MW_ANALOG_DIR','FIG_DIR_BASE','FITTING_DIR_BASE',\n",
    "            'RO','VO','ZO','LITTLE_H','MW_MASS_RANGE']\n",
    "data_dir,mw_analog_dir,fig_dir_base,fitting_dir_base,ro,vo,zo,h,\\\n",
    "    mw_mass_range = putil.parse_config_dict(cdict,keywords)\n",
    "\n",
    "# MW Analog \n",
    "mwsubs,mwsubs_vars = putil.prepare_mwsubs(mw_analog_dir,h=h,\n",
    "    mw_mass_range=mw_mass_range,return_vars=True,force_mwsubs=False,\n",
    "    bulge_disk_fraction_cuts=True)\n",
    "\n",
    "# Figure path\n",
    "local_fig_dir = './fig/'\n",
    "fig_dir = os.path.join(fig_dir_base, \n",
    "    'notebooks/5_compare_distribution_functions/1_construct_anisotropic_dfs/')\n",
    "os.makedirs(local_fig_dir,exist_ok=True)\n",
    "os.makedirs(fig_dir,exist_ok=True)\n",
    "show_plots = False\n",
    "\n",
    "# Load tree data\n",
    "tree_primary_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_primaries.pkl')\n",
    "with open(tree_primary_filename,'rb') as handle: \n",
    "    tree_primaries = pickle.load(handle)\n",
    "tree_major_mergers_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_major_mergers.pkl')\n",
    "with open(tree_major_mergers_filename,'rb') as handle:\n",
    "    tree_major_mergers = pickle.load(handle)\n",
    "n_mw = len(tree_primaries)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct and save the constant anisotropy distribution functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Some keywords and properties\n",
    "force_df = True\n",
    "test_pickling = True\n",
    "verbose = True\n",
    "dens_fitting_dir = os.path.join(fitting_dir_base,'density_profile/')\n",
    "df_fitting_dir = os.path.join(fitting_dir_base,'distribution_function/')\n",
    "\n",
    "# Stellar halo density information\n",
    "stellar_halo_density_version = 'poisson_twopower_softening'\n",
    "stellar_halo_density_ncut = 500\n",
    "\n",
    "# Stellar halo rotation information\n",
    "stellar_halo_rotation_version = 'tanh_rotation'\n",
    "stellar_halo_rotation_ncut = 500\n",
    "stellar_halo_densfunc = pdens.TwoPowerSpherical()\n",
    "\n",
    "# Anisotropy information\n",
    "df_type = 'constant_beta'\n",
    "anisotropy_fit_version = 'anisotropy_params_softening'\n",
    "anisotropy_ncut = 500\n",
    "\n",
    "# DF versioning\n",
    "df_version = 'df_density_softening'\n",
    "\n",
    "# Ignore some standard warnings\n",
    "warnings.filterwarnings(action='ignore', \n",
    "    message='No particle IDs found', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', \n",
    "    message='maxiter', category=scipy.integrate.AccuracyWarning)\n",
    "warnings.filterwarnings(action='ignore', \n",
    "    message='invalid value encountered', category=RuntimeWarning)\n",
    "\n",
    "# Begin logging\n",
    "os.makedirs('./log/',exist_ok=True)\n",
    "log_filename = './log/1_construct_constant_beta_dfs.log'\n",
    "if os.path.exists(log_filename):\n",
    "    os.remove(log_filename)\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO, filemode='w', \n",
    "    force=True)\n",
    "logging.info('Beginning constant anisotropy DF creation. Time: '+\\\n",
    "             time.strftime('%a, %d %b %Y %H:%M:%S',time.localtime()))\n",
    "\n",
    "for i in range(n_mw):\n",
    "    if i > 0: continue\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    major_mergers = primary.tree_major_mergers\n",
    "    n_major = primary.n_major_mergers\n",
    "    n_snap = len(primary.snapnum)\n",
    "    primary_filename = primary.get_cutout_filename(mw_analog_dir,\n",
    "        snapnum=primary.snapnum[0])\n",
    "    co = pcutout.TNGCutout(primary_filename)\n",
    "    co.center_and_rectify()\n",
    "    pid = co.get_property('stars','ParticleIDs')\n",
    "\n",
    "    # Load the interpolator for the sphericalized potential\n",
    "    interpolator_filename = os.path.join(dens_fitting_dir,\n",
    "        'spherical_interpolated_potential/',str(z0_sid),'interp_potential.pkl')\n",
    "    with open(interpolator_filename,'rb') as handle:\n",
    "        interpot = pickle.load(handle)\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j > 0: continue\n",
    "        if verbose:\n",
    "            msg = f'Analyzing major merger {j+1}/{n_major} for MW {i+1}/{n_mw}'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "\n",
    "        # Filename and pathing check\n",
    "        this_fitting_dir = os.path.join(df_fitting_dir,df_type,df_version,\n",
    "            str(z0_sid),'merger_'+str(j+1))\n",
    "        os.makedirs(this_fitting_dir,exist_ok=True)\n",
    "        df_filename = os.path.join(this_fitting_dir,'df.pkl')\n",
    "        if os.path.exists(df_filename) and not force_df:\n",
    "            print('  Already have DF, continuing')\n",
    "            continue\n",
    "            \n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_acc_sid = major_merger.subfind_id[0]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "        upid = major_merger.get_unique_particle_ids('stars',data_dir=data_dir)\n",
    "        indx = np.where(np.isin(pid,upid))[0]\n",
    "        orbs = co.get_orbs('stars')[indx]\n",
    "        rs = orbs.r().to(apu.kpc).value\n",
    "        # n_star = len(orbs)\n",
    "\n",
    "        # Get the stellar halo density profile (denspot for the DF)\n",
    "        stellar_halo_density_filename = os.path.join(dens_fitting_dir,\n",
    "            'stellar_halo/',stellar_halo_density_version,str(z0_sid),\n",
    "            'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        denspot = pfit.construct_pot_from_fit(\n",
    "            stellar_halo_density_filename, stellar_halo_densfunc, \n",
    "            stellar_halo_density_ncut, ro=ro, vo=vo)\n",
    "        \n",
    "        # Get the stellar halo beta information\n",
    "        anisotropy_param_dir = os.path.join(df_fitting_dir, df_type,\n",
    "                anisotropy_fit_version, str(z0_sid),'merger_'+str(j+1))\n",
    "        anisotropy_filename = os.path.join(anisotropy_param_dir,'sampler.pkl')\n",
    "        beta = pio.median_params_from_emcee_sampler(anisotropy_filename,\n",
    "            ncut=anisotropy_ncut)[0][0]\n",
    "\n",
    "        # Construct the distribution function and do some dummy sampling\n",
    "        # to set the interpolators. Then save.\n",
    "        try:\n",
    "            if verbose:\n",
    "                msg = f'Beta: {round(beta,3)}, building DF'\n",
    "                logging.info(msg)\n",
    "                print(msg)\n",
    "            dfcb = df.constantbetadf(pot=interpot, denspot=denspot, beta=beta, ro=ro, \n",
    "                vo=vo, rmax=rs.max()*apu.kpc*1.1)\n",
    "            if verbose:\n",
    "                msg = 'Sampling DF'\n",
    "                logging.info(msg)\n",
    "                print(msg)\n",
    "            _ = dfcb.sample(n=100, rmin=rs.min()*apu.kpc*0.9)\n",
    "        except Exception as e:\n",
    "            msg = f'Failed to build DF, skipping. Error: {e}'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "\n",
    "        # Filename built above\n",
    "        if test_pickling:\n",
    "            try:\n",
    "                pickle.loads(pickle.dumps(dfcb))\n",
    "            except RecursionError:\n",
    "                if verbose:\n",
    "                    msg = 'Caught recursion error when (un)pickling, quiting.'\n",
    "                    logging.info(msg)\n",
    "                    print(msg)\n",
    "                sys.exit()\n",
    "        with open(df_filename,'wb') as handle:\n",
    "            pickle.dump(dfcb,handle)\n",
    "        \n",
    "        if verbose:\n",
    "            msg = f'Done with merger'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also pickle the f(E) interpolator separately because it's the expensive thing to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(n_mw):\n",
    "    # if i > 0: continue\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    n_major = primary.n_major_mergers\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j > 0: continue\n",
    "        if verbose:\n",
    "            print(f'Saving f(E) interpolator for major merger {j+1}/{n_major}'\n",
    "                  f' of MW {i+1}/{n_mw}')\n",
    "\n",
    "        # Filename and pathing check\n",
    "        this_fitting_dir = os.path.join(df_fitting_dir,df_type,\n",
    "            str(z0_sid),'merger_'+str(j+1))\n",
    "        df_filename = os.path.join(this_fitting_dir,'df.pkl')\n",
    "\n",
    "        if not os.path.exists(df_filename):\n",
    "            print('Pickled DF not found, skipping')\n",
    "            continue\n",
    "\n",
    "        with open(df_filename, 'rb') as handle:\n",
    "            dfcb = pickle.load(handle)\n",
    "\n",
    "        # Pull the f(E) interpolator\n",
    "        fE_interp = dfcb._fE_interp\n",
    "        fE_interp_filename = os.path.join(this_fitting_dir, 'fE_interp.pkl')\n",
    "        with open(fE_interp_filename, 'wb') as handle:\n",
    "            pickle.dump(fE_interp, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct and save the Osipkov-Merritt distribution functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some keywords and properties\n",
    "force_df = True\n",
    "test_pickling = True\n",
    "verbose = True\n",
    "dens_fitting_dir = os.path.join(fitting_dir_base,'density_profile/')\n",
    "df_fitting_dir = os.path.join(fitting_dir_base,'distribution_function/')\n",
    "\n",
    "# Stellar halo density information\n",
    "stellar_halo_density_version = 'poisson_twopower_softening'\n",
    "stellar_halo_density_ncut = 500\n",
    "stellar_halo_densfunc = pdens.TwoPowerSpherical()\n",
    "\n",
    "# Stellar halo rotation information\n",
    "stellar_halo_rotation_version = 'tanh_rotation'\n",
    "stellar_halo_rotation_ncut = 500\n",
    "\n",
    "# Anisotropy information\n",
    "df_type = 'osipkov_merritt'\n",
    "anisotropy_fit_version = 'anisotropy_params_softening'\n",
    "anisotropy_ncut = 500\n",
    "\n",
    "# DF versioning\n",
    "df_version = 'df_density_softening'\n",
    "\n",
    "# Ignore some standard warnings\n",
    "warnings.filterwarnings(action='ignore', \n",
    "    message='No particle IDs found', category=UserWarning)\n",
    "warnings.filterwarnings(action='ignore', \n",
    "    message='maxiter', category=scipy.integrate.AccuracyWarning)\n",
    "warnings.filterwarnings(action='ignore', \n",
    "    message='invalid value encountered', category=RuntimeWarning)\n",
    "\n",
    "# Begin logging\n",
    "log_filename = './log/1_construct_osipkov_merritt_dfs.log'\n",
    "if os.path.exists(log_filename):\n",
    "    os.remove(log_filename)\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO, filemode='w', \n",
    "    force=True)\n",
    "logging.info('Beginning Osipkov-Merritt DF creation. Time: '+\\\n",
    "             time.strftime('%a, %d %b %Y %H:%M:%S',time.localtime()))\n",
    "\n",
    "for i in range(n_mw):\n",
    "    if i > 0: continue\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    major_mergers = primary.tree_major_mergers\n",
    "    n_major = primary.n_major_mergers\n",
    "    n_snap = len(primary.snapnum)\n",
    "    primary_filename = primary.get_cutout_filename(mw_analog_dir,\n",
    "        snapnum=primary.snapnum[0])\n",
    "    co = pcutout.TNGCutout(primary_filename)\n",
    "    co.center_and_rectify()\n",
    "    pid = co.get_property('stars','ParticleIDs')\n",
    "\n",
    "    # Load the interpolator for the sphericalized potential\n",
    "    interpolator_filename = os.path.join(dens_fitting_dir,\n",
    "        'spherical_interpolated_potential/',str(z0_sid),'interp_potential.pkl')\n",
    "    with open(interpolator_filename,'rb') as handle:\n",
    "        interpot = pickle.load(handle)\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j > 0: continue\n",
    "        if verbose:\n",
    "            msg = f'Analyzing major merger {j+1}/{n_major} for MW {i+1}/{n_mw}'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "\n",
    "        # Filename and pathing check\n",
    "        this_fitting_dir = os.path.join(df_fitting_dir,df_type,df_version,\n",
    "            str(z0_sid),'merger_'+str(j+1))\n",
    "        os.makedirs(this_fitting_dir,exist_ok=True)\n",
    "        df_filename = os.path.join(this_fitting_dir,'df.pkl')\n",
    "        if os.path.exists(df_filename) and not force_df:\n",
    "            print('  Already have DF, continuing')\n",
    "            continue\n",
    "            \n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_acc_sid = major_merger.subfind_id[0]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "        upid = major_merger.get_unique_particle_ids('stars',data_dir=data_dir)\n",
    "        indx = np.where(np.isin(pid,upid))[0]\n",
    "        orbs = co.get_orbs('stars')[indx]\n",
    "        rs = orbs.r().to(apu.kpc).value\n",
    "        # n_star = len(orbs)\n",
    "\n",
    "        # Get the stellar halo density profile (denspot for the DF)\n",
    "        stellar_halo_density_filename = os.path.join(dens_fitting_dir,\n",
    "            'stellar_halo/',stellar_halo_density_version,str(z0_sid),\n",
    "            'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        denspot = pfit.construct_pot_from_fit(\n",
    "            stellar_halo_density_filename, stellar_halo_densfunc, \n",
    "            stellar_halo_density_ncut, ro=ro, vo=vo)\n",
    "        \n",
    "        # Get the stellar halo beta information\n",
    "        anisotropy_param_dir = os.path.join(df_fitting_dir, df_type,\n",
    "                anisotropy_fit_version, str(z0_sid),'merger_'+str(j+1))\n",
    "        anisotropy_filename = os.path.join(anisotropy_param_dir,'sampler.pkl')\n",
    "        ra = pio.median_params_from_emcee_sampler(anisotropy_filename,\n",
    "            ncut=anisotropy_ncut)[0][0]\n",
    "\n",
    "        # Construct the distribution function and do some dummy sampling\n",
    "        # to set the interpolators. Then save.\n",
    "        try:\n",
    "            if verbose:\n",
    "                msg = f'Beta: {round(beta,3)}, building DF'\n",
    "                logging.info(msg)\n",
    "                print(msg)\n",
    "            dfom = df.osipkovmerrittdf(pot=interpot, denspot=denspot, \n",
    "                ra=ra*apu.kpc, ro=ro, vo=vo, rmax=rs.max()*apu.kpc*1.1)\n",
    "            print('  Sampling DF')\n",
    "            _ = dfom.sample(n=100, rmin=rs.min()*apu.kpc*0.9)\n",
    "        except Exception as e:\n",
    "            msg = f'Failed to build DF, skipping. Error: {e}'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "\n",
    "        # Filename built above\n",
    "        if test_pickling:\n",
    "            try:\n",
    "                pickle.loads(pickle.dumps(dfom))\n",
    "            except RecursionError:\n",
    "                if verbose:\n",
    "                    msg = 'Caught recursion error when (un)pickling, quiting.'\n",
    "                    logging.info(msg)\n",
    "                    print(msg)\n",
    "        with open(df_filename,'wb') as handle:\n",
    "            pickle.dump(dfom,handle)\n",
    "\n",
    "        if verbose:\n",
    "            msg = f'Done with merger'\n",
    "            logging.info(msg)\n",
    "            print(msg)\n",
    "\n",
    "\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also pickle the f(Q) interpolator separately because it's the expensive thing to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_mw):\n",
    "    # if i > 0: continue\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    n_major = primary.n_major_mergers\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j > 0: continue\n",
    "        if verbose:\n",
    "            print(f'Saving f(Q) interpolator for major merger {j+1}/{n_major}'\n",
    "                  f' of MW {i+1}/{n_mw}')\n",
    "\n",
    "        # Filename and pathing check\n",
    "        this_fitting_dir = os.path.join(df_fitting_dir,df_type,\n",
    "            str(z0_sid),'merger_'+str(j+1))\n",
    "        df_filename = os.path.join(this_fitting_dir,'df.pkl')\n",
    "\n",
    "        if not os.path.exists(df_filename):\n",
    "            print('Pickled DF not found, skipping')\n",
    "            continue\n",
    "\n",
    "        with open(df_filename, 'rb') as handle:\n",
    "            dfom = pickle.load(handle)\n",
    "\n",
    "        # Pull the f(Q) interpolator\n",
    "        fQ_interp = dfom._logfQ_interp\n",
    "        fQ_interp_filename = os.path.join(this_fitting_dir, 'fQ_interp.pkl')\n",
    "        with open(fQ_interp_filename, 'wb') as handle:\n",
    "            pickle.dump(fQ_interp, handle)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
