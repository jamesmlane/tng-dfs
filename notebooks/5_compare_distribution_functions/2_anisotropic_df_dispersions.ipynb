{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# TITLE - 2_plot_anisotropic_dfs.ipynb\n",
    "# AUTHOR - James Lane\n",
    "# PROJECT - tng-dfs\n",
    "#\n",
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# Docstrings and metadata:\n",
    "'''Use density profile fits to construct anisotropic DFs\n",
    "'''\n",
    "\n",
    "__author__ = \"James Lane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../src/nb_modules/nb_imports.txt\n",
    "### Imports\n",
    "\n",
    "## Basic\n",
    "import numpy as np\n",
    "import sys, os, dill as pickle\n",
    "import pdb, copy, glob, time\n",
    "\n",
    "## Matplotlib\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## Astropy\n",
    "from astropy import units as apu\n",
    "from astropy import constants as apc\n",
    "\n",
    "## Analysis\n",
    "import scipy.stats\n",
    "import scipy.interpolate\n",
    "\n",
    "## galpy\n",
    "from galpy import orbit\n",
    "from galpy import potential\n",
    "from galpy import actionAngle as aA\n",
    "from galpy import df\n",
    "from galpy import util as gputil\n",
    "\n",
    "## Project-specific\n",
    "src_path = 'src/'\n",
    "while True:\n",
    "    if os.path.exists(src_path): break\n",
    "    if os.path.realpath(src_path).split('/')[-1] in ['tng-dfs','/']:\n",
    "            raise FileNotFoundError('Failed to find src/ directory.')\n",
    "    src_path = os.path.join('..',src_path)\n",
    "sys.path.insert(0,src_path)\n",
    "from tng_dfs import cutout as pcutout\n",
    "from tng_dfs import densprofile as pdens\n",
    "from tng_dfs import fitting as pfit\n",
    "from tng_dfs import kinematics as pkin\n",
    "from tng_dfs import util as putil\n",
    "\n",
    "### Notebook setup\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(os.path.join(src_path,'mpl/project.mplstyle')) # This must be exactly here\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords, loading, pathing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../src/nb_modules/nb_setup.txt\n",
    "# Keywords\n",
    "cdict = putil.load_config_to_dict()\n",
    "keywords = ['DATA_DIR','MW_ANALOG_DIR','RO','VO','ZO','LITTLE_H',\n",
    "            'MW_MASS_RANGE']\n",
    "data_dir,mw_analog_dir,ro,vo,zo,h,mw_mass_range = \\\n",
    "    putil.parse_config_dict(cdict,keywords)\n",
    "\n",
    "# MW Analog \n",
    "mwsubs,mwsubs_vars = putil.prepare_mwsubs(mw_analog_dir,h=h,\n",
    "    mw_mass_range=mw_mass_range,return_vars=True,force_mwsubs=False,\n",
    "    bulge_disk_fraction_cuts=True)\n",
    "\n",
    "# Figure path\n",
    "# epsen_fig_dir = '/epsen_data/scr/lane/projects/tng-dfs/figs/notebooks/sample/'\n",
    "# os.makedirs(epsen_fig_dir,exist_ok=True)\n",
    "show_plots = False\n",
    "\n",
    "# Load tree data\n",
    "tree_primary_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_primaries.pkl')\n",
    "with open(tree_primary_filename,'rb') as handle: \n",
    "    tree_primaries = pickle.load(handle)\n",
    "tree_major_mergers_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_major_mergers.pkl')\n",
    "with open(tree_major_mergers_filename,'rb') as handle:\n",
    "    tree_major_mergers = pickle.load(handle)\n",
    "n_mw = len(tree_primaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ELz(nbody_orbs, sample_orbs, nbody_E, pot, interpot=None, \n",
    "    plot_hist=True, fig=None, axs=None):\n",
    "    '''Plot energy and angular momentum'''\n",
    "\n",
    "    # Some plotting kwargs\n",
    "    Lz_range = [-3,3]\n",
    "    label_fs = 12\n",
    "\n",
    "    # Set up figure\n",
    "    if fig is None or axs is None:\n",
    "        fig = plt.figure(figsize=(12,3))\n",
    "        axs = fig.subplots(nrows=1, ncols=3)\n",
    "\n",
    "    # N-body properties\n",
    "    nbody_Lz = nbody_orbs.Lz().to_value(apu.kpc*apu.km/apu.s)\n",
    "\n",
    "    # Sample properties\n",
    "    sample_Lz = sample_orbs.Lz().to_value(apu.kpc*apu.km/apu.s)\n",
    "    sample_potE = sample_orbs.E(pot=pot).to_value(apu.km**2/apu.s**2)\n",
    "    sample_interE = sample_orbs.E(pot=interpot).to_value(apu.km**2/apu.s**2)\n",
    "\n",
    "    # Angular momentum vs energy\n",
    "    Lzs = [nbody_Lz, sample_Lz, sample_Lz]\n",
    "    energies = [nbody_E, sample_potE, sample_interE]\n",
    "    labels = ['N-body',\n",
    "            'Samples (interpolated pot)', \n",
    "            'Samples (Best-fit pot)']\n",
    "    for k in range(3):\n",
    "        E_range = [np.nanmin(energies[k])/1e5, np.nanmax(energies[k])/1e5]\n",
    "        if plot_hist:\n",
    "            H, xedges, yedges = np.histogram2d(Lzs[k]/1e3, energies[k]/1e5, \n",
    "                bins=[45,30], range=[Lz_range,E_range])\n",
    "            H = np.rot90(H)\n",
    "            H = np.flipud(H)\n",
    "            Hmasked = np.ma.masked_where(H==0,H)\n",
    "            cmap = mpl.colormaps.get_cmap('viridis')\n",
    "            cmap.set_bad(color='white')\n",
    "            axs[k].pcolormesh(xedges,yedges,Hmasked,cmap=cmap)        \n",
    "        else:\n",
    "            axs[k].scatter(Lzs[k], energies[k], s=1, color='Black', alpha=0.1)\n",
    "        \n",
    "        # Decorate\n",
    "        axs[k].axvline(0, linestyle='dashed', linewidth=1., color='Grey')\n",
    "        _annotate_bbox_kwargs = dict(facecolor='White', edgecolor='Black', \n",
    "            fill=True, alpha=0.5)\n",
    "        axs[k].annotate(labels[k], xy=(0.05,0.05), xycoords='axes fraction',\n",
    "            fontsize=8, bbox=_annotate_bbox_kwargs)\n",
    "        axs[k].set_xlabel(r'Lz [$10^{3}$ kpc km/s]', fontsize=label_fs)\n",
    "        axs[k].set_ylabel(r'E [$10^{5}$ km$^{2}$/s$^{2}$]', fontsize=label_fs)\n",
    "        \n",
    "        axs[k].set_xlim(Lz_range[0],Lz_range[1])\n",
    "        axs[k].set_ylim(E_range[0],E_range[1])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig, axs\n",
    "\n",
    "def plot_beta_vdisp(nbody_orbs, sample_orbs, n_bs=100, fig=None, axs=None):\n",
    "    '''Plot the beta and then the velocity dispersion'''\n",
    "\n",
    "    # Some kwargs for plotting\n",
    "    nbody_color = 'Black'\n",
    "    sample_color = 'DodgerBlue'\n",
    "\n",
    "    # Set up figure\n",
    "    if fig is None or axs is None:\n",
    "        fig = plt.figure(figsize=(5,12))\n",
    "        axs = fig.subplots(nrows=4, ncols=1)\n",
    "\n",
    "    # Binning for velocity dispersions and betas\n",
    "    n_bin = np.min([500, len(nbody_orbs)//10]) # n per bin\n",
    "    adaptive_binning_kwargs = {\n",
    "        'n':n_bin,\n",
    "        'rmin':0.,\n",
    "        'rmax':np.max( nbody_orbs.r().to_value(apu.kpc) ),\n",
    "        'bin_mode':'exact numbers',\n",
    "        'bin_equal_n':True,\n",
    "        'end_mode':'ignore',\n",
    "        'bin_cents_mode':'median',\n",
    "    }\n",
    "    bin_edges, bin_cents, bin_n = pkin.get_radius_binning(nbody_orbs, \n",
    "        **adaptive_binning_kwargs)\n",
    "\n",
    "    # Compute velocity dispersions for N-body\n",
    "    compute_betas_kwargs = {'use_dispersions':True,\n",
    "                            'return_kinematics':True}\n",
    "    nbody_beta, nbody_vr2, nbody_vp2, nbody_vz2 = \\\n",
    "        pkin.compute_betas_bootstrap(nbody_orbs, bin_edges, n_bootstrap=n_bs, \n",
    "        compute_betas_kwargs=compute_betas_kwargs)\n",
    "\n",
    "    # Compute velocity dispersions for the DF samples\n",
    "    compute_betas_kwargs = {'use_dispersions':True,\n",
    "                            'return_kinematics':True}\n",
    "    sample_beta, sample_vr2, sample_vp2, sample_vz2 = \\\n",
    "        pkin.compute_betas_bootstrap(sample_orbs, bin_edges, n_bootstrap=n_bs, \n",
    "        compute_betas_kwargs=compute_betas_kwargs)\n",
    "\n",
    "    # Beta for the N-body\n",
    "    axs[0].plot(bin_cents, np.median(nbody_beta, axis=0), color=nbody_color, \n",
    "        label='N-body')\n",
    "    axs[0].fill_between(bin_cents, np.percentile(nbody_beta, 16, axis=0),\n",
    "        np.percentile(nbody_beta, 84, axis=0), color=nbody_color, alpha=0.25)\n",
    "\n",
    "    # Beta for the DF samples\n",
    "    axs[0].plot(bin_cents, np.median(sample_beta, axis=0), color=sample_color, \n",
    "        label='DF Samples')\n",
    "    axs[0].fill_between(bin_cents, np.percentile(sample_beta, 16, axis=0),\n",
    "        np.percentile(sample_beta, 84, axis=0), color=sample_color, alpha=0.25)\n",
    "\n",
    "    # Velocity dispersions for the N-body\n",
    "    sv2s = [nbody_vr2,nbody_vp2,nbody_vz2]\n",
    "    v_suffixes = ['r','\\phi','z']\n",
    "    for k in range(3):\n",
    "        axs[k+1].plot(bin_cents, np.sqrt(np.median(sv2s[k], axis=0)), \n",
    "            color=nbody_color)\n",
    "        axs[k+1].fill_between(bin_cents, \n",
    "            np.sqrt(np.percentile(sv2s[k], 16, axis=0)),\n",
    "            np.sqrt(np.percentile(sv2s[k], 84, axis=0)), \n",
    "            color=nbody_color, alpha=0.25)\n",
    "        axs[k+1].set_ylabel(r'$\\sigma_'+v_suffixes[k]+r'$')\n",
    "\n",
    "    # Velocity dispersions for the DF samples\n",
    "    sv2s = [sample_vr2,sample_vp2,sample_vz2]\n",
    "    v_suffixes = ['r','\\phi','z']\n",
    "    for k in range(3):\n",
    "        axs[k+1].plot(bin_cents, np.sqrt(np.median(sv2s[k], axis=0)), \n",
    "            color=sample_color)\n",
    "        axs[k+1].fill_between(bin_cents, \n",
    "            np.sqrt(np.percentile(sv2s[k], 16, axis=0)),\n",
    "            np.sqrt(np.percentile(sv2s[k], 84, axis=0)), \n",
    "            color=sample_color, alpha=0.25)\n",
    "        axs[k+1].set_ylabel(r'$\\sigma_'+v_suffixes[k]+r'$')\n",
    "\n",
    "    # Labels\n",
    "    axs[0].set_ylabel(r'$\\beta$')\n",
    "    axs[0].legend()\n",
    "    for k in range(4):\n",
    "        axs[k].set_xscale('log')\n",
    "        axs[k].set_xlabel(r'$r$ [kpc]')\n",
    "        if k > 0:\n",
    "            axs[k].set_yscale('log')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig, axs\n",
    "\n",
    "def compute_mass_error_weighted_deviation_beta_vdisp(nbody_orbs, sample_orbs, nbody_mass,\n",
    "    n_bs=100, adaptive_binning_kwargs={}, velocity_quantities_squared=False):\n",
    "    '''compute_mass_error_weighted_deviation_beta_vdisp:\n",
    "    \n",
    "    Compute the mass- and uncertainty-weighted deviation of the N-body \n",
    "    velocity dispersion / beta trend from the DF samples. \n",
    "\n",
    "    For the binning scheme the default kwargs are:\n",
    "    - n: min(500, number of N-body particles//10)\n",
    "    - rmin: 0.\n",
    "    - rmax: max(N-body particle radii)\n",
    "    - bin_mode: 'exact numbers'\n",
    "    - bin_equal_n: True\n",
    "    - end_mode: 'ignore'\n",
    "    - bin_cents_mode: 'median'\n",
    "\n",
    "    Args:\n",
    "        nbody_orbs (galpy.orbit.Orbit): N-body orbits\n",
    "        sample_orbs (galpy.orbit.Orbit): DF samples\n",
    "        nbody_mass (np.ndarray): N-body particle masses\n",
    "        n_bs (int): Number of times to bootstrap the DF/N-body samples\n",
    "            to compute the deviation statistic for error estimation\n",
    "        adaptive_binning_kwargs (dict): kwargs for get_radius_binning(), will\n",
    "            be populated with defaults listed above if not provided.\n",
    "        velocity_quantities_squared (bool): If True, use the squared velocity \n",
    "            dispersions/mean squares that are output from \n",
    "            pkin.compute_betas_bootstrap(). If False, take the square root of\n",
    "            these quantities.\n",
    "    \n",
    "    Returns:\n",
    "        mwed_[beta,vr2,vp2,vt2] (np.ndarray): Mass-weighted error deviation\n",
    "    '''\n",
    "\n",
    "    # Binning for velocity dispersions and betas\n",
    "    n_bin = np.min([500, len(nbody_orbs)//10]) # n per bin\n",
    "    if 'n' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['n'] = n_bin\n",
    "    if 'rmin' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['rmin'] = 0.\n",
    "    if 'rmax' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['rmax'] = np.max( nbody_orbs.r().to_value(apu.kpc) )\n",
    "    if 'bin_mode' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['bin_mode'] = 'exact numbers'\n",
    "    if 'bin_equal_n' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['bin_equal_n'] = True\n",
    "    if 'end_mode' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['end_mode'] = 'ignore'\n",
    "    if 'bin_cents_mode' not in adaptive_binning_kwargs.keys():\n",
    "        adaptive_binning_kwargs['bin_cents_mode'] = 'median'\n",
    "\n",
    "    adaptive_binning_kwargs = {\n",
    "        'n':n_bin,\n",
    "        'rmin':0.,\n",
    "        'rmax':np.max( nbody_orbs.r().to_value(apu.kpc) ),\n",
    "        'bin_mode':'exact numbers',\n",
    "        'bin_equal_n':True,\n",
    "        'end_mode':'ignore',\n",
    "        'bin_cents_mode':'median',\n",
    "    }\n",
    "    bin_edges, bin_cents, _ = pkin.get_radius_binning(nbody_orbs, \n",
    "        **adaptive_binning_kwargs)\n",
    "    bin_size = bin_edges[1:] - bin_edges[:-1]\n",
    "\n",
    "    # Compute velocity dispersions for N-body\n",
    "    compute_betas_kwargs = {'use_dispersions':True,\n",
    "                            'return_kinematics':True}\n",
    "    nbody_beta, nbody_vr2, nbody_vp2, nbody_vt2 = \\\n",
    "        pkin.compute_betas_bootstrap(nbody_orbs, bin_edges, n_bootstrap=n_bs, \n",
    "        compute_betas_kwargs=compute_betas_kwargs)\n",
    "\n",
    "    # Compute velocity dispersions for the DF samples\n",
    "    compute_betas_kwargs = {'use_dispersions':True,\n",
    "                            'return_kinematics':True}\n",
    "    sample_beta, sample_vr2, sample_vp2, sample_vt2 = \\\n",
    "        pkin.compute_betas_bootstrap(sample_orbs, bin_edges, n_bootstrap=n_bs, \n",
    "        compute_betas_kwargs=compute_betas_kwargs)\n",
    "\n",
    "    if not velocity_quantities_squared:\n",
    "        nbody_vr2 = np.sqrt(nbody_vr2)\n",
    "        nbody_vp2 = np.sqrt(nbody_vp2)\n",
    "        nbody_vt2 = np.sqrt(nbody_vt2)\n",
    "        sample_vr2 = np.sqrt(sample_vr2)\n",
    "        sample_vp2 = np.sqrt(sample_vp2)\n",
    "        sample_vt2 = np.sqrt(sample_vt2)\n",
    "\n",
    "    # Compute the mass profile for the N-body data\n",
    "    mass_profile = np.zeros(len(bin_cents))\n",
    "    rs = nbody_orbs.r().to_value(apu.kpc)\n",
    "    for i in range(len(bin_cents)):\n",
    "        mass_profile[i] = np.sum(nbody_mass[(rs > bin_edges[i]) & (rs < bin_edges[i+1])])\n",
    "\n",
    "    # Compute the inter-sigma range for the N-body data, which will be the error\n",
    "    nbody_beta_err = np.percentile(nbody_beta, 84, axis=0) - np.percentile(nbody_beta, 16, axis=0)\n",
    "    nbody_vr2_err = np.percentile(nbody_vr2, 84, axis=0) - np.percentile(nbody_vr2, 16, axis=0)\n",
    "    nbody_vp2_err = np.percentile(nbody_vp2, 84, axis=0) - np.percentile(nbody_vp2, 16, axis=0)\n",
    "    nbody_vt2_err = np.percentile(nbody_vt2, 84, axis=0) - np.percentile(nbody_vt2, 16, axis=0)\n",
    "\n",
    "    # Compute the mass-error-weighted deviation between the N-body and DF sample trends\n",
    "    mewd_beta = np.sum( np.abs(nbody_beta - sample_beta)*mass_profile/nbody_beta_err, axis=1 )/np.sum(mass_profile)\n",
    "    mewd_vr2 = np.sum( np.abs(nbody_vr2 - sample_vr2)*mass_profile/nbody_vr2_err, axis=1 )/np.sum(mass_profile)\n",
    "    mewd_vp2 = np.sum( np.abs(nbody_vp2 - sample_vp2)*mass_profile/nbody_vp2_err, axis=1 )/np.sum(mass_profile)\n",
    "    mewd_vt2 = np.sum( np.abs(nbody_vt2 - sample_vt2)*mass_profile/nbody_vt2_err, axis=1 )/np.sum(mass_profile)\n",
    "\n",
    "    return mewd_beta, mewd_vr2, mewd_vp2, mewd_vt2\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the constant beta DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "epsen_dens_fitting_dir = '/epsen_data/scr/lane/projects/tng-dfs/fitting/'+\\\n",
    "    'density_profile/'\n",
    "epsen_df_fitting_dir = '/epsen_data/scr/lane/projects/tng-dfs/fitting/'+\\\n",
    "    'distribution_function/'\n",
    "fig_dir = './fig/constant_beta/'\n",
    "os.makedirs(fig_dir,exist_ok=True)\n",
    "\n",
    "# DM halo information\n",
    "dm_halo_version = 'poisson_nfw'\n",
    "dm_halo_ncut = 500\n",
    "\n",
    "# Stellar bulge and disk information\n",
    "stellar_bulge_disk_version = 'miyamoto_disk_pswc_bulge_tps_halo'\n",
    "stellar_bulge_disk_ncut = 2000\n",
    "\n",
    "# Stellar halo density information\n",
    "stellar_halo_density_version = 'poisson_twopower'\n",
    "stellar_halo_density_ncut = 500\n",
    "\n",
    "# Stellar halo rotation information\n",
    "stellar_halo_rotation_version = 'tanh_rotation'\n",
    "stellar_halo_rotation_ncut = 500\n",
    "\n",
    "# Beta information\n",
    "beta_version = 'constant_beta'\n",
    "beta_ncut = 500\n",
    "\n",
    "# Define density profiles\n",
    "dm_halo_densfunc = pdens.NFWSpherical()\n",
    "disk_densfunc = pdens.MiyamotoNagaiDisk()\n",
    "bulge_densfunc = pdens.SinglePowerCutoffSpherical()\n",
    "stellar_halo_densfunc = pdens.TwoPowerSpherical()\n",
    "stellar_bulge_disk_densfunc = pdens.CompositeDensityProfile(\n",
    "    [disk_densfunc,\n",
    "     bulge_densfunc,\n",
    "     pdens.TwoPowerSpherical()]\n",
    "     )\n",
    "\n",
    "mwd_cb = []\n",
    "mwd_cb_self = []\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i not in [7,8,9,10]: continue\n",
    "    if verbose: print(f'Plotting MW {i+1}/{n_mw}')\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    major_mergers = primary.tree_major_mergers\n",
    "    n_major = primary.n_major_mergers\n",
    "    n_snap = len(primary.snapnum)\n",
    "    primary_filename = primary.get_cutout_filename(mw_analog_dir,\n",
    "        snapnum=primary.snapnum[0])\n",
    "    co = pcutout.TNGCutout(primary_filename)\n",
    "    co.center_and_rectify()\n",
    "    pid = co.get_property('stars','ParticleIDs')\n",
    "\n",
    "    # # Get the dark halo\n",
    "    dm_halo_filename = os.path.join(epsen_dens_fitting_dir,'dm_halo/',dm_halo_version,\n",
    "        str(z0_sid), 'sampler.pkl')\n",
    "    dm_halo_pot = pfit.construct_pot_from_fit(dm_halo_filename,\n",
    "        dm_halo_densfunc, dm_halo_ncut, ro=ro, vo=vo)\n",
    "    \n",
    "    # Get the stellar bulge and disk\n",
    "    stellar_bulge_disk_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "        'stellar_bulge_disk/',stellar_bulge_disk_version,str(z0_sid),\n",
    "        'sampler.pkl')\n",
    "    stellar_pots = pfit.construct_pot_from_fit(stellar_bulge_disk_filename,\n",
    "        stellar_bulge_disk_densfunc, stellar_bulge_disk_ncut, ro=ro, vo=vo)\n",
    "    fpot = [stellar_pots[1], stellar_pots[0], dm_halo_pot] # bulge, disk, halo\n",
    "\n",
    "    # Load the interpolator for the sphericalized potential\n",
    "    interpolator_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "        'spherical_interpolated_potential/',str(z0_sid),'interp_potential.pkl')\n",
    "    with open(interpolator_filename,'rb') as handle:\n",
    "        interpot = pickle.load(handle)\n",
    "\n",
    "    _mwd = []\n",
    "    _mwd_self = []\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j != 0: continue\n",
    "        if verbose: print(f'Constructing DF for major merger {j+1}/{n_major}')\n",
    "\n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_acc_sid = major_merger.subfind_id[0]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "        upid = major_merger.get_unique_particle_ids('stars',data_dir=data_dir)\n",
    "        indx = np.where(np.isin(pid,upid))[0]\n",
    "        orbs = co.get_orbs('stars')[indx]\n",
    "        rs = orbs.r().to(apu.kpc).value\n",
    "        n_star = len(orbs)\n",
    "        star_mass = co.get_masses('stars')[indx].to_value(apu.Msun)\n",
    "        pe = co.get_potential_energy('stars')[indx].to_value(apu.km**2/apu.s**2)\n",
    "        vels = co.get_velocities('stars')[indx].to_value(apu.km/apu.s)\n",
    "        vmag = np.linalg.norm(vels,axis=1)\n",
    "        energy = pe + 0.5*vmag**2\n",
    "        Lz = orbs.Lz().to_value(apu.kpc*apu.km/apu.s)\n",
    "\n",
    "        # Get the stellar halo density profile (denspot for the DF)\n",
    "        stellar_halo_density_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "            'stellar_halo/',stellar_halo_density_version,str(z0_sid),\n",
    "            'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        denspot = pfit.construct_pot_from_fit(\n",
    "            stellar_halo_density_filename, stellar_halo_densfunc, \n",
    "            stellar_halo_density_ncut, ro=ro, vo=vo)\n",
    "        \n",
    "        # Get the stellar halo rotation kernel\n",
    "        stellar_halo_rotation_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "            'stellar_halo/',stellar_halo_rotation_version,str(z0_sid),\n",
    "            'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        assert os.path.exists(stellar_halo_rotation_filename)\n",
    "        with open(stellar_halo_rotation_filename,'rb') as handle:\n",
    "            stellar_halo_rotation_sampler = pickle.load(handle)\n",
    "        stellar_halo_rotation_samples = stellar_halo_rotation_sampler.get_chain(\n",
    "            discard=stellar_halo_rotation_ncut, flat=True)\n",
    "        # Params are frot, chi\n",
    "        stellar_halo_frot, stellar_halo_chi = \\\n",
    "            np.median(stellar_halo_rotation_samples,axis=0)\n",
    "        \n",
    "        # try:\n",
    "        # Load the distribution function and wrangle\n",
    "        df_filename = os.path.join(epsen_df_fitting_dir,beta_version,\n",
    "            str(z0_sid),'merger_'+str(j+1),'df.pkl')\n",
    "        with open(df_filename,'rb') as handle:\n",
    "            dfcb = pickle.load(handle)\n",
    "        dfcb = pkin.reconstruct_anisotropic_df(dfcb, interpot, denspot)\n",
    "        \n",
    "        # Create sample and apply rotation\n",
    "        sample = dfcb.sample(n=n_star, rmin=rs.min()*apu.kpc*0.9)\n",
    "        sample = pkin.rotate_df_samples(sample,stellar_halo_frot,stellar_halo_chi)\n",
    "        # except Exception as e:\n",
    "        # print('Caught an error when loading DF / sampling:',e,'continuing')\n",
    "        # continue\n",
    "\n",
    "        _mwd.append( compute_mass_error_weighted_deviation_beta_vdisp(orbs, \n",
    "            sample, star_mass, n_bs=10) )\n",
    "        _mwd_self.append( compute_mass_error_weighted_deviation_beta_vdisp(orbs,\n",
    "            orbs, star_mass, n_bs=10) )\n",
    "\n",
    "        # ### Plotting\n",
    "        # print('Plotting')\n",
    "        # this_fig_dir = os.path.join(fig_dir, str(z0_sid), 'merger_'+str(j+1))\n",
    "        # os.makedirs(this_fig_dir,exist_ok=True)\n",
    "\n",
    "        # fig,axs = plot_ELz(orbs, sample, energy, fpot, interpot)\n",
    "        # fig.tight_layout()\n",
    "        # figname = os.path.join(this_fig_dir,'energy_Lz.png')\n",
    "        # fig.savefig(figname, dpi=300)\n",
    "        # plt.close(fig)\n",
    "\n",
    "        # fig,axs = plot_beta_vdisp(orbs, sample)\n",
    "        # fig.tight_layout()\n",
    "        # figname = os.path.join(this_fig_dir,'velocity_dispersions.png')\n",
    "        # fig.savefig(figname, dpi=300)\n",
    "        # plt.close(fig)\n",
    "\n",
    "    mwd_cb.append( _mwd )\n",
    "    mwd_cb_self.append( _mwd_self )\n",
    "\n",
    "os.makedirs('./data/', exist_ok=True)\n",
    "with open('./data/mwd_cb.pkl','wb') as handle:\n",
    "    pickle.dump(mwd_cb, handle)\n",
    "with open('./data/mwd_cb_self.pkl','wb') as handle:\n",
    "    pickle.dump(mwd_cb_self, handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Osipkov-Merritt DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "epsen_dens_fitting_dir = '/epsen_data/scr/lane/projects/tng-dfs/fitting/'+\\\n",
    "    'density_profile/'\n",
    "epsen_df_fitting_dir = '/epsen_data/scr/lane/projects/tng-dfs/fitting/'+\\\n",
    "    'distribution_function/'\n",
    "fig_dir = './fig/osipkov_merritt/'\n",
    "os.makedirs(fig_dir,exist_ok=True)\n",
    "\n",
    "# DM halo information\n",
    "dm_halo_version = 'poisson_nfw'\n",
    "dm_halo_ncut = 500\n",
    "\n",
    "# Stellar bulge and disk information\n",
    "stellar_bulge_disk_version = 'miyamoto_disk_pswc_bulge_tps_halo'\n",
    "stellar_bulge_disk_ncut = 2000\n",
    "\n",
    "# Stellar halo density information\n",
    "stellar_halo_density_version = 'poisson_twopower'\n",
    "stellar_halo_density_ncut = 500\n",
    "\n",
    "# Stellar halo rotation information\n",
    "stellar_halo_rotation_version = 'tanh_rotation'\n",
    "stellar_halo_rotation_ncut = 500\n",
    "\n",
    "# Beta information\n",
    "beta_version = 'osipkov_merritt'\n",
    "beta_ncut = 500\n",
    "\n",
    "# Define density profiles\n",
    "dm_halo_densfunc = pdens.NFWSpherical()\n",
    "disk_densfunc = pdens.MiyamotoNagaiDisk()\n",
    "bulge_densfunc = pdens.SinglePowerCutoffSpherical()\n",
    "stellar_halo_densfunc = pdens.TwoPowerSpherical()\n",
    "stellar_bulge_disk_densfunc = pdens.CompositeDensityProfile(\n",
    "    [disk_densfunc,\n",
    "     bulge_densfunc,\n",
    "     pdens.TwoPowerSpherical()]\n",
    "     )\n",
    "\n",
    "mwd_om = []\n",
    "mwd_om_self = []\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i > 5: continue\n",
    "    if verbose: print(f'Plotting MW {i+1}/{n_mw}')\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    major_mergers = primary.tree_major_mergers\n",
    "    n_major = primary.n_major_mergers\n",
    "    n_snap = len(primary.snapnum)\n",
    "    primary_filename = primary.get_cutout_filename(mw_analog_dir,\n",
    "        snapnum=primary.snapnum[0])\n",
    "    co = pcutout.TNGCutout(primary_filename)\n",
    "    co.center_and_rectify()\n",
    "    pid = co.get_property('stars','ParticleIDs')\n",
    "\n",
    "    # # Get the dark halo\n",
    "    dm_halo_filename = os.path.join(epsen_dens_fitting_dir,'dm_halo/',dm_halo_version,\n",
    "        str(z0_sid), 'sampler.pkl')\n",
    "    dm_halo_pot = pfit.construct_pot_from_fit(dm_halo_filename,\n",
    "        dm_halo_densfunc, dm_halo_ncut, ro=ro, vo=vo)\n",
    "    \n",
    "    # Get the stellar bulge and disk\n",
    "    stellar_bulge_disk_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "        'stellar_bulge_disk/',stellar_bulge_disk_version,str(z0_sid),\n",
    "        'sampler.pkl')\n",
    "    stellar_pots = pfit.construct_pot_from_fit(stellar_bulge_disk_filename,\n",
    "        stellar_bulge_disk_densfunc, stellar_bulge_disk_ncut, ro=ro, vo=vo)\n",
    "    fpot = [stellar_pots[1], stellar_pots[0], dm_halo_pot] # bulge, disk, halo\n",
    "\n",
    "    # Load the interpolator for the sphericalized potential\n",
    "    interpolator_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "        'spherical_interpolated_potential/',str(z0_sid),'interp_potential.pkl')\n",
    "    with open(interpolator_filename,'rb') as handle:\n",
    "        interpot = pickle.load(handle)\n",
    "\n",
    "    _mwd = []\n",
    "    _mwd_self = []\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j > 0: continue\n",
    "        if verbose: print(f'Constructing DF for major merger {j+1}/{n_major}')\n",
    "\n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_acc_sid = major_merger.subfind_id[0]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "        upid = major_merger.get_unique_particle_ids('stars',data_dir=data_dir)\n",
    "        indx = np.where(np.isin(pid,upid))[0]\n",
    "        orbs = co.get_orbs('stars')[indx]\n",
    "        rs = orbs.r().to(apu.kpc).value\n",
    "        n_star = len(orbs)\n",
    "        star_mass = co.get_masses('stars')[indx].to_value(apu.Msun)\n",
    "        pe = co.get_potential_energy('stars')[indx].to_value(apu.km**2/apu.s**2)\n",
    "        vels = co.get_velocities('stars')[indx].to_value(apu.km/apu.s)\n",
    "        vmag = np.linalg.norm(vels,axis=1)\n",
    "        energy = pe + 0.5*vmag**2\n",
    "        Lz = orbs.Lz().to_value(apu.kpc*apu.km/apu.s)\n",
    "\n",
    "        # Get the stellar halo density profile (denspot for the DF)\n",
    "        stellar_halo_density_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "            'stellar_halo/',stellar_halo_density_version,str(z0_sid),\n",
    "            'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        denspot = pfit.construct_pot_from_fit(\n",
    "            stellar_halo_density_filename, stellar_halo_densfunc, \n",
    "            stellar_halo_density_ncut, ro=ro, vo=vo)\n",
    "        \n",
    "        # Get the stellar halo rotation kernel\n",
    "        stellar_halo_rotation_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "            'stellar_halo/',stellar_halo_rotation_version,str(z0_sid),\n",
    "            'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        assert os.path.exists(stellar_halo_rotation_filename)\n",
    "        with open(stellar_halo_rotation_filename,'rb') as handle:\n",
    "            stellar_halo_rotation_sampler = pickle.load(handle)\n",
    "        stellar_halo_rotation_samples = stellar_halo_rotation_sampler.get_chain(\n",
    "            discard=stellar_halo_rotation_ncut, flat=True)\n",
    "        # Params are frot, chi\n",
    "        stellar_halo_frot, stellar_halo_chi = \\\n",
    "            np.median(stellar_halo_rotation_samples,axis=0)\n",
    "\n",
    "        # try:\n",
    "        # Load the distribution function and wrangle\n",
    "        df_filename = os.path.join(epsen_df_fitting_dir,beta_version,\n",
    "            str(z0_sid),'merger_'+str(j+1),'df.pkl')\n",
    "        with open(df_filename,'rb') as handle:\n",
    "            dfom = pickle.load(handle)\n",
    "        dfom = pkin.reconstruct_anisotropic_df(dfom, interpot, denspot)\n",
    "        \n",
    "        # Create sample and apply rotation\n",
    "        sample = dfom.sample(n=n_star, rmin=rs.min()*apu.kpc*0.9)\n",
    "        sample = pkin.rotate_df_samples(sample,stellar_halo_frot,stellar_halo_chi)\n",
    "        # except Exception as e:\n",
    "        #     print('Caught an error when loading DF / sampling:',e,'continuing')\n",
    "        #     continue\n",
    "            \n",
    "        _mwd.append( compute_mass_error_weighted_deviation_beta_vdisp(orbs, \n",
    "            sample, star_mass, n_bs=10) )\n",
    "        _mwd_self.append( compute_mass_error_weighted_deviation_beta_vdisp(orbs,\n",
    "            orbs, star_mass, n_bs=10) )\n",
    "        \n",
    "        # ### Plotting\n",
    "        # print('Plotting')\n",
    "        # this_fig_dir = os.path.join(fig_dir, str(z0_sid), 'merger_'+str(j+1))\n",
    "        # os.makedirs(this_fig_dir,exist_ok=True)\n",
    "\n",
    "        # fig,axs = plot_ELz(orbs, sample, energy, fpot, interpot)\n",
    "        # fig.tight_layout()\n",
    "        # figname = os.path.join(this_fig_dir,'energy_Lz.png')\n",
    "        # fig.savefig(figname, dpi=300)\n",
    "        # plt.close(fig)\n",
    "\n",
    "        # fig,axs = plot_beta_vdisp(orbs, sample)\n",
    "        # fig.tight_layout()\n",
    "        # figname = os.path.join(this_fig_dir,'velocity_dispersions.png')\n",
    "        # fig.savefig(figname, dpi=300)\n",
    "        # plt.close(fig)\n",
    "    \n",
    "    mwd_om.append( _mwd )\n",
    "    mwd_om_self.append( _mwd_self )\n",
    "\n",
    "os.makedirs('./data/', exist_ok=True)\n",
    "with open('./data/mwd_om.pkl','wb') as handle:\n",
    "    pickle.dump(mwd_om, handle)\n",
    "with open('./data/mwd_om_self.pkl','wb') as handle:\n",
    "    pickle.dump(mwd_om_self, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All MWD values should have been saved if already calculated, so don't need to \n",
    "# ask permission to overwrite\n",
    "with open('./data/mwd_cb.pkl','rb') as handle:\n",
    "    mwd_cb = pickle.load(handle)\n",
    "with open('./data/mwd_cb_self.pkl','rb') as handle:\n",
    "    mwd_cb_self = pickle.load(handle)\n",
    "\n",
    "with open('./data/mwd_om.pkl','rb') as handle:\n",
    "    mwd_om = pickle.load(handle)\n",
    "with open('./data/mwd_om_self.pkl','rb') as handle:\n",
    "    mwd_om_self = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./data/star_mass.pkl') or not os.path.exists('./data/dm_mass.pkl'):\n",
    "    star_mass = []\n",
    "    dm_mass = []\n",
    "\n",
    "    verbose = True\n",
    "\n",
    "    for i in range(n_mw):\n",
    "        # if i > 1: continue\n",
    "        if verbose: print(f'Getting MW {i+1}/{n_mw}')\n",
    "\n",
    "        # Get the primary\n",
    "        primary = tree_primaries[i]\n",
    "        z0_sid = primary.subfind_id[0]\n",
    "        n_snap = len(primary.snapnum)\n",
    "        n_major = primary.n_major_mergers\n",
    "        primary_filename = primary.get_cutout_filename(mw_analog_dir,\n",
    "            snapnum=primary.snapnum[0])\n",
    "        co = pcutout.TNGCutout(primary_filename)\n",
    "        dmpid = co.get_property('dm','ParticleIDs')\n",
    "        dmass = co.get_masses('dm').to_value(apu.Msun)\n",
    "        spid = co.get_property('stars','ParticleIDs')\n",
    "        smass = co.get_masses('stars').to_value(apu.Msun)\n",
    "\n",
    "        _star_mass = []\n",
    "        _dm_mass = []\n",
    "\n",
    "        for j in range(n_major):\n",
    "            if verbose: print(f'Merger {j+1}/{n_major}')\n",
    "\n",
    "            # Get the major merger particle IDs and mask\n",
    "            major_merger = primary.tree_major_mergers[j]\n",
    "            dmupid = major_merger.get_unique_particle_ids('dm',data_dir=data_dir)\n",
    "            supid = major_merger.get_unique_particle_ids('stars',data_dir=data_dir)\n",
    "            dmindx = np.isin(dmpid, dmupid)\n",
    "            sindx = np.isin(spid, supid)\n",
    "\n",
    "            _star_mass.append( np.sum(smass[sindx]) )\n",
    "            _dm_mass.append( np.sum(dmass[dmindx]) )\n",
    "        \n",
    "        star_mass.append(_star_mass)\n",
    "        dm_mass.append(_dm_mass)\n",
    "\n",
    "    os.makedirs('./data/', exist_ok=True)\n",
    "    with open('./data/star_mass.pkl','wb') as handle:\n",
    "        pickle.dump(star_mass, handle)\n",
    "    with open('./data/dm_mass.pkl','wb') as handle:\n",
    "        pickle.dump(dm_mass, handle)\n",
    "else:\n",
    "    with open('./data/star_mass.pkl','rb') as handle:\n",
    "        star_mass = pickle.load(handle)\n",
    "    with open('./data/dm_mass.pkl','rb') as handle:\n",
    "        dm_mass = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Finally construct a structured numpy array\n",
    "\n",
    "# Compute the loglike differences\n",
    "data = []\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i > 5: continue\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    n_snap = len(primary.snapnum)\n",
    "    n_major = primary.n_major_mergers\n",
    "\n",
    "    for j in range(n_major):\n",
    "\n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "\n",
    "        # l\n",
    "\n",
    "        _data = (mwd_cb[i][j],\n",
    "                 mwd_cb_self[i][j],\n",
    "                 mwd_om[i][j],\n",
    "                 mwd_om_self[i][j],\n",
    "                 star_mass[i][j], \n",
    "                 dm_mass[i][j],\n",
    "                 major_merger.star_mass_ratio,\n",
    "                 major_merger.dm_mass_ratio,\n",
    "                 putil.snapshot_to_redshift( major_merger.merger_snapnum ),\n",
    "                 z0_sid, \n",
    "                 j+1, \n",
    "                 major_mlpid, \n",
    "                 )\n",
    "\n",
    "        data.append( _data )\n",
    "    \n",
    "dt = np.dtype([('mwd_cb',object),\n",
    "                ('mwd_cb_self',object),\n",
    "                ('mwd_om',object),\n",
    "                ('mwd_om_self',object),\n",
    "                ('star_mass',float),\n",
    "                ('dm_mass',float),\n",
    "                ('star_mass_ratio',float),\n",
    "                ('dm_mass_ratio',float),\n",
    "                ('z_merger',float),\n",
    "                ('z0_sid',int),\n",
    "                ('major_merger',int),\n",
    "                ('major_mlpid',int),\n",
    "                ])\n",
    "mwddata = np.array(data,dtype=dt)\n",
    "\n",
    "os.makedirs('./data/', exist_ok=True)\n",
    "np.save('./data/mwddata.npy', mwddata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the mass-weighted $\\sigma$ differences for each DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwd_cb_self_percs = np.zeros((4,3))\n",
    "\n",
    "for k in range(4):\n",
    "    p = np.array([])\n",
    "    for i in range(len(mwd_cb_self)):\n",
    "        for j in range(len(mwd_cb_self[i])):\n",
    "            p = np.concatenate( (p, mwd_cb_self[i][j][k]) )\n",
    "    mwd_cb_self_percs[k] = np.percentile(p, [16,50,84])\n",
    "\n",
    "mwd_om_self_percs = np.zeros((4,3))\n",
    "\n",
    "for k in range(4):\n",
    "    p = np.array([])\n",
    "    for i in range(len(mwd_om_self)):\n",
    "        for j in range(len(mwd_om_self[i])):\n",
    "            p = np.concatenate( (p, mwd_om_self[i][j][k]) )\n",
    "    mwd_om_self_percs[k] = np.percentile(p, [16,50,84])\n",
    "\n",
    "\n",
    "### Figure showing each MWD as a function of merger stellar mass\n",
    "\n",
    "facecolor='none'\n",
    "edgecolor='Black'\n",
    "s=10\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "axs = fig.subplots(nrows=1, ncols=4)\n",
    "\n",
    "ylabels = [r'$\\beta$', r'$\\sigma_{r}$', r'$\\sigma_{\\phi}$', r'$\\sigma_{\\theta}$']\n",
    "star_counter = 0\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i > 5: continue\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    n_snap = len(primary.snapnum)\n",
    "    n_major = primary.n_major_mergers\n",
    "\n",
    "    for j in range(n_major):\n",
    "\n",
    "        for k in range(4):\n",
    "            axs[k].scatter(np.log10(star_mass[i][j]), np.median(mwd_cb[i][j][k]), \n",
    "                        facecolor=facecolor, edgecolor=edgecolor, s=s )\n",
    "            # axs[k].errorbar(np.log10(star_mass[i][j]), \n",
    "            #                 np.median(mwd_cb[i][j][k]), \n",
    "            #                 yerr=np.std(mwd_cb[i][j][k]),\n",
    "            #             markerfacecolor=facecolor, color=edgecolor, \n",
    "            #             markersize=s, ecolor=edgecolor, capsize=2)\n",
    "        star_counter += 1\n",
    "\n",
    "for k in range(4):\n",
    "    axs[k].set_xlabel(r'$\\log_{10} M_{\\star}$')\n",
    "    axs[k].set_ylabel(r'CB $\\delta$'+ylabels[k])\n",
    "    if k == 0:\n",
    "        axs[k].set_ylim(0,5)\n",
    "    else:\n",
    "        axs[k].set_ylim(0,20)\n",
    "    axs[k].axhline(mwd_cb_self_percs[k,1], color='Black', ls='solid')\n",
    "    axs[k].axhspan(mwd_cb_self_percs[k,0], mwd_cb_self_percs[k,2], \n",
    "        color='Black', alpha=0.25)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "####################\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "axs = fig.subplots(nrows=1, ncols=4)\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i > 5: continue\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    n_snap = len(primary.snapnum)\n",
    "    n_major = primary.n_major_mergers\n",
    "\n",
    "    for j in range(n_major):\n",
    "\n",
    "        for k in range(4):\n",
    "            axs[k].scatter(np.log10(star_mass[i][j]), np.median(mwd_om[i][j][k]), \n",
    "                        facecolor=facecolor, edgecolor=edgecolor, s=s )\n",
    "\n",
    "        star_counter += 1\n",
    "\n",
    "for k in range(4):\n",
    "    axs[k].set_xlabel(r'$\\log_{10} M_{\\star}$')\n",
    "    axs[k].set_ylabel(r'OM $\\delta$'+ylabels[k])\n",
    "    if k == 0:\n",
    "        axs[k].set_ylim(0,5)\n",
    "    else:\n",
    "        axs[k].set_ylim(0,20)\n",
    "    axs[k].axhline(mwd_om_self_percs[k,1], color='Black', ls='solid')\n",
    "    axs[k].axhspan(mwd_om_self_percs[k,0], mwd_om_self_percs[k,2], \n",
    "        color='Black', alpha=0.25)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "####################\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "axs = fig.subplots(nrows=1, ncols=4)\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i > 5: continue\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    n_snap = len(primary.snapnum)\n",
    "    n_major = primary.n_major_mergers\n",
    "\n",
    "    for j in range(n_major):\n",
    "\n",
    "        for k in range(4):\n",
    "            axs[k].scatter(np.median(mwd_cb[i][j][k]), np.median(mwd_om[i][j][k]), \n",
    "                        facecolor=facecolor, edgecolor=edgecolor, s=s )\n",
    "\n",
    "        star_counter += 1\n",
    "\n",
    "for k in range(4):\n",
    "    axs[k].set_xlabel(r'CB $\\delta$'+ylabels[k])\n",
    "    axs[k].set_ylabel(r'OM $\\delta$'+ylabels[k])\n",
    "    if k == 0:\n",
    "        axs[k].set_xlim(0,5)\n",
    "        axs[k].set_ylim(0,5)\n",
    "    else:\n",
    "        axs[k].set_xlim(0,10)\n",
    "        axs[k].set_ylim(0,10)\n",
    "    axs[k].axline(xy1 = [0,0], slope=1., color='k', ls='--')\n",
    "    axs[k].axvline(mwd_cb_self_percs[k,1], color='Black', ls='solid')\n",
    "    axs[k].axvspan(mwd_cb_self_percs[k,0], mwd_cb_self_percs[k,2], \n",
    "        color='Black', alpha=0.25)\n",
    "    axs[k].axhline(mwd_om_self_percs[k,1], color='Black', ls='solid')\n",
    "    axs[k].axhspan(mwd_om_self_percs[k,0], mwd_om_self_percs[k,2], \n",
    "        color='Black', alpha=0.25)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_star_mass = []\n",
    "this_zmerge = []\n",
    "this_star_mass_ratio = []\n",
    "this_dm_mass_ratio = []\n",
    "\n",
    "ratio_beta = []\n",
    "ratio_sigma_r = []\n",
    "ratio_sigma_phi = []\n",
    "ratio_sigma_theta = []\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i > 5: continue\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    n_snap = len(primary.snapnum)\n",
    "    n_major = primary.n_major_mergers\n",
    "\n",
    "    for j in range(n_major):\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "\n",
    "        # Properties of the merger\n",
    "        this_star_mass.append( star_mass[i][j] )\n",
    "        this_zmerge.append( putil.snapshot_to_redshift( major_merger.merger_snapnum ) )\n",
    "        this_star_mass_ratio.append( major_merger.star_mass_ratio )\n",
    "        this_dm_mass_ratio.append( major_merger.dm_mass_ratio )\n",
    "        \n",
    "        ratio_beta.append( np.median(mwd_cb[i][j][0])/np.median(mwd_om[i][j][0]) )\n",
    "        ratio_sigma_r.append( np.median(mwd_cb[i][j][1])/np.median(mwd_om[i][j][1]) )\n",
    "        ratio_sigma_phi.append( np.median(mwd_cb[i][j][2])/np.median(mwd_om[i][j][2]) )\n",
    "        ratio_sigma_theta.append( np.median(mwd_cb[i][j][3])/np.median(mwd_om[i][j][3]) )\n",
    "\n",
    "this_star_mass = np.array(this_star_mass).flatten()\n",
    "this_zmerge = np.array(this_zmerge).flatten()\n",
    "this_star_mass_ratio = 1/np.array(this_star_mass_ratio).flatten()\n",
    "this_dm_mass_ratio = 1/np.array(this_dm_mass_ratio).flatten()\n",
    "\n",
    "ratio_beta = np.array(ratio_beta).flatten()\n",
    "ratio_sigma_r = np.array(ratio_sigma_r).flatten()\n",
    "ratio_sigma_phi = np.array(ratio_sigma_phi).flatten()\n",
    "ratio_sigma_theta = np.array(ratio_sigma_theta).flatten()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "axs = fig.subplots(nrows=4, ncols=4).T\n",
    "\n",
    "xlabels = [r'$M_{\\star}$', r'$z_{\\rm merger}$', \n",
    "           r'$M_{\\rm \\star,p}/M_{\\rm \\star,s}$', \n",
    "           r'$M_{\\rm DM,p}/M_{\\rm DM,s}$']\n",
    "\n",
    "xs = [this_star_mass, this_zmerge, this_star_mass_ratio, this_dm_mass_ratio]\n",
    "ys = [ratio_beta, ratio_sigma_r, ratio_sigma_phi, ratio_sigma_theta]\n",
    "\n",
    "text_quantities = [r'$\\beta$', r'$\\sigma_{r}$', \n",
    "                   r'$\\sigma_{\\phi}$', r'$\\sigma_{\\theta}$']\n",
    "\n",
    "for i in range(4):\n",
    "\n",
    "    for j in range(4):\n",
    "\n",
    "        axs[i,j].scatter(xs[i], ys[j], \n",
    "                        facecolor=facecolor, edgecolor=edgecolor, s=s )\n",
    "        axs[i,j].axhline(1., color='Gray', linestyle='dashed')\n",
    "        if i in [0,2,3]:\n",
    "            axs[i,j].set_xscale('log')\n",
    "        if i > 0:\n",
    "            axs[i,j].tick_params(labelleft=False)\n",
    "        if j < 3:\n",
    "            axs[i,j].tick_params(labelbottom=False)\n",
    "        \n",
    "        x_argsort = np.argsort(xs[i])\n",
    "        window_size=10\n",
    "        ma_x = np.convolve(xs[i][x_argsort], \n",
    "            np.ones(window_size)/window_size, mode='valid')\n",
    "        ma_y = np.convolve(ys[j][x_argsort], \n",
    "            np.ones(window_size)/window_size, mode='valid')\n",
    "        axs[i,j].plot(ma_x, ma_y, color='Red', ls='solid', lw=1, zorder=3)\n",
    "\n",
    "                \n",
    "for k in range(4):                    \n",
    "    axs[k,3].set_xlabel(xlabels[k])\n",
    "    axs[0,k].text(0.9,0.9, text_quantities[k], \n",
    "        transform=axs[0,k].transAxes)\n",
    "    axs[0,k].set_ylabel(r'$\\delta_{\\rm CB}/\\delta_{\\rm OM}$')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
