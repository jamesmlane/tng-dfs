{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# TITLE - 3_anisotropic_df_likelihoods.ipynb\n",
    "# AUTHOR - James Lane\n",
    "# PROJECT - tng-dfs\n",
    "#\n",
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# Docstrings and metadata:\n",
    "'''Compute likelihoods for the anisotropic DFs.\n",
    "'''\n",
    "\n",
    "__author__ = \"James Lane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../src/nb_modules/nb_imports.txt\n",
    "### Imports\n",
    "\n",
    "## Basic\n",
    "import numpy as np\n",
    "import sys, os, dill as pickle\n",
    "import pdb, copy, glob, time\n",
    "\n",
    "## Matplotlib\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## Astropy\n",
    "from astropy import units as apu\n",
    "from astropy import constants as apc\n",
    "\n",
    "## Analysis\n",
    "import scipy.stats\n",
    "import scipy.interpolate\n",
    "\n",
    "## galpy\n",
    "from galpy import orbit\n",
    "from galpy import potential\n",
    "from galpy import actionAngle as aA\n",
    "from galpy import df\n",
    "from galpy import util as gputil\n",
    "\n",
    "## Project-specific\n",
    "src_path = 'src/'\n",
    "while True:\n",
    "    if os.path.exists(src_path): break\n",
    "    if os.path.realpath(src_path).split('/')[-1] in ['tng-dfs','/']:\n",
    "            raise FileNotFoundError('Failed to find src/ directory.')\n",
    "    src_path = os.path.join('..',src_path)\n",
    "sys.path.insert(0,src_path)\n",
    "from tng_dfs import cutout as pcutout\n",
    "from tng_dfs import densprofile as pdens\n",
    "from tng_dfs import fitting as pfit\n",
    "from tng_dfs import io as pio\n",
    "from tng_dfs import kinematics as pkin\n",
    "from tng_dfs import util as putil\n",
    "from tng_dfs import plot as pplot\n",
    "\n",
    "### Notebook setup\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(os.path.join(src_path,'mpl/project.mplstyle')) # This must be exactly here\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords, loading, pathing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../src/nb_modules/nb_setup.txt\n",
    "# Keywords\n",
    "cdict = putil.load_config_to_dict()\n",
    "keywords = ['DATA_DIR','MW_ANALOG_DIR','FIG_DIR_BASE','FITTING_DIR_BASE',\n",
    "            'RO','VO','ZO','LITTLE_H','MW_MASS_RANGE']\n",
    "data_dir,mw_analog_dir,fig_dir_base,fitting_dir_base,ro,vo,zo,h,\\\n",
    "    mw_mass_range = putil.parse_config_dict(cdict,keywords)\n",
    "\n",
    "# MW Analog \n",
    "mwsubs,mwsubs_vars = putil.prepare_mwsubs(mw_analog_dir,h=h,\n",
    "    mw_mass_range=mw_mass_range,return_vars=True,force_mwsubs=False,\n",
    "    bulge_disk_fraction_cuts=True)\n",
    "\n",
    "# Figure path\n",
    "local_fig_dir = './fig/'\n",
    "fig_dir = os.path.join(fig_dir_base, \n",
    "    'notebooks/5_compare_distribution_functions/3_anisotropic_df_likelihoods/')\n",
    "os.makedirs(local_fig_dir,exist_ok=True)\n",
    "os.makedirs(fig_dir,exist_ok=True)\n",
    "show_plots = False\n",
    "\n",
    "# Load tree data\n",
    "tree_primary_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_primaries.pkl')\n",
    "with open(tree_primary_filename,'rb') as handle: \n",
    "    tree_primaries = pickle.load(handle)\n",
    "tree_major_mergers_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_major_mergers.pkl')\n",
    "with open(tree_major_mergers_filename,'rb') as handle:\n",
    "    tree_major_mergers = pickle.load(handle)\n",
    "n_mw = len(tree_primaries)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute DF values for each DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "\n",
    "# Pathing\n",
    "dens_fitting_dir = os.path.join(fitting_dir_base,'density_profile')\n",
    "df_fitting_dir = os.path.join(fitting_dir_base,'distribution_function')\n",
    "analysis_version = 'v1.1'\n",
    "analysis_dir = os.path.join(mw_analog_dir,'analysis',analysis_version)\n",
    "\n",
    "# Get the orbits\n",
    "sample_data_cb = np.load(os.path.join(analysis_dir,'sample_data_cb.npy'),\n",
    "    allow_pickle=True)\n",
    "sample_data_om = np.load(os.path.join(analysis_dir,'sample_data_om.npy'),\n",
    "    allow_pickle=True)\n",
    "sample_data_om2 = np.load(os.path.join(analysis_dir,'sample_data_om2.npy'),\n",
    "    allow_pickle=True)\n",
    "\n",
    "# Potential interpolator version\n",
    "interpot_version = 'all_star_dm_enclosed_mass'\n",
    "\n",
    "# Stellar halo density information\n",
    "stellar_halo_density_version = 'poisson_twopower_softening'\n",
    "stellar_halo_density_ncut = 500\n",
    "\n",
    "# Stellar halo rotation information\n",
    "stellar_halo_rotation_dftype = 'tanh_rotation'\n",
    "stellar_halo_rotation_version = 'asymmetry_fit'\n",
    "stellar_halo_rotation_ncut = 500\n",
    "\n",
    "# Define density profiles\n",
    "stellar_halo_densfunc = pdens.TwoPowerSpherical()\n",
    "\n",
    "df_val_data_cb = []\n",
    "df_val_data_om = []\n",
    "df_val_data_om2 = []\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i > 0: continue\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    major_mergers = primary.tree_major_mergers\n",
    "    n_major = primary.n_major_mergers\n",
    "    n_snap = len(primary.snapnum)\n",
    "    primary_filename = primary.get_cutout_filename(mw_analog_dir,\n",
    "        snapnum=primary.snapnum[0])\n",
    "    co = pcutout.TNGCutout(primary_filename)\n",
    "    co.center_and_rectify()\n",
    "    pid = co.get_property('stars','ParticleIDs')\n",
    "\n",
    "    # Load the interpolator for the sphericalized potential\n",
    "    interpolator_filename = os.path.join(dens_fitting_dir,\n",
    "        'spherical_interpolated_potential/',interpot_version,\n",
    "        str(z0_sid),'interp_potential.pkl')\n",
    "    with open(interpolator_filename,'rb') as handle:\n",
    "        interpot = pickle.load(handle)\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j > 0: continue\n",
    "        if verbose: print(f'Calculating anisotropy profiles for MW analog '\n",
    "                          f'{i+1}/{n_mw}, merger {j+1}/{n_major}', end='\\r')\n",
    "\n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_acc_sid = major_merger.subfind_id[0]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "        upid = major_merger.get_unique_particle_ids('stars',data_dir=data_dir)\n",
    "        indx = np.where(np.isin(pid,upid))[0]\n",
    "\n",
    "        # Get energy and angular momentum\n",
    "        orbs = co.get_orbs('stars')[indx]\n",
    "        n_star = len(orbs)\n",
    "        rs = orbs.r().to_value(apu.kpc)\n",
    "        masses = co.get_masses('stars')[indx].to_value(apu.Msun)\n",
    "        L = np.sum(orbs.L().to_value(apu.kpc*apu.km/apu.s)**2,axis=1)**0.5\n",
    "        Lz = orbs.Lz().to_value(apu.kpc*apu.km/apu.s)\n",
    "        pe = co.get_potential_energy('stars')[indx].to_value(apu.km**2/apu.s**2)\n",
    "        vels = co.get_velocities('stars')[indx].to_value(apu.km/apu.s)\n",
    "        vmag = np.linalg.norm(vels,axis=1)\n",
    "        energy = pe + 0.5*vmag**2\n",
    "\n",
    "        # Mask the input orbits to only include those with radius greater\n",
    "        # than the softening length\n",
    "        r_softening = putil.get_softening_length('stars', z=0, physical=True)\n",
    "        mask = rs > r_softening\n",
    "        orbs = orbs[mask]\n",
    "        rs = rs[mask]\n",
    "        masses = masses[mask]\n",
    "        L = L[mask]\n",
    "        Lz = Lz[mask]\n",
    "        pe = pe[mask]\n",
    "        vels = vels[mask]\n",
    "        vmag = vmag[mask]\n",
    "        energy = energy[mask]\n",
    "        n_star = len(orbs)\n",
    "\n",
    "        # Get the stellar halo density profile (denspot for the DF)\n",
    "        stellar_halo_density_filename = os.path.join(dens_fitting_dir,\n",
    "            'stellar_halo/',stellar_halo_density_version,str(z0_sid),\n",
    "            'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        denspot = pfit.construct_pot_from_fit(\n",
    "            stellar_halo_density_filename, stellar_halo_densfunc, \n",
    "            stellar_halo_density_ncut, ro=ro, vo=vo)\n",
    "        \n",
    "        # Get the stellar halo rotation kernel\n",
    "        stellar_halo_rotation_filename = os.path.join(df_fitting_dir,\n",
    "            stellar_halo_rotation_dftype,stellar_halo_rotation_version,\n",
    "            str(z0_sid),'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        stellar_halo_k, stellar_halo_chi = \\\n",
    "            pio.median_params_from_emcee_sampler(stellar_halo_rotation_filename,\n",
    "                ncut=stellar_halo_rotation_ncut)\n",
    "        \n",
    "        # Scale the input energies by the potential energy of interpot at the \n",
    "        # stellar half-mass radius\n",
    "        rhalf = pkin.half_mass_radius(rs, masses)\n",
    "        rhalf_perc = 0.05\n",
    "        rhalf_mask = np.abs(rs-rhalf) < rhalf_perc*rhalf\n",
    "        rhalf_pe_star = np.median(pe[rhalf_mask])\n",
    "        rhalf_pe_interpot = potential.evaluatePotentials(\n",
    "            interpot, rhalf*apu.kpc, 0.).to_value(apu.km**2/apu.s**2)\n",
    "        rhalf_pe_offset = rhalf_pe_star - rhalf_pe_interpot\n",
    "\n",
    "        ########### Compute the DF values for the constant beta DFs ###########\n",
    "\n",
    "        # Load the distribution function and wrangle\n",
    "        df_type = 'constant_beta'\n",
    "        df_version = 'df_density_softening'\n",
    "        df_filename = os.path.join(df_fitting_dir,df_type,df_version,\n",
    "            str(z0_sid),'merger_'+str(j+1),'df.pkl')\n",
    "        with open(df_filename,'rb') as handle:\n",
    "            dfcb = pickle.load(handle)\n",
    "        dfcb = pkin.reconstruct_anisotropic_df(dfcb, interpot, denspot)\n",
    "\n",
    "        # Compute the internal energy and angular momentum\n",
    "        energy_offset = energy - rhalf_pe_offset\n",
    "        energy_offset_internal = energy_offset / (vo**2)\n",
    "        L_internal = L/ro/vo\n",
    "        \n",
    "        # Compute the value of the DF speedily using the interpolator\n",
    "        fE = dfcb._fE_interp(energy_offset_internal)\n",
    "        fL = L_internal**(-2*dfcb._beta)\n",
    "        fLz = (stellar_halo_k*np.tanh(Lz/stellar_halo_chi)-stellar_halo_k+1)\n",
    "        f = fE*fL*fLz\n",
    "\n",
    "        # Do the self-similar assessment of the DF\n",
    "        # Create the samples for the DF\n",
    "        mask = (sample_data_cb['z0_sid'] == z0_sid) &\\\n",
    "               (sample_data_cb['major_acc_sid'] == major_acc_sid) &\\\n",
    "               (sample_data_cb['major_mlpid'] == major_mlpid) &\\\n",
    "               (sample_data_cb['merger_number'] == j+1)\n",
    "        indx = np.where(mask)[0]\n",
    "        assert len(indx) == 1, 'Something went wrong'\n",
    "        indx = indx[0]\n",
    "        sample = sample_data_cb['sample'][indx]\n",
    "\n",
    "        # Compute the likelihoods for the self-similar DF\n",
    "        pe_offset_sample = 0. # PE offset for DF samples should be 0\n",
    "        energy_offset_sample = sample.E(pot=interpot).to_value(apu.km**2/apu.s**2) - pe_offset_sample\n",
    "        energy_offset_internal_sample = energy_offset_sample / (vo**2)\n",
    "        L_sample = (np.sum(sample.L()**2,1)**0.5).to_value(apu.kpc*apu.km/apu.s)\n",
    "        L_internal_sample = L_sample/ro/vo\n",
    "        Lz_sample = sample.Lz().to_value(apu.kpc*apu.km/apu.s)\n",
    "        fE_sample = dfcb._fE_interp(energy_offset_internal_sample)\n",
    "        fL_sample = L_internal_sample**(-2*dfcb._beta)\n",
    "        fLz_sample = (stellar_halo_k*np.tanh(Lz_sample/stellar_halo_chi)-stellar_halo_k+1)\n",
    "        f_sample = fE_sample*fL_sample*fLz_sample\n",
    "\n",
    "        # Save the values\n",
    "        _data = (\n",
    "            z0_sid,\n",
    "            major_acc_sid,\n",
    "            major_mlpid,\n",
    "            j+1,\n",
    "            fE,\n",
    "            fL,\n",
    "            fLz,\n",
    "            f,\n",
    "            fE_sample,\n",
    "            fL_sample,\n",
    "            fLz_sample,\n",
    "            f_sample\n",
    "        )\n",
    "        df_val_data_cb.append(_data)\n",
    "\n",
    "        ########## Compute the DF values for the Osipkov-Merritt DFs ##########\n",
    "\n",
    "        # Load the distribution function and wrangle\n",
    "        df_type = 'osipkov_merritt'\n",
    "        df_version = 'df_density_softening'\n",
    "        df_filename = os.path.join(df_fitting_dir,df_type,df_version,\n",
    "            str(z0_sid),'merger_'+str(j+1),'df.pkl')\n",
    "        with open(df_filename,'rb') as handle:\n",
    "            dfom = pickle.load(handle)\n",
    "        dfom = pkin.reconstruct_anisotropic_df(dfom, interpot, denspot)\n",
    "\n",
    "        # Compute the energy and Q\n",
    "        energy_offset = energy - rhalf_pe_offset\n",
    "        Q_offset = -energy_offset - 0.5*L**2/(dfom._ra*ro)**2\n",
    "        Q_offset_internal = Q_offset / (vo**2)\n",
    "        \n",
    "        # Compute the value of the DF speedily using the interpolator\n",
    "        fQ = np.exp( dfom._logfQ_interp(Q_offset_internal) )\n",
    "        fLz = (stellar_halo_k*np.tanh(Lz/stellar_halo_chi)-stellar_halo_k+1)\n",
    "        f = fQ*fLz\n",
    "\n",
    "        # Do the self-similar assessment of the DF\n",
    "        # Create the samples for the DF\n",
    "        mask = (sample_data_om['z0_sid'] == z0_sid) &\\\n",
    "               (sample_data_om['major_acc_sid'] == major_acc_sid) &\\\n",
    "               (sample_data_om['major_mlpid'] == major_mlpid) &\\\n",
    "               (sample_data_om['merger_number'] == j+1)\n",
    "        indx = np.where(mask)[0]\n",
    "        assert len(indx) == 1, 'Something went wrong'\n",
    "        indx = indx[0]\n",
    "        sample = sample_data_om['sample'][indx]\n",
    "\n",
    "        # Compute the likelihoods for the self-similar DF\n",
    "        pe_offset_sample = 0. # PE offset for DF samples should be 0\n",
    "        energy_offset_sample = sample.E(pot=interpot).to_value(apu.km**2/apu.s**2) - pe_offset_sample\n",
    "        L_sample = (np.sum(sample.L()**2,1)**0.5).to_value(apu.kpc*apu.km/apu.s)\n",
    "        Q_offset_sample = -energy_offset_sample - 0.5*L_sample**2/(dfom._ra*ro)**2\n",
    "        Q_offset_internal_sample = Q_offset_sample / (vo**2)\n",
    "        Lz_sample = sample.Lz().to_value(apu.kpc*apu.km/apu.s)\n",
    "        fQ_sample = np.exp( dfom._logfQ_interp(Q_offset_internal_sample) )\n",
    "        fLz_sample = (stellar_halo_k*np.tanh(Lz_sample/stellar_halo_chi)-stellar_halo_k+1)\n",
    "        f_sample = fQ_sample*fLz_sample\n",
    "\n",
    "        # Save the values\n",
    "        _data = (\n",
    "            z0_sid,\n",
    "            major_acc_sid,\n",
    "            major_mlpid,\n",
    "            j+1,\n",
    "            fE,\n",
    "            fL,\n",
    "            fLz,\n",
    "            f,\n",
    "            fE_sample,\n",
    "            fL_sample,\n",
    "            fLz_sample,\n",
    "            f_sample\n",
    "        )\n",
    "        df_val_data_om.append(_data)\n",
    "\n",
    "        ########## Compute the DF values for the OM2 DFs ##########\n",
    "\n",
    "        # Load the distribution function and wrangle\n",
    "        df_type = 'osipkov_merritt_2_combination'\n",
    "        df_version = 'ra_N10_01_to_300_softening'\n",
    "        df_filename = os.path.join(df_fitting_dir,df_type,df_version,\n",
    "            str(z0_sid),'merger_'+str(j+1),'df.pkl')\n",
    "        with open(df_filename,'rb') as handle:\n",
    "            data = pickle.load(handle)\n",
    "        dfoms = data[0]\n",
    "        ras = data[1]\n",
    "        kom = data[2]\n",
    "        for k in range(len(dfoms)):\n",
    "            dfoms[k] = pkin.reconstruct_anisotropic_df(dfoms[k], interpot, \n",
    "                denspot)\n",
    "        \n",
    "        # Compute the energy and Q\n",
    "        energy_offset = energy - rhalf_pe_offset\n",
    "        Q_offset = -energy_offset - 0.5*L**2/(dfom._ra*ro)**2\n",
    "        Q_offset_internal = Q_offset / (vo**2)\n",
    "\n",
    "        # Compute the value of the DF speedily using the interpolator\n",
    "        fQ1 = np.exp( dfoms[0]._logfQ_interp(Q_offset_internal) )\n",
    "        fQ2 = np.exp( dfoms[1]._logfQ_interp(Q_offset_internal) )\n",
    "        fLz = (kom*np.tanh(Lz/stellar_halo_chi)-kom+1)\n",
    "        f = (fQ1*kom + fQ2*(1-kom))*fLz\n",
    "\n",
    "        # Do the self-similar assessment of the DF\n",
    "        # Create the samples for the DF\n",
    "        mask = (sample_data_om2['z0_sid'] == z0_sid) &\\\n",
    "                (sample_data_om2['major_acc_sid'] == major_acc_sid) &\\\n",
    "                (sample_data_om2['major_mlpid'] == major_mlpid) &\\\n",
    "                (sample_data_om2['merger_number'] == j+1)\n",
    "        indx = np.where(mask)[0]\n",
    "        assert len(indx) == 1, 'Something went wrong'\n",
    "        indx = indx[0]\n",
    "        sample = sample_data_om2['sample'][indx]\n",
    "\n",
    "        # Compute the likelihoods for the self-similar DF\n",
    "        pe_offset_sample = 0. # PE offset for DF samples should be 0\n",
    "        energy_offset_sample = sample.E(pot=interpot).to_value(apu.km**2/apu.s**2) - pe_offset_sample\n",
    "        L_sample = (np.sum(sample.L()**2,1)**0.5).to_value(apu.kpc*apu.km/apu.s)\n",
    "        ra_sample = np.zeros(len(sample))\n",
    "        ra_sample[:int(n_star*kom)] = dfoms[0]._ra\n",
    "        ra_sample[int(n_star*kom):] = dfoms[1]._ra\n",
    "        Q_offset_sample = -energy_offset_sample - 0.5*L_sample**2/(ra_sample*ro)**2\n",
    "        Q_offset_internal_sample = Q_offset_sample / (vo**2)\n",
    "        Lz_sample = sample.Lz().to_value(apu.kpc*apu.km/apu.s)\n",
    "        fQ_sample = np.zeros(len(sample))\n",
    "        fQ_sample[:int(n_star*kom)] = np.exp( \n",
    "            dfoms[0]._logfQ_interp(Q_offset_internal_sample[:int(n_star*kom)]) )\n",
    "        fQ_sample[int(n_star*kom):] = np.exp(\n",
    "            dfoms[1]._logfQ_interp(Q_offset_internal_sample[int(n_star*kom):]) )\n",
    "        fLz_sample = (stellar_halo_k*np.tanh(Lz_sample/stellar_halo_chi)-stellar_halo_k+1)\n",
    "        f_sample = (fQ_sample*kom + (1-kom))*fLz_sample\n",
    "\n",
    "        # Save the values\n",
    "        _data = (\n",
    "            z0_sid,\n",
    "            major_acc_sid,\n",
    "            major_mlpid,\n",
    "            j+1,\n",
    "            fE,\n",
    "            fL,\n",
    "            fLz,\n",
    "            f,\n",
    "            fE_sample,\n",
    "            fL_sample,\n",
    "            fLz_sample,\n",
    "            f_sample\n",
    "        )\n",
    "        df_val_data_om2.append(_data)\n",
    "\n",
    "# Save the data as a pickle\n",
    "header = ['z0_sid','major_acc_sid','major_mlpid','merger_number',\n",
    "          'fE','fL','fLz','f',\n",
    "          'fE_sample','fL_sample','fLz_sample','f_sample']\n",
    "with open(os.path.join(analysis_dir,'df_val_data_cb.pkl'),'wb') as handle:\n",
    "    pickle.dump([header,df_val_data_cb], handle)\n",
    "with open(os.path.join(analysis_dir,'df_val_data_om.pkl'),'wb') as handle:\n",
    "    pickle.dump([header,df_val_data_om], handle)\n",
    "with open(os.path.join(analysis_dir,'df_val_data_om2.pkl'),'wb') as handle:\n",
    "    pickle.dump([header,df_val_data_om2], handle)\n",
    "\n",
    "# Also save as a structured array\n",
    "df_val_data_dtype = np.dtype([\n",
    "    ('z0_sid',int),\n",
    "    ('major_acc_sid',int),\n",
    "    ('major_mlpid',int),\n",
    "    ('merger_number',int),\n",
    "    ('fE',object),\n",
    "    ('fL',object),\n",
    "    ('fLz',object),\n",
    "    ('f',object),\n",
    "    ('fE_sample',object),\n",
    "    ('fL_sample',object),\n",
    "    ('fLz_sample',object),\n",
    "    ('f_sample',object)\n",
    "    ])\n",
    "df_val_data_cb = np.array(df_val_data_cb, dtype=df_val_data_dtype)\n",
    "df_val_data_om = np.array(df_val_data_om, dtype=df_val_data_dtype)\n",
    "df_val_data_om2 = np.array(df_val_data_om2, dtype=df_val_data_dtype)\n",
    "np.save(os.path.join(analysis_dir,'df_val_data_cb.npy'), df_val_data_cb)\n",
    "np.save(os.path.join(analysis_dir,'df_val_data_om.npy'), df_val_data_om)\n",
    "np.save(os.path.join(analysis_dir,'df_val_data_om2.npy'), df_val_data_om2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_version = 'v1.1'\n",
    "analysis_dir = os.path.join(mw_analog_dir,'analysis',analysis_version)\n",
    "\n",
    "# Load the structured arrays\n",
    "df_val_data_cb = np.load(os.path.join(analysis_dir,'df_val_data_cb.npy'),\n",
    "    allow_pickle=True)\n",
    "df_val_data_om = np.load(os.path.join(analysis_dir,'df_val_data_om.npy'),\n",
    "    allow_pickle=True)\n",
    "df_val_data_om2 = np.load(os.path.join(analysis_dir,'df_val_data_om2.npy'),\n",
    "    allow_pickle=True)\n",
    "\n",
    "# Load the merger information\n",
    "merger_data = np.load(os.path.join(analysis_dir,'merger_data.npy'), \n",
    "    allow_pickle=True)\n",
    "\n",
    "checks = True\n",
    "if checks:\n",
    "    assert np.all(df_val_data_cb['z0_sid'] ==df_val_data_om['z0_sid'] ), \\\n",
    "        'Something went wrong'\n",
    "    assert np.all(df_val_data_cb['major_acc_sid'] ==df_val_data_om['major_acc_sid'] ), \\\n",
    "        'Something went wrong'\n",
    "    assert np.all(df_val_data_cb['major_mlpid'] ==df_val_data_om['major_mlpid'] ), \\\n",
    "        'Something went wrong'\n",
    "    assert np.all(df_val_data_cb['merger_number'] ==df_val_data_om2['merger_number'] ), \\\n",
    "        'Something went wrong'\n",
    "    assert np.all(df_val_data_cb['z0_sid'] ==df_val_data_om2['z0_sid'] ), \\\n",
    "        'Something went wrong'\n",
    "    assert np.all(df_val_data_cb['major_acc_sid'] ==df_val_data_om2['major_acc_sid'] ), \\\n",
    "        'Something went wrong'\n",
    "    assert np.all(df_val_data_cb['z0_sid'] == merger_data['z0_sid']), \\\n",
    "        'Something went wrong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrange the input loglikelihood a bit and construct a loglikelihood data\n",
    "# structured array\n",
    "\n",
    "loglike_data = []\n",
    "\n",
    "for i in range(len(merger_data)):\n",
    "    z0_sid = merger_data['z0_sid'][i]\n",
    "    major_acc_sid = merger_data['major_acc_sid'][i]\n",
    "    major_mlpid = merger_data['major_mlpid'][i]\n",
    "    merger_number = merger_data['merger_number'][i]\n",
    "\n",
    "    # Mask out negative values\n",
    "    f_mask = (df_val_data_cb['f'][i] > 0) &\\\n",
    "                (df_val_data_om['f'][i] > 0) &\\\n",
    "                (df_val_data_om2['f'][i] > 0) &\\\n",
    "                (df_val_data_cb['f_sample'][i] > 0) &\\\n",
    "                (df_val_data_om['f_sample'][i] > 0) &\\\n",
    "                (df_val_data_om2['f_sample'][i] > 0)\n",
    "    ll_cb = np.log(df_val_data_cb['f'][i][f_mask])\n",
    "    ll_om = np.log(df_val_data_om['f'][i][f_mask])\n",
    "    ll_om2 = np.log(df_val_data_om2['f'][i][f_mask])\n",
    "    ll_cb_sample = np.log(df_val_data_cb['f_sample'][i][f_mask])\n",
    "    ll_om_sample = np.log(df_val_data_om['f_sample'][i][f_mask])\n",
    "    ll_om2_sample = np.log(df_val_data_om2['f_sample'][i][f_mask])\n",
    "    ll_N = np.sum(f_mask)\n",
    "    \n",
    "    # Compute the differences\n",
    "    lld_cb_om = np.sum(ll_cb) - np.sum(ll_om)\n",
    "    lld_cb_om2 = np.sum(ll_cb) - np.sum(ll_om2)\n",
    "    lld_om_om2 = np.sum(ll_om) - np.sum(ll_om2)\n",
    "    lld_cb_cb_sample = np.sum(ll_cb) - np.sum(ll_cb_sample)\n",
    "    lld_om_om_sample = np.sum(ll_om) - np.sum(ll_om_sample)\n",
    "    lld_om2_om2_sample = np.sum(ll_om2) - np.sum(ll_om2_sample)\n",
    "    lld_cb_om_self = np.sum(ll_cb) - np.sum(ll_om_sample)\n",
    "    lld_cb_om_self = (np.sum(ll_cb) - np.sum(ll_cb_sample)) -\\\n",
    "                        (np.sum(ll_om) - np.sum(ll_om_sample))\n",
    "    lld_cb_om2_self = (np.sum(ll_cb) - np.sum(ll_cb_sample)) -\\\n",
    "                        (np.sum(ll_om2) - np.sum(ll_om2_sample))\n",
    "    lld_om_om2_self = (np.sum(ll_om) - np.sum(ll_om_sample)) -\\\n",
    "                        (np.sum(ll_om2) - np.sum(ll_om2_sample))    \n",
    "\n",
    "    # Load into the structured array\n",
    "    _data = (\n",
    "        z0_sid,\n",
    "        major_acc_sid,\n",
    "        major_mlpid,\n",
    "        merger_number,\n",
    "        ll_cb,\n",
    "        ll_om,\n",
    "        ll_om2,\n",
    "        ll_cb_sample,\n",
    "        ll_om_sample,\n",
    "        ll_om2_sample,\n",
    "        ll_N,\n",
    "        lld_cb_om,\n",
    "        lld_cb_om2,\n",
    "        lld_om_om2,\n",
    "        lld_cb_cb_sample,\n",
    "        lld_om_om_sample,\n",
    "        lld_om2_om2_sample,\n",
    "        lld_cb_om_self,\n",
    "        lld_cb_om2_self,\n",
    "        lld_om_om2_self\n",
    "    )\n",
    "    loglike_data.append(_data)\n",
    "\n",
    "loglike_data_dtype = np.dtype([('z0_sid',int),\n",
    "                               ('major_acc_sid',int),\n",
    "                               ('major_mlpid',int),\n",
    "                               ('merger_number',int),\n",
    "                               ('ll_cb',object),\n",
    "                               ('ll_om',object),\n",
    "                               ('ll_om2',object),\n",
    "                               ('ll_cb_sample',object),\n",
    "                               ('ll_om_sample',object),\n",
    "                               ('ll_om2_sample',object),\n",
    "                               ('ll_N',int),\n",
    "                               ('lld_cb_om',float),\n",
    "                               ('lld_cb_om2',float),\n",
    "                               ('lld_om_om2',float),\n",
    "                               ('lld_cb_self',float),\n",
    "                               ('lld_om_self',float),\n",
    "                               ('lld_om2_self',float),\n",
    "                               ('lld_cb_om_self',float),\n",
    "                               ('lld_cb_om2_self',float),\n",
    "                               ('lld_om_om2_self',float)\n",
    "                               ])\n",
    "loglike_data = np.array(loglike_data,dtype=loglike_data_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try and make some plots without relying on the 1/N factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will need merger data for this\n",
    "analysis_version = 'v1.1'\n",
    "analysis_dir = os.path.join(mw_analog_dir,'analysis',analysis_version)\n",
    "merger_data = np.load(os.path.join(analysis_dir,'merger_data.npy'), \n",
    "    allow_pickle=True)\n",
    "\n",
    "# Validate it vs lldata\n",
    "assert np.all( merger_data['major_mlpid'] == loglike_data['major_mlpid'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some statistics about the range of values\n",
    "llbins = [[-np.inf,-1000],\n",
    "          [-1000,-100],\n",
    "          [-100,0],\n",
    "          [0,100],\n",
    "          [100,1000],\n",
    "          [1000,np.inf],\n",
    "         ]\n",
    "\n",
    "for i in range(len(llbins)):\n",
    "    print(f'Number of values in bin ',llbins[i])\n",
    "    print(f'CB-OM: {np.sum((loglike_data[\"lld_cb_om\"] > llbins[i][0]) & (loglike_data[\"lld_cb_om\"] < llbins[i][1]))}')\n",
    "    print(f'CB-OM2: {np.sum((loglike_data[\"lld_cb_om2\"] > llbins[i][0]) & (loglike_data[\"lld_cb_om2\"] < llbins[i][1]))}')\n",
    "    print(f'OM-OM2: {np.sum((loglike_data[\"lld_om_om2\"] > llbins[i][0]) & (loglike_data[\"lld_om_om2\"] < llbins[i][1]))}')\n",
    "    print(f'CB-SIM: {np.sum((loglike_data[\"lld_cb_self\"] > llbins[i][0]) & (loglike_data[\"lld_cb_self\"] < llbins[i][1]))}')\n",
    "    print(f'OM-SIM: {np.sum((loglike_data[\"lld_om_self\"] > llbins[i][0]) & (loglike_data[\"lld_om_self\"] < llbins[i][1]))}')\n",
    "    print(f'OM2-SIM: {np.sum((loglike_data[\"lld_om2_self\"] > llbins[i][0]) & (loglike_data[\"lld_om2_self\"] < llbins[i][1]))}')\n",
    "    print(f'CB-OM-SIM: {np.sum((loglike_data[\"lld_cb_om_self\"] > llbins[i][0]) & (loglike_data[\"lld_cb_om_self\"] < llbins[i][1]))}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a similar comparison with a symlog plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 10\n",
    "facecolor = 'none'\n",
    "edgecolor = 'Black'\n",
    "ticklabelsize = 6\n",
    "axislabelsize = 11\n",
    "legendfontsize = 5\n",
    "columnwidth, textwidth = pplot.get_latex_columnwidth_textwidth_inches()\n",
    "\n",
    "### CB vs OM\n",
    "fig = plt.figure(figsize=(columnwidth, 2))\n",
    "ax = fig.subplots(nrows=1, ncols=1)\n",
    "\n",
    "lld_cb_om_mask = loglike_data['lld_cb_om'] > 0\n",
    "ax.scatter(loglike_data['lld_cb_om'][lld_cb_om_mask], \n",
    "           merger_data['anisotropy'][lld_cb_om_mask], \n",
    "           facecolor='none', edgecolor=edgecolor, s=s, marker='^')\n",
    "ax.scatter(loglike_data['lld_cb_om'][~lld_cb_om_mask], \n",
    "           merger_data['anisotropy'][~lld_cb_om_mask],\n",
    "           facecolor=facecolor, edgecolor=edgecolor, s=s, marker='o')\n",
    "\n",
    "ax.set_ylabel(r'$\\beta$', fontsize=axislabelsize)\n",
    "ax.set_ylim(-0.1, 1.0)\n",
    "\n",
    "ax.tick_params(labelsize=ticklabelsize)\n",
    "ax.set_xlabel(r'$\\log \\mathcal{L}_\\mathrm{CM} - \\log \\mathcal{L}_\\mathrm{OM}$', \n",
    "    fontsize=axislabelsize)\n",
    "ax.set_xscale('symlog', linthresh=1)\n",
    "ax.set_xlim(-1e6,1e6)\n",
    "ax.set_xticks([-1e6,-1e4,-1e2,-1e0,0,1e0,1e2,1e4,1e6])\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.subplots_adjust(hspace=0.1)\n",
    "# fig.savefig('./fig/loglike_number_beta_comparison.pdf')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "### OM vs OM2\n",
    "fig = plt.figure(figsize=(columnwidth, 2))\n",
    "ax = fig.subplots(nrows=1, ncols=1)\n",
    "\n",
    "ax.scatter(loglike_data['lld_om_om2'][lld_cb_om_mask], \n",
    "           merger_data['anisotropy'][lld_cb_om_mask], \n",
    "           facecolor='none', edgecolor=edgecolor, s=s, marker='^')\n",
    "ax.scatter(-loglike_data['lld_om_om2'][~lld_cb_om_mask], \n",
    "           merger_data['anisotropy'][~lld_cb_om_mask],\n",
    "           facecolor=facecolor, edgecolor=edgecolor, s=s, marker='o')\n",
    "\n",
    "ax.set_ylabel(r'$\\beta$', fontsize=axislabelsize)\n",
    "ax.set_ylim(-0.1, 1.0)\n",
    "\n",
    "ax.tick_params(labelsize=ticklabelsize)\n",
    "ax.set_xlabel(r'$\\log \\mathcal{L}_\\mathrm{OM2} - \\log \\mathcal{L}_\\mathrm{OM}$', \n",
    "    fontsize=axislabelsize)\n",
    "ax.set_xscale('symlog', linthresh=1)\n",
    "ax.set_xlim(-1e6,1e6)\n",
    "ax.set_xticks([-1e6,-1e4,-1e2,-1e0,0,1e0,1e2,1e4,1e6])\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.subplots_adjust(hspace=0.1)\n",
    "# fig.savefig('./fig/loglike_number_beta_comparison.pdf')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 10\n",
    "facecolor = 'none'\n",
    "edgecolor = 'Black'\n",
    "ticklabelsize = 6\n",
    "axislabelsize = 11\n",
    "legendfontsize = 4\n",
    "hist_colors = ['DodgerBlue','Red']\n",
    "hist_linestyles = ['solid','dashed']\n",
    "hist_linewidths = [2,1]\n",
    "\n",
    "columnwidth, textwidth = pplot.get_latex_columnwidth_textwidth_inches()\n",
    "fig = plt.figure(figsize=(columnwidth, 3))\n",
    "gs = fig.add_gridspec(2,7)\n",
    "axs = [fig.add_subplot(gs[0,0:5]),\n",
    "       fig.add_subplot(gs[1,0:5])]\n",
    "raxs = [fig.add_subplot(gs[0,5:]),\n",
    "        fig.add_subplot(gs[1,5:])]\n",
    "\n",
    "# ax = fig.subplots(nrows=1, ncols=1)\n",
    "\n",
    "## Plot the main panels: anisotropy and logarithmic stellar mass\n",
    "axs[0].scatter(loglike_data['lld_cb_om'], merger_data['anisotropy'], \n",
    "           facecolor='none', edgecolor=edgecolor, s=s)\n",
    "axs[0].set_ylabel(r'$\\beta$', fontsize=axislabelsize)\n",
    "axs[0].set_ylim(-0.1, 1.0)\n",
    "axs[0].tick_params(labelsize=ticklabelsize, labelbottom=False)\n",
    "# axs[0].set_xlabel(r'$\\Delta \\log \\mathcal{L}$', fontsize=axislabelsize)\n",
    "# axs[0].set_xscale('symlog', linthresh=1)\n",
    "# axs[0].set_xlim(-1e7,1e7)\n",
    "axs[0].set_xticks([-1e6,-1e4,-1e2,-1e0,0,1e0,1e2,1e4,1e6])\n",
    "\n",
    "axs[1].scatter(loglike_data['lld_cb_om'], np.log10(merger_data['star_mass']), \n",
    "           facecolor='none', edgecolor=edgecolor, s=s)\n",
    "axs[1].set_ylabel(r'$\\log_{10}(\\mathrm{M}_{\\star} / \\mathrm{M}_{\\odot})$', \n",
    "    fontsize=axislabelsize)\n",
    "axs[1].set_ylim(6.5, 11)\n",
    "axs[1].tick_params(labelsize=ticklabelsize)\n",
    "# axs[1].set_xlabel(r'$\\Delta \\log \\mathcal{L}$', fontsize=axislabelsize)\n",
    "axs[1].set_xlabel(r'$\\mathcal{L}_\\mathrm{CB} - \\mathcal{L}_\\mathrm{OM}$', \n",
    "    fontsize=axislabelsize)\n",
    "# axs[1].set_xscale('symlog', linthresh=1)\n",
    "# axs[1].set_xlim(-1e7,1e7)\n",
    "# axs[1].set_xticks([-1e6,-1e4,-1e2,-1e0,0,1e0,1e2,1e4,1e6])\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xscale('symlog', linthresh=1)\n",
    "    ax.set_xlim(-1e7,1e7)\n",
    "    ax.set_xticks([-1e6,-1e4,-1e2,-1e0,0,1e0,1e2,1e4,1e6])\n",
    "    ax.axvline(0, color='Gray', ls='dashed', lw=1, zorder=1)\n",
    "axs[1].set_xticklabels([r'$-10^{6}$',r'$-10^{4}$',r'$-10^{2}$',r'$-1$',r'$0$',\n",
    "                        r'$1$',r'$10^{2}$',r'$10^{4}$',r'$10^{6}$'])\n",
    "\n",
    "## Plot the margins on the side panels, separated by positive/negative loglike\n",
    "mask = loglike_data['lld_cb_om'] > 0\n",
    "raxs[0].hist(merger_data['anisotropy'][mask], range=[-0.1,1], bins=11, \n",
    "             histtype='step', color=hist_colors[0], lw=hist_linewidths[0], \n",
    "             ls=hist_linestyles[0], density=True, orientation='horizontal')\n",
    "raxs[0].hist(merger_data['anisotropy'][~mask], range=[-0.1,1], bins=11, \n",
    "             histtype='step', color=hist_colors[1], lw=hist_linewidths[1], \n",
    "             ls=hist_linestyles[1], density=True, orientation='horizontal')\n",
    "raxs[0].set_ylim(-0.1, 1.0)\n",
    "raxs[0].tick_params(labelleft=False, labelbottom=True, \n",
    "    labeltop=False, labelsize=ticklabelsize)\n",
    "# raxs[0].set_xlabel(r'p($\\beta$)', fontsize=axislabelsize)\n",
    "# raxs[0].xaxis.set_label_position('top')\n",
    "# raxs[0].set_xlim(0, 20)\n",
    "\n",
    "raxs[1].hist(np.log10(merger_data['star_mass'][mask]), range=[6.5,10.5], bins=10,\n",
    "                histtype='step', color=hist_colors[0], lw=hist_linewidths[0], \n",
    "                ls=hist_linestyles[0], density=True, orientation='horizontal')\n",
    "raxs[1].hist(np.log10(merger_data['star_mass'][~mask]), range=[6.5,10.5], bins=10,\n",
    "                histtype='step', color=hist_colors[1], lw=hist_linewidths[1], \n",
    "                ls=hist_linestyles[1], density=True, orientation='horizontal')\n",
    "raxs[1].set_ylim(6.5, 11)\n",
    "raxs[1].tick_params(labelleft=False, labelsize=ticklabelsize)\n",
    "raxs[1].set_xlabel(r'Density', fontsize=axislabelsize)\n",
    "# raxs[1].set_xlim(0, 20)\n",
    "\n",
    "# Annotate the top plot to show the number of points in either group\n",
    "axs[0].annotate(int(np.sum(mask)), xy=(0.56,0.9), xycoords='axes fraction', \n",
    "                ha='center', va='center', fontsize=ticklabelsize, \n",
    "                color=hist_colors[0])\n",
    "axs[0].annotate(int(np.sum(~mask)), xy=(0.46,0.9), xycoords='axes fraction', \n",
    "                ha='center', va='center', fontsize=ticklabelsize,\n",
    "                color=hist_colors[1])\n",
    "\n",
    "# Legend\n",
    "raxs[0].plot([], [], color=hist_colors[0], linewidth=hist_linewidths[0],\n",
    "    linestyle=hist_linestyles[0], label=r'$\\Delta \\mathrm{log} \\mathcal{L} > 0$')\n",
    "raxs[0].plot([], [], color=hist_colors[1], linewidth=hist_linewidths[1],\n",
    "    linestyle=hist_linestyles[1], label=r'$\\Delta \\mathrm{log} \\mathcal{L} < 0$')\n",
    "raxs[0].legend(loc='upper right', fontsize=legendfontsize, handlelength=2, \n",
    "    frameon=False)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.15, wspace=0.15)\n",
    "fig.savefig('./fig/loglike_symlog_starmass_beta_comparison_cb_om.pdf')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 10\n",
    "facecolor = 'none'\n",
    "edgecolor = 'Black'\n",
    "ticklabelsize = 6\n",
    "axislabelsize = 11\n",
    "legendfontsize = 4\n",
    "hist_colors = ['DodgerBlue','Red']\n",
    "hist_linestyles = ['solid','dashed']\n",
    "hist_linewidths = [2,1]\n",
    "\n",
    "columnwidth, textwidth = pplot.get_latex_columnwidth_textwidth_inches()\n",
    "fig = plt.figure(figsize=(columnwidth, 3))\n",
    "gs = fig.add_gridspec(2,7)\n",
    "axs = [fig.add_subplot(gs[0,0:5]),\n",
    "       fig.add_subplot(gs[1,0:5])]\n",
    "raxs = [fig.add_subplot(gs[0,5:]),\n",
    "        fig.add_subplot(gs[1,5:])]\n",
    "\n",
    "# ax = fig.subplots(nrows=1, ncols=1)\n",
    "\n",
    "## Plot the main panels: anisotropy and logarithmic stellar mass\n",
    "axs[0].scatter(-loglike_data['lld_om_om2'], merger_data['anisotropy'], \n",
    "           facecolor='none', edgecolor=edgecolor, s=s)\n",
    "axs[0].set_ylabel(r'$\\beta$', fontsize=axislabelsize)\n",
    "axs[0].set_ylim(-0.1, 1.0)\n",
    "axs[0].tick_params(labelsize=ticklabelsize, labelbottom=False)\n",
    "# axs[0].set_xlabel(r'$\\Delta \\log \\mathcal{L}$', fontsize=axislabelsize)\n",
    "# axs[0].set_xscale('symlog', linthresh=1)\n",
    "# axs[0].set_xlim(-1e7,1e7)\n",
    "axs[0].set_xticks([-1e6,-1e4,-1e2,-1e0,0,1e0,1e2,1e4,1e6])\n",
    "\n",
    "axs[1].scatter(-loglike_data['lld_om_om2'], np.log10(merger_data['star_mass']), \n",
    "           facecolor='none', edgecolor=edgecolor, s=s)\n",
    "axs[1].set_ylabel(r'$\\log_{10}(\\mathrm{M}_{\\star} / \\mathrm{M}_{\\odot})$', \n",
    "    fontsize=axislabelsize)\n",
    "axs[1].set_ylim(6.5, 11)\n",
    "axs[1].tick_params(labelsize=ticklabelsize)\n",
    "# axs[1].set_xlabel(r'$\\Delta \\log \\mathcal{L}$', fontsize=axislabelsize)\n",
    "axs[1].set_xlabel(r'$\\mathcal{L}_\\mathrm{OM2} - \\mathcal{L}_\\mathrm{OM}$', \n",
    "    fontsize=axislabelsize)\n",
    "# axs[1].set_xscale('symlog', linthresh=1)\n",
    "# axs[1].set_xlim(-1e7,1e7)\n",
    "# axs[1].set_xticks([-1e6,-1e4,-1e2,-1e0,0,1e0,1e2,1e4,1e6])\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xscale('symlog', linthresh=1)\n",
    "    ax.set_xlim(-1e7,1e7)\n",
    "    ax.set_xticks([-1e6,-1e4,-1e2,-1e0,0,1e0,1e2,1e4,1e6])\n",
    "    ax.axvline(0, color='Gray', ls='dashed', lw=1, zorder=1)\n",
    "axs[1].set_xticklabels([r'$-10^{6}$',r'$-10^{4}$',r'$-10^{2}$',r'$-1$',r'$0$',\n",
    "                        r'$1$',r'$10^{2}$',r'$10^{4}$',r'$10^{6}$'])\n",
    "\n",
    "## Plot the margins on the side panels, separated by positive/negative loglike\n",
    "mask = -loglike_data['lld_om_om2'] > 0\n",
    "raxs[0].hist(merger_data['anisotropy'][mask], range=[-0.1,1], bins=11, \n",
    "             histtype='step', color=hist_colors[0], lw=hist_linewidths[0], \n",
    "             ls=hist_linestyles[0], density=True, orientation='horizontal')\n",
    "raxs[0].hist(merger_data['anisotropy'][~mask], range=[-0.1,1], bins=11, \n",
    "             histtype='step', color=hist_colors[1], lw=hist_linewidths[1], \n",
    "             ls=hist_linestyles[1], density=True, orientation='horizontal')\n",
    "# raxs[0].hist(merger_data['anisotropy'][mask], range=[-0.1,1], bins=11, \n",
    "#              histtype='step', color=hist_colors[0], lw=1, density=True,\n",
    "#              orientation='horizontal')\n",
    "# raxs[0].hist(merger_data['anisotropy'][~mask], range=[-0.1,1], bins=11, \n",
    "#              histtype='step', color=hist_colors[1], lw=1, density=True,\n",
    "#              orientation='horizontal')\n",
    "raxs[0].set_ylim(-0.1, 1.0)\n",
    "raxs[0].tick_params(labelleft=False, labelbottom=True, \n",
    "    labeltop=False, labelsize=ticklabelsize)\n",
    "# raxs[0].set_xlabel(r'p($\\beta$)', fontsize=axislabelsize)\n",
    "# raxs[0].xaxis.set_label_position('top')\n",
    "# raxs[0].set_xlim(0, 20)\n",
    "\n",
    "raxs[1].hist(np.log10(merger_data['star_mass'][mask]), range=[6.5,10.5], bins=10,\n",
    "                histtype='step', color=hist_colors[0], lw=hist_linewidths[0], \n",
    "                ls=hist_linestyles[0], density=True, orientation='horizontal')\n",
    "raxs[1].hist(np.log10(merger_data['star_mass'][~mask]), range=[6.5,10.5], bins=10,\n",
    "                histtype='step', color=hist_colors[1], lw=hist_linewidths[1], \n",
    "                ls=hist_linestyles[1], density=True, orientation='horizontal')\n",
    "# raxs[1].hist(np.log10(merger_data['star_mass'][mask]), range=[6.5,10.5], bins=10,\n",
    "#                 histtype='step', color=hist_colors[0], lw=1, density=True,\n",
    "#                 orientation='horizontal')\n",
    "# raxs[1].hist(np.log10(merger_data['star_mass'][~mask]), range=[6.5,10.5], bins=10,\n",
    "#                 histtype='step', color=hist_colors[1], lw=1, density=True,\n",
    "#                 orientation='horizontal')\n",
    "raxs[1].set_ylim(6.5, 11)\n",
    "raxs[1].tick_params(labelleft=False, labelsize=ticklabelsize)\n",
    "raxs[1].set_xlabel(r'Density', fontsize=axislabelsize)\n",
    "# raxs[1].set_xlim(0, 20)\n",
    "\n",
    "# Annotate the top plot to show the number of points in either group\n",
    "axs[0].annotate(int(np.sum(mask)), xy=(0.56,0.9), xycoords='axes fraction', \n",
    "                ha='center', va='center', fontsize=ticklabelsize, \n",
    "                color=hist_colors[0])\n",
    "axs[0].annotate(int(np.sum(~mask)), xy=(0.46,0.9), xycoords='axes fraction', \n",
    "                ha='center', va='center', fontsize=ticklabelsize,\n",
    "                color=hist_colors[1])\n",
    "\n",
    "# Legend\n",
    "# raxs[0].plot([], [], color=hist_colors[0], linewidth=hist_linewidths[0],\n",
    "#     linestyle=hist_linestyles[0], label=r'$\\Delta \\mathrm{log} \\mathcal{L} > 0$')\n",
    "# raxs[0].plot([], [], color=hist_colors[1], linewidth=hist_linewidths[1],\n",
    "#     linestyle=hist_linestyles[1], label=r'$\\Delta \\mathrm{log} \\mathcal{L} < 0$')\n",
    "# raxs[0].legend(loc='upper right', fontsize=legendfontsize, handlelength=2, \n",
    "#     frameon=False)\n",
    "# raxs[0].plot([], [], color=hist_colors[0], label=r'$\\Delta \\mathrm{log} \\mathcal{L} > 0$')\n",
    "# raxs[0].plot([], [], color=hist_colors[1], label=r'$\\Delta \\mathrm{log} \\mathcal{L} < 0$')\n",
    "# raxs[0].legend(loc='upper right', fontsize=legendfontsize, handlelength=0.5, \n",
    "#     frameon=False)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.15, wspace=0.15)\n",
    "fig.savefig('./fig/loglike_symlog_starmass_beta_comparison_om2_om.pdf')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_loglike_diff_violin_plot(merger_data_key, is_log=False, ylabel=''):\n",
    "#     # A single plot that can be used to query any quantity in merger_data\n",
    "\n",
    "#     llbins = [[-np.inf,-1000],\n",
    "#             [-1000,-100],\n",
    "#             [-100,0],\n",
    "#             [0,100],\n",
    "#             [100,1000],\n",
    "#             [1000,np.inf],\n",
    "#             ]\n",
    "#     llkeys = ['lld_cb_om','lld_cb_self','lld_om_self','lld_cb_om_self']\n",
    "#     labels = [r'${\\rm CB-OM}$', r'${\\rm CB-SIM}$', r'${\\rm OM-SIM}$', r'${\\rm (CB-SIM)-(OM-SIM)}$']\n",
    "#     # labels = [r'$\\mathcal{L}_{\\rm CB-OM}$',\n",
    "#     #           r'$\\mathcal{L}_{\\rm CB-S}$',\n",
    "#     #           r'$\\mathcal{L}_{\\rm OM-S}$',\n",
    "#     #           r'$\\mathcal{L}_{\\rm (OM-S)-(CB-S)}$'\n",
    "#     #           ]\n",
    "#     # labels but in scientific notation\n",
    "#     x_labels = [r'$< -10^3$', r'$-10^3$ to $-10^2$', r'$-10^2$ to $0$',\n",
    "#             r'$0$ to $10^2$', r'$10^2$ to $10^3$', r'$> 10^3$']\n",
    "#     # labels = [r'$< -1000$', r'$-1000$ to $-100$', r'$-100$ to $0$',\n",
    "#     #           r'$0$ to $100$', r'$100$ to $1000$', r'$> 1000$']\n",
    "#     s = 10\n",
    "#     colors = ['Red', 'DarkOrange', 'DodgerBlue', 'Purple']\n",
    "#     edgecolor = 'Black'\n",
    "#     offset = [-0.24,-0.08,0.08,0.24]\n",
    "#     ticklabelsize = 6\n",
    "#     axislabelsize = 12\n",
    "#     legendfontsize = 6\n",
    "\n",
    "#     columnwidth, textwidth = pplot.get_latex_columnwidth_textwidth_inches()\n",
    "#     fig = plt.figure(figsize=(columnwidth, 3))\n",
    "#     axs = fig.subplots(nrows=2, ncols=1)\n",
    "\n",
    "#     for i in range(len(llbins)):\n",
    "#         for j in range(len(llkeys)):\n",
    "#             x = lldata[llkeys[j]]\n",
    "#             mask = (x > llbins[i][0]) & (x < llbins[i][1])\n",
    "#             x = x[mask]\n",
    "#             N = len(x)\n",
    "#             if i == 0:\n",
    "#                 axs[0].scatter(i+offset[j], N, s=20, color=colors[j], \n",
    "#                     label=labels[j])\n",
    "#             else:\n",
    "#                 axs[0].scatter(i+offset[j], N, s=20, color=colors[j])\n",
    "#                 # facecolor=colors[j], edgecolor=edgecolor)\n",
    "\n",
    "#     axs[0].set_xticks(range(len(llbins)))\n",
    "#     axs[0].tick_params(labelsize=ticklabelsize, labelbottom=False)\n",
    "#     axs[0].set_ylabel('N', fontsize=axislabelsize)\n",
    "#     axs[0].set_ylim(0, 60)\n",
    "#     axs[0].legend(loc='upper right', fontsize=legendfontsize)\n",
    "\n",
    "#     for i in range(len(llbins)):\n",
    "#         for j in range(len(llkeys)):\n",
    "#             x = lldata[llkeys[j]]\n",
    "#             mask = (x > llbins[i][0]) & (x < llbins[i][1])\n",
    "#             y = merger_data[merger_data_key][mask]\n",
    "#             if is_log:\n",
    "#                 y = np.log10(y)\n",
    "#             ly, my, uy = np.percentile(y, [16,50,84])\n",
    "#             axs[1].errorbar(i+offset[j], my, yerr=[[my-ly],[uy-my]], \n",
    "#                 fmt='o', color=colors[j], ms=5, lw=1, capsize=0, capthick=0)\n",
    "#             # axs[1].scatter(i+offset[j], N, s=s, facecolor=colors[j], \n",
    "#             #     edgecolor=edgecolor)\n",
    "\n",
    "#     for j in range(len(llkeys)):\n",
    "#         axs[1].scatter([], [], s=s, facecolor=colors[j], edgecolor=edgecolor, \n",
    "#             label=labels[j])\n",
    "\n",
    "#     axs[1].set_xticks(range(len(llbins)))\n",
    "#     axs[1].set_xticklabels(x_labels)\n",
    "#     axs[1].tick_params(labelsize=ticklabelsize, rotation=30)\n",
    "#     axs[1].set_xlabel(r'$\\Delta \\log \\mathcal{L}$', fontsize=axislabelsize)\n",
    "#     axs[1].set_ylabel(ylabel, fontsize=axislabelsize)\n",
    "#     # axs[1].set_ylim(0,0.8)\n",
    "\n",
    "#     return fig,axs\n",
    "# # fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = make_loglike_diff_violin_plot('star_mass_ratio', is_log=True, ylabel=r'star $\\log M_{s}/M_{p}$')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = make_loglike_diff_violin_plot('merger_redshift', ylabel=r'$z$')\n",
    "# fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
