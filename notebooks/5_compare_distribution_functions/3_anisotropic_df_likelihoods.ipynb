{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# TITLE - 3_anisotropic_df_likelihoods.ipynb\n",
    "# AUTHOR - James Lane\n",
    "# PROJECT - tng-dfs\n",
    "#\n",
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# Docstrings and metadata:\n",
    "'''Compute likelihoods for the anisotropic DFs.\n",
    "'''\n",
    "\n",
    "__author__ = \"James Lane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../src/nb_modules/nb_imports.txt\n",
    "### Imports\n",
    "\n",
    "## Basic\n",
    "import numpy as np\n",
    "import sys, os, dill as pickle\n",
    "import pdb, copy, glob, time\n",
    "\n",
    "## Matplotlib\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## Astropy\n",
    "from astropy import units as apu\n",
    "from astropy import constants as apc\n",
    "\n",
    "## Analysis\n",
    "import scipy.stats\n",
    "import scipy.interpolate\n",
    "\n",
    "## galpy\n",
    "from galpy import orbit\n",
    "from galpy import potential\n",
    "from galpy import actionAngle as aA\n",
    "from galpy import df\n",
    "from galpy import util as gputil\n",
    "\n",
    "## Project-specific\n",
    "src_path = 'src/'\n",
    "while True:\n",
    "    if os.path.exists(src_path): break\n",
    "    if os.path.realpath(src_path).split('/')[-1] in ['tng-dfs','/']:\n",
    "            raise FileNotFoundError('Failed to find src/ directory.')\n",
    "    src_path = os.path.join('..',src_path)\n",
    "sys.path.insert(0,src_path)\n",
    "from tng_dfs import cutout as pcutout\n",
    "from tng_dfs import densprofile as pdens\n",
    "from tng_dfs import fitting as pfit\n",
    "from tng_dfs import kinematics as pkin\n",
    "from tng_dfs import util as putil\n",
    "from tng_dfs import plot as pplot\n",
    "\n",
    "### Notebook setup\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(os.path.join(src_path,'mpl/project.mplstyle')) # This must be exactly here\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords, loading, pathing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../src/nb_modules/nb_setup.txt\n",
    "# Keywords\n",
    "cdict = putil.load_config_to_dict()\n",
    "keywords = ['DATA_DIR','MW_ANALOG_DIR','RO','VO','ZO','LITTLE_H',\n",
    "            'MW_MASS_RANGE']\n",
    "data_dir,mw_analog_dir,ro,vo,zo,h,mw_mass_range = \\\n",
    "    putil.parse_config_dict(cdict,keywords)\n",
    "\n",
    "# MW Analog \n",
    "mwsubs,mwsubs_vars = putil.prepare_mwsubs(mw_analog_dir,h=h,\n",
    "    mw_mass_range=mw_mass_range,return_vars=True,force_mwsubs=False,\n",
    "    bulge_disk_fraction_cuts=True)\n",
    "\n",
    "# Figure path\n",
    "# epsen_fig_dir = '/epsen_data/scr/lane/projects/tng-dfs/figs/notebooks/sample/'\n",
    "# os.makedirs(epsen_fig_dir,exist_ok=True)\n",
    "show_plots = False\n",
    "\n",
    "# Load tree data\n",
    "tree_primary_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_primaries.pkl')\n",
    "with open(tree_primary_filename,'rb') as handle: \n",
    "    tree_primaries = pickle.load(handle)\n",
    "tree_major_mergers_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_major_mergers.pkl')\n",
    "with open(tree_major_mergers_filename,'rb') as handle:\n",
    "    tree_major_mergers = pickle.load(handle)\n",
    "n_mw = len(tree_primaries)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute DF values for the constant beta DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "epsen_dens_fitting_dir = '/epsen_data/scr/lane/projects/tng-dfs/fitting/'+\\\n",
    "    'density_profile/'\n",
    "epsen_df_fitting_dir = '/epsen_data/scr/lane/projects/tng-dfs/fitting/'+\\\n",
    "    'distribution_function/'\n",
    "fig_dir = './fig/constant_beta/'\n",
    "os.makedirs(fig_dir,exist_ok=True)\n",
    "\n",
    "# DM halo information\n",
    "dm_halo_version = 'poisson_nfw'\n",
    "dm_halo_ncut = 500\n",
    "\n",
    "# Stellar bulge and disk information\n",
    "stellar_bulge_disk_version = 'miyamoto_disk_pswc_bulge_tps_halo'\n",
    "stellar_bulge_disk_ncut = 2000\n",
    "\n",
    "# Stellar halo density information\n",
    "stellar_halo_density_version = 'poisson_twopower'\n",
    "stellar_halo_density_ncut = 500\n",
    "\n",
    "# Stellar halo rotation information\n",
    "stellar_halo_rotation_version = 'tanh_rotation'\n",
    "stellar_halo_rotation_ncut = 500\n",
    "\n",
    "# Beta information\n",
    "beta_version = 'constant_beta'\n",
    "beta_ncut = 500\n",
    "\n",
    "# Define density profiles\n",
    "dm_halo_densfunc = pdens.NFWSpherical()\n",
    "disk_densfunc = pdens.MiyamotoNagaiDisk()\n",
    "bulge_densfunc = pdens.SinglePowerCutoffSpherical()\n",
    "stellar_halo_densfunc = pdens.TwoPowerSpherical()\n",
    "stellar_bulge_disk_densfunc = pdens.CompositeDensityProfile(\n",
    "    [disk_densfunc,\n",
    "     bulge_densfunc,\n",
    "     stellar_halo_densfunc]\n",
    "     )\n",
    "\n",
    "df_vals_cb = []\n",
    "df_vals_self_cb = []\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i != 2: continue\n",
    "    if verbose: print(f'Plotting MW {i+1}/{n_mw}')\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    major_mergers = primary.tree_major_mergers\n",
    "    n_major = primary.n_major_mergers\n",
    "    n_snap = len(primary.snapnum)\n",
    "    primary_filename = primary.get_cutout_filename(mw_analog_dir,\n",
    "        snapnum=primary.snapnum[0])\n",
    "    co = pcutout.TNGCutout(primary_filename)\n",
    "    co.center_and_rectify()\n",
    "    pid = co.get_property('stars','ParticleIDs')\n",
    "\n",
    "    # # Get the dark halo\n",
    "    dm_halo_filename = os.path.join(epsen_dens_fitting_dir,'dm_halo/',dm_halo_version,\n",
    "        str(z0_sid), 'sampler.pkl')\n",
    "    dm_halo_pot = pfit.construct_pot_from_fit(dm_halo_filename,\n",
    "        dm_halo_densfunc, dm_halo_ncut, ro=ro, vo=vo)\n",
    "    \n",
    "    # Get the stellar bulge and disk\n",
    "    stellar_bulge_disk_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "        'stellar_bulge_disk/',stellar_bulge_disk_version,str(z0_sid),\n",
    "        'sampler.pkl')\n",
    "    stellar_pots = pfit.construct_pot_from_fit(stellar_bulge_disk_filename,\n",
    "        stellar_bulge_disk_densfunc, stellar_bulge_disk_ncut, ro=ro, vo=vo)\n",
    "    fpot = [stellar_pots[1], stellar_pots[0], dm_halo_pot] # bulge, disk, halo\n",
    "\n",
    "    # Load the interpolator for the sphericalized potential\n",
    "    interpolator_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "        'spherical_interpolated_potential/',str(z0_sid),'interp_potential.pkl')\n",
    "    with open(interpolator_filename,'rb') as handle:\n",
    "        interpot = pickle.load(handle)\n",
    "\n",
    "    _df_vals = []\n",
    "    _df_vals_self = []\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j > 0: continue\n",
    "        if verbose: print(f'  Constructing DF for major merger {j+1}/{n_major}')\n",
    "\n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_acc_sid = major_merger.subfind_id[0]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "        upid = major_merger.get_unique_particle_ids('stars',data_dir=data_dir)\n",
    "        indx = np.where(np.isin(pid,upid))[0]\n",
    "\n",
    "        # Get energy and angular momentum\n",
    "        orbs = co.get_orbs('stars')[indx]\n",
    "        n_star = len(orbs)\n",
    "        rs = orbs.r().to_value(apu.kpc)\n",
    "        masses = co.get_masses('stars')[indx].to_value(apu.Msun)\n",
    "        L = np.sum(orbs.L().to_value(apu.kpc*apu.km/apu.s)**2,axis=1)**0.5\n",
    "        Lz = orbs.Lz().to_value(apu.kpc*apu.km/apu.s)\n",
    "        pe = co.get_potential_energy('stars')[indx].to_value(apu.km**2/apu.s**2)\n",
    "        vels = co.get_velocities('stars')[indx].to_value(apu.km/apu.s)\n",
    "        vmag = np.linalg.norm(vels,axis=1)\n",
    "        energy = pe + 0.5*vmag**2\n",
    "\n",
    "        # Get the stellar halo density profile (denspot for the DF)\n",
    "        stellar_halo_density_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "            'stellar_halo/',stellar_halo_density_version,str(z0_sid),\n",
    "            'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        denspot = pfit.construct_pot_from_fit(\n",
    "            stellar_halo_density_filename, stellar_halo_densfunc, \n",
    "            stellar_halo_density_ncut, ro=ro, vo=vo)\n",
    "        \n",
    "        # Get the stellar halo rotation kernel\n",
    "        stellar_halo_rotation_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "            'stellar_halo/',stellar_halo_rotation_version,str(z0_sid),\n",
    "            'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        assert os.path.exists(stellar_halo_rotation_filename)\n",
    "        with open(stellar_halo_rotation_filename,'rb') as handle:\n",
    "            stellar_halo_rotation_sampler = pickle.load(handle)\n",
    "        stellar_halo_rotation_samples = stellar_halo_rotation_sampler.get_chain(\n",
    "            discard=stellar_halo_rotation_ncut, flat=True)\n",
    "        # Params are frot, chi\n",
    "        stellar_halo_frot, stellar_halo_chi = \\\n",
    "            np.median(stellar_halo_rotation_samples,axis=0)\n",
    "        \n",
    "        # Load the distribution function and wrangle\n",
    "        df_filename = os.path.join(epsen_df_fitting_dir,beta_version,\n",
    "            str(z0_sid),'merger_'+str(j+1),'df.pkl')\n",
    "        with open(df_filename,'rb') as handle:\n",
    "            dfcb = pickle.load(handle)\n",
    "        dfcb = pkin.reconstruct_anisotropic_df(dfcb, interpot, denspot)\n",
    "\n",
    "        # Scale the input energies by the potential energy of interpot at the \n",
    "        # stellar half-mass radius\n",
    "        rhalf = pkin.half_mass_radius(rs, masses)\n",
    "        rhalf_perc = 0.05\n",
    "        rhalf_mask = np.abs(rs-rhalf) < rhalf_perc*rhalf\n",
    "        rhalf_pe_star = np.median(pe[rhalf_mask])\n",
    "        rhalf_pe_interpot = potential.evaluatePotentials(\n",
    "            interpot, rhalf*apu.kpc, 0.).to_value(apu.km**2/apu.s**2)\n",
    "        rhalf_pe_offset = rhalf_pe_star - rhalf_pe_interpot\n",
    "\n",
    "        # Compute the internal energy and angular momentum\n",
    "        energy_offset = energy - rhalf_pe_offset\n",
    "        energy_offset_internal = energy_offset / (vo**2)\n",
    "        L_internal = L/ro/vo\n",
    "        \n",
    "        # Compute the value of the DF speedily using the interpolator\n",
    "        fE = dfcb._fE_interp(energy_offset_internal)\n",
    "        fL = L_internal**(-2*dfcb._beta)\n",
    "        fLz = (stellar_halo_frot*np.tanh(Lz/stellar_halo_chi)-stellar_halo_frot+1)\n",
    "        f = fE*fL*fLz\n",
    "\n",
    "        # Do the self-similar assessment of the DF\n",
    "        # Create the samples for the DF\n",
    "        sample = dfcb.sample(n=n_star, rmin=rs.min()*apu.kpc*0.9)\n",
    "        sample = pkin.rotate_df_samples(sample,stellar_halo_frot,stellar_halo_chi)\n",
    "\n",
    "        # Compute the likelihoods for the self-similar DF\n",
    "        pe_offset_sample = 0. # PE offset for DF samples should be 0\n",
    "        energy_offset_sample = sample.E(pot=interpot).to_value(apu.km**2/apu.s**2) - pe_offset_sample\n",
    "        energy_offset_internal_sample = energy_offset_sample / (vo**2)\n",
    "        L_sample = (np.sum(sample.L()**2,1)**0.5).to_value(apu.kpc*apu.km/apu.s)\n",
    "        L_internal_sample = L_sample/ro/vo\n",
    "        Lz_sample = sample.Lz().to_value(apu.kpc*apu.km/apu.s)\n",
    "        fE_sample = dfcb._fE_interp(energy_offset_internal_sample)\n",
    "        fL_sample = L_internal_sample**(-2*dfcb._beta)\n",
    "        fLz_sample = (stellar_halo_frot*np.tanh(Lz_sample/stellar_halo_chi)-stellar_halo_frot+1)\n",
    "        f_sample = fE_sample*fL_sample*fLz_sample\n",
    "\n",
    "        # Save the values\n",
    "        _df_vals.append([f,fE,fL,fLz,z0_sid,j+1,major_mlpid])\n",
    "        _df_vals_self.append([f_sample,fE_sample,fL_sample,fLz_sample,z0_sid,j+1,major_mlpid])\n",
    "\n",
    "    df_vals_cb.append( _df_vals )\n",
    "    df_vals_self_cb.append( _df_vals_self )\n",
    "\n",
    "os.makedirs('./data/', exist_ok=True)\n",
    "with open('./data/dfvals_cb.pkl','wb') as handle:\n",
    "    pickle.dump(df_vals_cb, handle)\n",
    "with open('./data/dfvals_self_cb.pkl','wb') as handle:\n",
    "    pickle.dump(df_vals_self_cb, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute DF values for the Osipkov-Merritt DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "epsen_dens_fitting_dir = '/epsen_data/scr/lane/projects/tng-dfs/fitting/'+\\\n",
    "    'density_profile/'\n",
    "epsen_df_fitting_dir = '/epsen_data/scr/lane/projects/tng-dfs/fitting/'+\\\n",
    "    'distribution_function/'\n",
    "fig_dir = './fig/constant_beta/'\n",
    "os.makedirs(fig_dir,exist_ok=True)\n",
    "\n",
    "# DM halo information\n",
    "dm_halo_version = 'poisson_nfw'\n",
    "dm_halo_ncut = 500\n",
    "\n",
    "# Stellar bulge and disk information\n",
    "stellar_bulge_disk_version = 'miyamoto_disk_pswc_bulge_tps_halo'\n",
    "stellar_bulge_disk_ncut = 2000\n",
    "\n",
    "# Stellar halo density information\n",
    "stellar_halo_density_version = 'poisson_twopower'\n",
    "stellar_halo_density_ncut = 500\n",
    "\n",
    "# Stellar halo rotation information\n",
    "stellar_halo_rotation_version = 'tanh_rotation'\n",
    "stellar_halo_rotation_ncut = 500\n",
    "\n",
    "# Beta information\n",
    "beta_version = 'osipkov_merritt'\n",
    "beta_ncut = 500\n",
    "\n",
    "# Define density profiles\n",
    "dm_halo_densfunc = pdens.NFWSpherical()\n",
    "disk_densfunc = pdens.MiyamotoNagaiDisk()\n",
    "bulge_densfunc = pdens.SinglePowerCutoffSpherical()\n",
    "stellar_halo_densfunc = pdens.TwoPowerSpherical()\n",
    "stellar_bulge_disk_densfunc = pdens.CompositeDensityProfile(\n",
    "    [disk_densfunc,\n",
    "     bulge_densfunc,\n",
    "     stellar_halo_densfunc]\n",
    "     )\n",
    "\n",
    "df_vals_om = []\n",
    "df_vals_self_om = []\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i != 0: continue\n",
    "    if verbose: print(f'Plotting MW {i+1}/{n_mw}')\n",
    "\n",
    "    # Get the primary\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    major_mergers = primary.tree_major_mergers\n",
    "    n_major = primary.n_major_mergers\n",
    "    n_snap = len(primary.snapnum)\n",
    "    primary_filename = primary.get_cutout_filename(mw_analog_dir,\n",
    "        snapnum=primary.snapnum[0])\n",
    "    co = pcutout.TNGCutout(primary_filename)\n",
    "    co.center_and_rectify()\n",
    "    pid = co.get_property('stars','ParticleIDs')\n",
    "\n",
    "    # # Get the dark halo\n",
    "    dm_halo_filename = os.path.join(epsen_dens_fitting_dir,'dm_halo/',dm_halo_version,\n",
    "        str(z0_sid), 'sampler.pkl')\n",
    "    dm_halo_pot = pfit.construct_pot_from_fit(dm_halo_filename,\n",
    "        dm_halo_densfunc, dm_halo_ncut, ro=ro, vo=vo)\n",
    "    \n",
    "    # Get the stellar bulge and disk\n",
    "    stellar_bulge_disk_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "        'stellar_bulge_disk/',stellar_bulge_disk_version,str(z0_sid),\n",
    "        'sampler.pkl')\n",
    "    stellar_pots = pfit.construct_pot_from_fit(stellar_bulge_disk_filename,\n",
    "        stellar_bulge_disk_densfunc, stellar_bulge_disk_ncut, ro=ro, vo=vo)\n",
    "    fpot = [stellar_pots[1], stellar_pots[0], dm_halo_pot] # bulge, disk, halo\n",
    "\n",
    "    # Load the interpolator for the sphericalized potential\n",
    "    interpolator_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "        'spherical_interpolated_potential/',str(z0_sid),'interp_potential.pkl')\n",
    "    with open(interpolator_filename,'rb') as handle:\n",
    "        interpot = pickle.load(handle)\n",
    "\n",
    "    _df_vals = []\n",
    "    _df_vals_self = []\n",
    "\n",
    "    for j in range(n_major):\n",
    "        # if j > 0: continue\n",
    "        if verbose: print(f'  Constructing DF for major merger {j+1}/{n_major}')\n",
    "\n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_acc_sid = major_merger.subfind_id[0]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "        upid = major_merger.get_unique_particle_ids('stars',data_dir=data_dir)\n",
    "        indx = np.where(np.isin(pid,upid))[0]\n",
    "\n",
    "        # Get energy and angular momentum\n",
    "        orbs = co.get_orbs('stars')[indx]\n",
    "        n_star = len(orbs)\n",
    "        rs = orbs.r().to_value(apu.kpc)\n",
    "        masses = co.get_masses('stars')[indx].to_value(apu.Msun)\n",
    "        L = np.sum(orbs.L().to_value(apu.kpc*apu.km/apu.s)**2,axis=1)**0.5\n",
    "        Lz = orbs.Lz().to_value(apu.kpc*apu.km/apu.s)\n",
    "        pe = co.get_potential_energy('stars')[indx].to_value(apu.km**2/apu.s**2)\n",
    "        vels = co.get_velocities('stars')[indx].to_value(apu.km/apu.s)\n",
    "        vmag = np.linalg.norm(vels,axis=1)\n",
    "        energy = pe + 0.5*vmag**2\n",
    "\n",
    "        # Get the stellar halo density profile (denspot for the DF)\n",
    "        stellar_halo_density_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "            'stellar_halo/',stellar_halo_density_version,str(z0_sid),\n",
    "            'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        denspot = pfit.construct_pot_from_fit(\n",
    "            stellar_halo_density_filename, stellar_halo_densfunc, \n",
    "            stellar_halo_density_ncut, ro=ro, vo=vo)\n",
    "        \n",
    "        # Get the stellar halo rotation kernel\n",
    "        stellar_halo_rotation_filename = os.path.join(epsen_dens_fitting_dir,\n",
    "            'stellar_halo/',stellar_halo_rotation_version,str(z0_sid),\n",
    "            'merger_'+str(j+1)+'/', 'sampler.pkl')\n",
    "        assert os.path.exists(stellar_halo_rotation_filename)\n",
    "        with open(stellar_halo_rotation_filename,'rb') as handle:\n",
    "            stellar_halo_rotation_sampler = pickle.load(handle)\n",
    "        stellar_halo_rotation_samples = stellar_halo_rotation_sampler.get_chain(\n",
    "            discard=stellar_halo_rotation_ncut, flat=True)\n",
    "        # Params are frot, chi\n",
    "        stellar_halo_frot, stellar_halo_chi = \\\n",
    "            np.median(stellar_halo_rotation_samples,axis=0)\n",
    "        \n",
    "        # Load the distribution function and wrangle\n",
    "        df_filename = os.path.join(epsen_df_fitting_dir,beta_version,\n",
    "            str(z0_sid),'merger_'+str(j+1),'df.pkl')\n",
    "        with open(df_filename,'rb') as handle:\n",
    "            dfom = pickle.load(handle)\n",
    "        dfom = pkin.reconstruct_anisotropic_df(dfom, interpot, denspot)\n",
    "\n",
    "        # Scale the input energies by the potential energy of interpot at the \n",
    "        # stellar half-mass radius\n",
    "        rhalf = pkin.half_mass_radius(rs, masses)\n",
    "        rhalf_perc = 0.05\n",
    "        rhalf_mask = np.abs(rs-rhalf) < rhalf_perc*rhalf\n",
    "        rhalf_pe_star = np.median(pe[rhalf_mask])\n",
    "        rhalf_pe_interpot = potential.evaluatePotentials(\n",
    "            interpot, rhalf*apu.kpc, 0.).to_value(apu.km**2/apu.s**2)\n",
    "        rhalf_pe_offset = rhalf_pe_star - rhalf_pe_interpot\n",
    "\n",
    "        # Compute the energy and Q\n",
    "        energy_offset = energy - rhalf_pe_offset\n",
    "        Q_offset = -energy_offset - 0.5*L**2/(dfom._ra*ro)**2\n",
    "        Q_offset_internal = Q_offset / (vo**2)\n",
    "        \n",
    "        # Compute the value of the DF speedily using the interpolator\n",
    "        fQ = np.exp( dfom._logfQ_interp(Q_offset_internal) )\n",
    "        fLz = (stellar_halo_frot*np.tanh(Lz/stellar_halo_chi)-stellar_halo_frot+1)\n",
    "        f = fQ*fLz\n",
    "\n",
    "        # Do the self-similar assessment of the DF\n",
    "        # Create the samples for the DF\n",
    "        sample = dfom.sample(n=n_star, rmin=rs.min()*apu.kpc*0.9)\n",
    "        sample = pkin.rotate_df_samples(sample,stellar_halo_frot,stellar_halo_chi)\n",
    "\n",
    "        # Compute the likelihoods for the self-similar DF\n",
    "        pe_offset_sample = 0. # PE offset for DF samples should be 0\n",
    "        energy_offset_sample = sample.E(pot=interpot).to_value(apu.km**2/apu.s**2) - pe_offset_sample\n",
    "        L_sample = (np.sum(sample.L()**2,1)**0.5).to_value(apu.kpc*apu.km/apu.s)\n",
    "        Q_offset_sample = -energy_offset_sample - 0.5*L_sample**2/(dfom._ra*ro)**2\n",
    "        Q_offset_internal_sample = Q_offset_sample / (vo**2)\n",
    "        Lz_sample = sample.Lz().to_value(apu.kpc*apu.km/apu.s)\n",
    "        fQ_sample = np.exp( dfom._logfQ_interp(Q_offset_internal_sample) )\n",
    "        fLz_sample = (stellar_halo_frot*np.tanh(Lz_sample/stellar_halo_chi)-stellar_halo_frot+1)\n",
    "        f_sample = fQ_sample*fLz_sample\n",
    "\n",
    "        _df_vals.append([f,fQ,fLz,z0_sid,j+1,major_mlpid])\n",
    "        _df_vals_self.append([f_sample,fQ_sample,fLz_sample,z0_sid,j+1,major_mlpid])\n",
    "\n",
    "    df_vals_om.append( _df_vals )\n",
    "    df_vals_self_om.append( _df_vals_self )\n",
    "\n",
    "os.makedirs('./data/', exist_ok=True)\n",
    "with open('./data/dfvals_om.pkl','wb') as handle:\n",
    "    pickle.dump(df_vals_om, handle)\n",
    "with open('./data/dfvals_self_om.pkl','wb') as handle:\n",
    "    pickle.dump(df_vals_self_om, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All DF vals should have been saved if already calculated, so don't need to \n",
    "# ask permission to overwrite\n",
    "with open('./data/dfvals_cb.pkl','rb') as handle:\n",
    "    df_vals_cb = pickle.load(handle)\n",
    "with open('./data/dfvals_self_cb.pkl','rb') as handle:\n",
    "    df_vals_self_cb = pickle.load(handle)\n",
    "\n",
    "with open('./data/dfvals_om.pkl','rb') as handle:\n",
    "    df_vals_om = pickle.load(handle)\n",
    "with open('./data/dfvals_self_om.pkl','rb') as handle:\n",
    "    df_vals_self_om = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the input star and dark matter masses\n",
    "if not os.path.exists('./data/star_mass.pkl') or not os.path.exists('./data/dm_mass.pkl'):\n",
    "    star_mass = []\n",
    "    dm_mass = []\n",
    "\n",
    "    verbose = True\n",
    "\n",
    "    for i in range(n_mw):\n",
    "        # if i > 1: continue\n",
    "        if verbose: print(f'Getting MW {i+1}/{n_mw}')\n",
    "\n",
    "        # Get the primary\n",
    "        primary = tree_primaries[i]\n",
    "        z0_sid = primary.subfind_id[0]\n",
    "        n_snap = len(primary.snapnum)\n",
    "        n_major = primary.n_major_mergers\n",
    "        primary_filename = primary.get_cutout_filename(mw_analog_dir,\n",
    "            snapnum=primary.snapnum[0])\n",
    "        co = pcutout.TNGCutout(primary_filename)\n",
    "        dmpid = co.get_property('dm','ParticleIDs')\n",
    "        dmass = co.get_masses('dm').to_value(apu.Msun)\n",
    "        spid = co.get_property('stars','ParticleIDs')\n",
    "        smass = co.get_masses('stars').to_value(apu.Msun)\n",
    "\n",
    "        _star_mass = []\n",
    "        _dm_mass = []\n",
    "\n",
    "        for j in range(n_major):\n",
    "            if verbose: print(f'Merger {j+1}/{n_major}')\n",
    "\n",
    "            # Get the major merger particle IDs and mask\n",
    "            major_merger = primary.tree_major_mergers[j]\n",
    "            dmupid = major_merger.get_unique_particle_ids('dm',data_dir=data_dir)\n",
    "            supid = major_merger.get_unique_particle_ids('stars',data_dir=data_dir)\n",
    "            dmindx = np.isin(dmpid, dmupid)\n",
    "            sindx = np.isin(spid, supid)\n",
    "\n",
    "            _star_mass.append( np.sum(smass[sindx]) )\n",
    "            _dm_mass.append( np.sum(dmass[dmindx]) )\n",
    "        \n",
    "        star_mass.append(_star_mass)\n",
    "        dm_mass.append(_dm_mass)\n",
    "\n",
    "else:\n",
    "    with open('./data/star_mass.pkl','rb') as handle:\n",
    "        star_mass = pickle.load(handle)\n",
    "    with open('./data/dm_mass.pkl','rb') as handle:\n",
    "        dm_mass = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Wrangle the input loglikelihoods a bit\n",
    "\n",
    "loglike_data = []\n",
    "indx = 0\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i > 5: continue\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    n_snap = len(primary.snapnum)\n",
    "    n_major = primary.n_major_mergers\n",
    "\n",
    "    _loglike_data = []\n",
    "\n",
    "    for j in range(n_major):\n",
    "        \n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "\n",
    "        f_cb = df_vals_cb[i][j][0]\n",
    "        f_om = df_vals_om[i][j][0]\n",
    "        f_cb_self = df_vals_self_cb[i][j][0]\n",
    "        f_om_self = df_vals_self_om[i][j][0]\n",
    "\n",
    "        f_cb_mask = f_cb > 0\n",
    "        f_om_mask = f_om > 0\n",
    "        f_cb_self_mask = f_cb_self > 0\n",
    "        f_om_self_mask = f_om_self > 0\n",
    "        f_mask = f_cb_mask & f_om_mask & f_cb_self_mask & f_om_self_mask\n",
    "\n",
    "        _loglike_cb = np.log(f_cb[f_mask])\n",
    "        _loglike_om = np.log(f_om[f_mask])\n",
    "        _loglike_cb_self = np.log(f_cb_self[f_mask])\n",
    "        _loglike_om_self = np.log(f_om_self[f_mask])\n",
    "\n",
    "        assert z0_sid == df_vals_cb[i][j][4]\n",
    "        assert major_mlpid == df_vals_cb[i][j][6]\n",
    "\n",
    "        _loglike_data.append( [_loglike_cb, _loglike_om, _loglike_cb_self, \n",
    "            _loglike_om_self, z0_sid, j+1, major_mlpid] )\n",
    "    \n",
    "    loglike_data.append( _loglike_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the loglike differences\n",
    "loglike_diff_data = []\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i > 5: continue\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    n_snap = len(primary.snapnum)\n",
    "    n_major = primary.n_major_mergers\n",
    "\n",
    "    _loglike_diff_data = []\n",
    "\n",
    "    for j in range(n_major):\n",
    "\n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "\n",
    "        assert loglike_data[i][j][4] == z0_sid\n",
    "        assert loglike_data[i][j][5] == j+1\n",
    "        assert loglike_data[i][j][6] == major_mlpid\n",
    "\n",
    "        lld_cb_om = np.sum(loglike_data[i][j][0] - loglike_data[i][j][1])\n",
    "        lld_cb_self = np.sum(loglike_data[i][j][0] - loglike_data[i][j][2])\n",
    "        lld_om_self = np.sum(loglike_data[i][j][1] - loglike_data[i][j][3])\n",
    "        lld_cb_om_self = np.sum((loglike_data[i][j][0] - loglike_data[i][j][2]) - \\\n",
    "                                (loglike_data[i][j][1] - loglike_data[i][j][3]))\n",
    "        lld_N = len(loglike_data[i][j][0])\n",
    "        \n",
    "        _loglike_diff_data.append([lld_cb_om, \n",
    "                                   lld_cb_self, \n",
    "                                   lld_om_self,\n",
    "                                   lld_cb_om_self, \n",
    "                                   lld_N\n",
    "                                   ])\n",
    "\n",
    "    loglike_diff_data.append(_loglike_diff_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Finally construct a structured numpy array\n",
    "\n",
    "# Compute the loglike differences\n",
    "data = []\n",
    "\n",
    "for i in range(n_mw):\n",
    "    # if i > 5: continue\n",
    "    primary = tree_primaries[i]\n",
    "    z0_sid = primary.subfind_id[0]\n",
    "    n_snap = len(primary.snapnum)\n",
    "    n_major = primary.n_major_mergers\n",
    "\n",
    "    for j in range(n_major):\n",
    "\n",
    "        # Get the major merger\n",
    "        major_merger = primary.tree_major_mergers[j]\n",
    "        major_mlpid = major_merger.secondary_mlpid\n",
    "\n",
    "        assert loglike_data[i][j][4] == z0_sid\n",
    "        assert loglike_data[i][j][5] == j+1\n",
    "        assert loglike_data[i][j][6] == major_mlpid\n",
    "\n",
    "        _data = (loglike_diff_data[i][j][0], # CB - OM\n",
    "                 loglike_diff_data[i][j][1], # CB - CB_self\n",
    "                 loglike_diff_data[i][j][2], # OM - OM_self\n",
    "                 loglike_diff_data[i][j][3], # (CB - CB_self) - (OM - OM_self)\n",
    "                 loglike_diff_data[i][j][4], # loglike N\n",
    "                 star_mass[i][j], \n",
    "                 dm_mass[i][j],\n",
    "                 major_merger.star_mass_ratio,\n",
    "                 major_merger.dm_mass_ratio,\n",
    "                 putil.snapshot_to_redshift( major_merger.merger_snapnum ),\n",
    "                 z0_sid, \n",
    "                 j+1, \n",
    "                 major_mlpid, \n",
    "                 )\n",
    "\n",
    "        data.append( _data )\n",
    "    \n",
    "dt = np.dtype([('lld_cb_om',float),\n",
    "                ('lld_cb_self',float),\n",
    "                ('lld_om_self',float),\n",
    "                ('lld_cb_om_self',float),\n",
    "                ('lld_N',int),\n",
    "                ('star_mass',float),\n",
    "                ('dm_mass',float),\n",
    "                ('star_mass_ratio',float),\n",
    "                ('dm_mass_ratio',float),\n",
    "                ('z_merger',float),\n",
    "                ('z0_sid',int),\n",
    "                ('major_merger',int),\n",
    "                ('major_mlpid',int),\n",
    "                ])\n",
    "lldata = np.array(data,dtype=dt)\n",
    "os.makedirs('./data/', exist_ok=True)\n",
    "np.save('./data/lldata.npy',lldata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the number-weighted $\\mathcal{L}$ differences for each DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "x = lldata['lld_cb_om']/lldata['lld_N']\n",
    "ax.hist(x, range=[-1,1], bins=20, histtype='step', color='k', lw=2 )\n",
    "_y = np.ones_like(x)\n",
    "\n",
    "ax = pplot.plot_elements_out_of_bounds_as_arrows(x, _y, ax, \n",
    "    arrow_with_count=True, fx=0.1, fy=0.1)\n",
    "\n",
    "ax.axvline(0., color='k', ls='--', lw=2)\n",
    "ax.set_xlabel(r'$\\frac{1}{N} \\Delta \\log \\mathcal{L}_{\\rm CB-OM}$')\n",
    "ax.set_ylabel('N')\n",
    "fig.tight_layout()\n",
    "\n",
    "print(ax.get_xlim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,8))\n",
    "axs = fig.subplots(nrows=3, ncols=1)\n",
    "\n",
    "x1 = lldata['lld_cb_om_self']/lldata['lld_N']\n",
    "axs[0].hist(x1, range=[-1,1], bins=20, histtype='step', color='k', lw=2 )\n",
    "_y = np.ones_like(x1)\n",
    "axs[0] = pplot.plot_elements_out_of_bounds_as_arrows(x1, _y, axs[0], \n",
    "    arrow_with_count=True, fx=0.1, fy=0.1)\n",
    "axs[0].axvline(0., color='k', ls='--', lw=2)\n",
    "axs[0].set_xlabel(r'$\\frac{1}{N} \\Delta \\log \\mathcal{L}_{\\rm CB-OM-SIM}$')\n",
    "axs[0].set_ylabel('N')\n",
    "\n",
    "x2 = lldata['lld_cb_self']/lldata['lld_N']\n",
    "axs[1].hist(x2, range=[-1,1], bins=20, histtype='step', color='k', lw=2 )\n",
    "_y = np.ones_like(x2)\n",
    "axs[1] = pplot.plot_elements_out_of_bounds_as_arrows(x2, _y, axs[1], \n",
    "    arrow_with_count=True, fx=0.1, fy=0.1)\n",
    "axs[1].axvline(0., color='k', ls='--', lw=2)\n",
    "axs[1].set_xlabel(r'$\\frac{1}{N} \\Delta \\log \\mathcal{L}_{\\rm CB-SIM}$')\n",
    "axs[1].set_ylabel('N')\n",
    "\n",
    "x3 = lldata['lld_om_self']/lldata['lld_N']\n",
    "axs[2].hist(x3, range=[-1,1], bins=20, histtype='step', color='k', lw=2 )\n",
    "_y = np.ones_like(x3)\n",
    "axs[2] = pplot.plot_elements_out_of_bounds_as_arrows(x3, _y, axs[2], \n",
    "    arrow_with_count=True, fx=0.1, fy=0.1)\n",
    "axs[2].axvline(0., color='k', ls='--', lw=2)\n",
    "axs[2].set_xlabel(r'$\\frac{1}{N} \\Delta \\log \\mathcal{L}_{\\rm OM-SIM}$')\n",
    "axs[2].set_ylabel('N')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facecolor='none'\n",
    "edgecolor='k'\n",
    "s = 10\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "axs = fig.subplots(nrows=1, ncols=4)\n",
    "\n",
    "xlabels = [r'$M_{\\star}$', r'$z_{\\rm merger}$', \n",
    "           r'$M_{\\rm \\star,p}/M_{\\rm \\star,s}$', \n",
    "           r'$M_{\\rm DM,p}/M_{\\rm DM,s}$']\n",
    "\n",
    "ys = [lldata['star_mass'], lldata['z_merger'], \n",
    "      1/lldata['star_mass_ratio'], 1/lldata['dm_mass_ratio']]\n",
    "x = lldata['lld_cb_om']/lldata['lld_N']\n",
    "\n",
    "for k in range(4):\n",
    "    axs[k].scatter(ys[k], x, facecolor=facecolor, edgecolor=edgecolor, s=s, \n",
    "        zorder=2)\n",
    "                  \n",
    "    axs[k].set_xlabel(xlabels[k])\n",
    "    axs[k].set_ylim(-1.5,1)\n",
    "    axs[k].axhline(0., color='Gray', ls='--', lw=1, zorder=1)\n",
    "    arrow_kwargs = {'alpha':0.5}\n",
    "    axs[k] = pplot.plot_elements_out_of_bounds_as_arrows(ys[k], x, axs[k], \n",
    "        arrow_with_count=False, fx=0.1, fy=0.1, arrow_kwargs=arrow_kwargs)\n",
    "    if k in [0,2,3]:\n",
    "        axs[k].set_xscale('log')\n",
    "    if k in [1,2,3]:\n",
    "        axs[k].tick_params(labelleft=False)\n",
    "    \n",
    "    indep_argsort = np.argsort(ys[k])\n",
    "    window_size=15\n",
    "    ma_x = np.convolve(ys[k][indep_argsort], \n",
    "        np.ones(window_size)/window_size, mode='valid')\n",
    "    ma_y = np.convolve(x[indep_argsort], \n",
    "        np.ones(window_size)/window_size, mode='valid')\n",
    "    axs[k].plot(ma_x, ma_y, color='Red', ls='solid', lw=1, zorder=3)\n",
    "\n",
    "axs[0].set_ylabel(r'$\\frac{1}{N} \\Delta \\log \\mathcal{L}_{\\rm CB-OM}$')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
