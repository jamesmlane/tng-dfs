{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# TITLE - 2-download_secondary_cutouts.ipynb\n",
    "# AUTHOR - James Lane\n",
    "# PROJECT - tng-dfs\n",
    "#\n",
    "# ------------------------------------------------------------------------\n",
    "#\n",
    "# Docstring:\n",
    "'''Download primary cutouts for merger analogs.\n",
    "'''\n",
    "\n",
    "__author__ = \"James Lane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../src/nb_modules/nb_imports.txt\n",
    "### Imports\n",
    "\n",
    "## Basic\n",
    "import numpy as np\n",
    "import sys, os, pdb, glob, dill as pickle, h5py\n",
    "\n",
    "## Matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## Astropy\n",
    "from astropy import units as apu\n",
    "\n",
    "## Project-specific\n",
    "src_path = 'src/'\n",
    "while True:\n",
    "    if os.path.exists(src_path): break\n",
    "    if os.path.realpath(src_path).split('/')[-1] in ['tng-dfs','/']:\n",
    "            raise FileNotFoundError('Failed to find src/ directory.')\n",
    "    src_path = os.path.join('..',src_path)\n",
    "sys.path.insert(0,'../../src/')\n",
    "from tng_dfs import cutout as pcutout\n",
    "from tng_dfs import tree as ptree\n",
    "from tng_dfs import util as putil\n",
    "\n",
    "### Notebook setup\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('../../src/mpl/project.mplstyle') # This must be exactly here\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords, loading, pathing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../src/nb_modules/nb_setup.txt\n",
    "# Keywords\n",
    "cdict = putil.load_config_to_dict()\n",
    "keywords = ['DATA_DIR','MW_ANALOG_DIR','RO','VO','ZO','LITTLE_H',\n",
    "            'MW_MASS_RANGE']\n",
    "data_dir,mw_analog_dir,ro,vo,zo,h,mw_mass_range = \\\n",
    "    putil.parse_config_dict(cdict,keywords)\n",
    "\n",
    "# MW Analog \n",
    "mwsubs,mwsubs_vars = putil.prepare_mwsubs(mw_analog_dir,h=h,\n",
    "    mw_mass_range=mw_mass_range,return_vars=True,force_mwsubs=False,\n",
    "    bulge_disk_fraction_cuts=True)\n",
    "\n",
    "# Figure path\n",
    "fig_dir = './fig/sample/'\n",
    "log_dir = './log/'\n",
    "epsen_fig_dir = '/epsen_data/scr/lane/projects/tng-dfs/figs/notebooks/sample/'\n",
    "os.makedirs(fig_dir,exist_ok=True)\n",
    "os.makedirs(epsen_fig_dir,exist_ok=True)\n",
    "show_plots = False\n",
    "\n",
    "# Load tree data\n",
    "tree_primary_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_primaries.pkl')\n",
    "with open(tree_primary_filename,'rb') as handle: \n",
    "    tree_primaries = pickle.load(handle)\n",
    "tree_major_mergers_filename = os.path.join(mw_analog_dir,\n",
    "    'major_mergers/tree_major_mergers.pkl')\n",
    "with open(tree_major_mergers_filename,'rb') as handle:\n",
    "    tree_major_mergers = pickle.load(handle)\n",
    "n_mw = len(tree_primaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some information about the simulation snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snaps = putil.get( mwsubs_vars['sim']['snapshots'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the cutouts for the main branch of each secondary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_download_cutouts = False\n",
    "\n",
    "# Prepare directory structure\n",
    "for i in range(len(snaps)):\n",
    "    snap_path = data_dir+'cutouts/snap_'+str(snaps[i]['number'])+'/'\n",
    "    os.makedirs(snap_path,exist_ok=True)\n",
    "\n",
    "# Open some log files\n",
    "log_file = open(os.path.join(log_dir,\n",
    "    'download_cutouts_secondary_main_branches.log'),'w')\n",
    "exceptions = []\n",
    "\n",
    "# First loop over all primaries and download cutouts for each\n",
    "txt = 'Downloading cutouts for all secondary main branches...\\n'+'-'*50\n",
    "print(txt)\n",
    "log_file.write(txt+'\\n')\n",
    "for i in range(n_mw):\n",
    "    \n",
    "    # if i > 2: continue\n",
    "\n",
    "    # Load the tree, get the subfind IDs and snapshot numbers\n",
    "    primary = tree_primaries[i]\n",
    "    tree = ptree.SublinkTree(primary.tree_filename)\n",
    "    sfids = tree.get_property('SubfindID')\n",
    "    snapnums = tree.get_property('SnapNum')\n",
    "    mlpids = tree.get_property('MainLeafProgenitorID')\n",
    "    assert mwsubs[i]['id'] == sfids[0], 'Consistency check failed.'\n",
    "    z0_sid = sfids[0]\n",
    "\n",
    "    txt = 'Getting cutouts for primary with z=0 subfind id: '+str(z0_sid)+'...'\n",
    "    print(txt)\n",
    "    log_file.write(txt+'\\n')\n",
    "\n",
    "    # Loop over all secondaries and download their main branch cutouts\n",
    "    n_major_mergers = primary.n_major_mergers\n",
    "    for j in range(n_major_mergers):\n",
    "        secondary = primary.tree_major_mergers[j]\n",
    "        secondary_mask = (mlpids == secondary.secondary_mlpid)\n",
    "        secondary_snapnums = snapnums[secondary_mask]\n",
    "        secondary_sfids = sfids[secondary_mask]\n",
    "        secondary_has_data = np.zeros(len(secondary_snapnums),dtype=bool)\n",
    "\n",
    "        # Loop over all secondary snapshots\n",
    "        for k in range(len(secondary_snapnums)):\n",
    "            ssn = secondary_snapnums[k]\n",
    "            sfid = secondary_sfids[k]\n",
    "            snap_path = data_dir+'mw_analogs/cutouts/snap_'+str(ssn)+'/'\n",
    "            snap_filename = snap_path+'cutout_'+str(sfid)+'.hdf5'\n",
    "\n",
    "            # Check if the cutout already exists\n",
    "            if os.path.isfile(snap_filename):\n",
    "                secondary_has_data[k] = True\n",
    "                txt = 'Already have cutout for subhalo '+str(sfid)+' of '+\\\n",
    "                    'snapshot '+str(ssn)\n",
    "                log_file.write(txt+'\\n')\n",
    "                continue\n",
    "\n",
    "            # Fetch the subhalo\n",
    "            try:\n",
    "                subhalo = putil.get(snaps[ssn]['url']+'subhalos/'+str(sfid),\n",
    "                    timeout=None)\n",
    "            except Exception as e:\n",
    "                exceptions.append((e,ssn,sfid))\n",
    "                txt = 'Exception raised for subhalo '+str(sfid)+' of '+\\\n",
    "                    'snapshot '+str(ssn)+' while fetching subhalo information'\n",
    "                print(txt)\n",
    "                log_file.write(txt+'\\n')\n",
    "                continue\n",
    "\n",
    "            # Some consistency checks\n",
    "            assert ssn == subhalo['snap']\n",
    "            assert sfid == subhalo['id']\n",
    "            # Download the cutout\n",
    "            txt = 'Downloading subhalo '+str(sfid)+' of snapshot '+str(ssn)\n",
    "            print(txt)\n",
    "            log_file.write(txt+'\\n')\n",
    "            try:\n",
    "                _=putil.get(subhalo['cutouts']['subhalo'],directory=snap_path,\n",
    "                    timeout=None)\n",
    "            except Exception as e:\n",
    "                exceptions.append((e,ssn,sfid))\n",
    "                txt = 'Exception raised for subhalo '+str(sfid)+' of '+\\\n",
    "                    'snapshot '+str(ssn)+' while downloading cutout'\n",
    "                print(txt)\n",
    "                log_file.write(txt+'\\n')\n",
    "            \n",
    "        # Communicate if all cutouts already exist\n",
    "        if np.all(secondary_has_data):\n",
    "            print('Already have all cutouts for secondary')\n",
    "\n",
    "# Close the log file\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the cutouts for the entire secondary branch rooted at the point where the secondary merges with the primary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_download_cutouts = False\n",
    "\n",
    "# Prepare directory structure\n",
    "for i in range(len(snaps)):\n",
    "    snap_path = data_dir+'cutouts/snap_'+str(snaps[i]['number'])+'/'\n",
    "    os.makedirs(snap_path,exist_ok=True)\n",
    "\n",
    "# Open some log files\n",
    "log_file = open(os.path.join(log_dir,\n",
    "    'download_cutouts_secondary_all.log'),'w')\n",
    "exceptions = []\n",
    "\n",
    "# First loop over all primaries and download cutouts for each\n",
    "txt = 'Downloading cutouts for all secondary branches...\\n'+'-'*50\n",
    "print(txt)\n",
    "log_file.write(txt+'\\n')\n",
    "for i in range(n_mw):\n",
    "    \n",
    "    if i > 10: continue\n",
    "\n",
    "    # Load the tree, get the subfind IDs and snapshot numbers\n",
    "    primary = tree_primaries[i]\n",
    "    tree = ptree.SublinkTree(primary.tree_filename)\n",
    "    sids = tree.get_property('SubhaloID')\n",
    "    sfids = tree.get_property('SubfindID')\n",
    "    snapnums = tree.get_property('SnapNum')\n",
    "    mlpids = tree.get_property('MainLeafProgenitorID')\n",
    "    lpids = tree.get_property('LastProgenitorID')\n",
    "    assert mwsubs[i]['id'] == sfids[0], 'Consistency check failed.'\n",
    "    z0_sid = sfids[0]\n",
    "\n",
    "    txt = 'Getting cutouts for primary with z=0 subfind id: '+str(z0_sid)+'...'\n",
    "    print(txt)\n",
    "    log_file.write(txt+'\\n')\n",
    "\n",
    "    # Loop over all secondaries and download their main branch cutouts\n",
    "    n_major_mergers = primary.n_major_mergers\n",
    "    for j in range(n_major_mergers):\n",
    "        secondary = primary.tree_major_mergers[j]\n",
    "        secondary_main_mask = (mlpids == secondary.secondary_mlpid)\n",
    "        secondary_sid = sids[secondary_main_mask][0]\n",
    "        secondary_lpid = lpids[secondary_main_mask][0]\n",
    "        secondary_branch_sids = np.arange(0,secondary_lpid-secondary_sid+1) + \\\n",
    "            secondary_sid\n",
    "        secondary_branch_mask = np.isin(sids,secondary_branch_sids)\n",
    "        secondary_branch_snapnums = snapnums[secondary_branch_mask]\n",
    "        secondary_branch_sfids = sfids[secondary_branch_mask]\n",
    "        secondary_has_data = np.zeros(len(secondary_branch_sids),dtype=bool)\n",
    "\n",
    "        txt = 'Getting '+str(len(secondary_branch_sids))+' cutouts for '+\\\n",
    "            'secondary with MLPID '+str(secondary.secondary_mlpid)+'...'\n",
    "        print(txt)\n",
    "        log_file.write(txt+'\\n')\n",
    "\n",
    "        # Loop over all secondary snapshots\n",
    "        for k in range(len(secondary_branch_sids)):\n",
    "            ssn = secondary_branch_snapnums[k]\n",
    "            sfid = secondary_branch_sfids[k]\n",
    "            snap_path = data_dir+'mw_analogs/cutouts/snap_'+str(ssn)+'/'\n",
    "            snap_filename = snap_path+'cutout_'+str(sfid)+'.hdf5'\n",
    "\n",
    "            # Check if the cutout already exists\n",
    "            if os.path.isfile(snap_filename):\n",
    "                secondary_has_data[k] = True\n",
    "                txt = 'Already have cutout for subhalo '+str(sfid)+' of '+\\\n",
    "                    'snapshot '+str(ssn)\n",
    "                log_file.write(txt+'\\n')\n",
    "                continue\n",
    "\n",
    "            # Fetch the subhalo\n",
    "            try:\n",
    "                subhalo = putil.get(snaps[ssn]['url']+'subhalos/'+str(sfid),\n",
    "                    timeout=None)\n",
    "            except Exception as e:\n",
    "                exceptions.append((e,ssn,sfid))\n",
    "                txt = 'Exception raised for subhalo '+str(sfid)+' of '+\\\n",
    "                    'snapshot '+str(ssn)+' while fetching subhalo information'\n",
    "                print(txt)\n",
    "                log_file.write(txt+'\\n')\n",
    "                continue\n",
    "\n",
    "            # Some consistency checks\n",
    "            assert ssn == subhalo['snap']\n",
    "            assert sfid == subhalo['id']\n",
    "            # Download the cutout\n",
    "            txt = 'Downloading subhalo '+str(sfid)+' of snapshot '+str(ssn)\n",
    "            print(txt)\n",
    "            log_file.write(txt+'\\n')\n",
    "            try:\n",
    "                _=putil.get(subhalo['cutouts']['subhalo'],directory=snap_path,\n",
    "                    timeout=None)\n",
    "            except Exception as e:\n",
    "                exceptions.append((e,ssn,sfid))\n",
    "                txt = 'Exception raised for subhalo '+str(sfid)+' of '+\\\n",
    "                    'snapshot '+str(ssn)+' while downloading cutout'\n",
    "                print(txt)\n",
    "                log_file.write(txt+'\\n')\n",
    "            \n",
    "        # Communicate if all cutouts already exist\n",
    "        if np.all(secondary_has_data):\n",
    "            print('Already have all cutouts for secondary')\n",
    "\n",
    "# Close the log file\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
